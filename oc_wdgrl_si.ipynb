{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def gen_data(mu, delta, n, d: int = 2):\n",
    "    noise = np.random.normal(loc = 0, scale = 1, size=(n, d))\n",
    "    mu = np.full((n, d), mu, dtype=np.float64)\n",
    "\n",
    "    if len(delta) == 1 and delta[0] == 0:\n",
    "        return mu + noise, np.zeros(n)\n",
    "    \n",
    "    # 10% of the data are abnormal\n",
    "    m = len(delta)\n",
    "    abnormal_idx = np.random.choice(n, int(n/10), replace=False)\n",
    "\n",
    "    ptr = 0\n",
    "    for i in range(m):\n",
    "        for j in range(len(abnormal_idx)//m):\n",
    "            mu[abnormal_idx[ptr], :] += delta[i]\n",
    "            ptr += 1\n",
    "    \n",
    "    X = mu + noise \n",
    "    Y = np.zeros(n)\n",
    "    Y[abnormal_idx] = 1\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"Feature extractor network.\"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"Domain critic network.\"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class WDGRL():\n",
    "    def __init__(self, input_dim: int=2, generator_hidden_dims: List[int]=[32, 16, 8, 4, 2], critic_hidden_dims: List[int]=[32, 16, 8, 4, 2],\n",
    "                 gamma: float = 0.1, _lr_generator: float = 1e-2, _lr_critic: float = 1e-2, \n",
    "                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.generator = Generator(input_dim, generator_hidden_dims).to(self.device)\n",
    "        self.critic = Critic(generator_hidden_dims[-1], critic_hidden_dims).to(self.device)\n",
    "        self.generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=_lr_generator)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=_lr_critic)\n",
    "    \n",
    "    def compute_gradient_penalty(self, source_data: torch.Tensor, target_data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute gradient penalty.\"\"\"\n",
    "        if source_data.size(0) > target_data.size(0):\n",
    "            ms = source_data.size(0)\n",
    "            mt = target_data.size(0)\n",
    "            gradient_penalty = 0\n",
    "            for _ in range(0, ms, mt):\n",
    "                source_chunk = source_data[_:_+mt]\n",
    "                target_chunk = target_data\n",
    "                alpha = torch.rand(target_chunk.size(0), 1).to(self.device)\n",
    "                interpolates = (alpha * source_chunk + ((1 - alpha) * target_chunk)).requires_grad_(True)\n",
    "                \n",
    "                # Domain critic outputs\n",
    "                dc_output = self.critic(interpolates)\n",
    "                \n",
    "                # Compute gradients\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=dc_output,\n",
    "                    inputs=interpolates,\n",
    "                    grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True,\n",
    "                    only_inputs=True,\n",
    "                )\n",
    "                gradients = gradients[0]\n",
    "                gradient_penalty += ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "            if ms % mt != 0:\n",
    "                source_chunk = source_data[ms-mt:]\n",
    "                perm = torch.randperm(mt)\n",
    "                idx = perm[:ms % mt]\n",
    "                target_chunk = target_data[idx]\n",
    "                alpha = torch.rand(target_chunk.size(0), 1).to(self.device)\n",
    "                interpolates = (alpha * source_chunk + ((1 - alpha) * target_chunk)).requires_grad_(True)\n",
    "                \n",
    "                # Domain critic outputs\n",
    "                dc_output = self.critic(interpolates)\n",
    "                \n",
    "                # Compute gradients\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=dc_output,\n",
    "                    inputs=interpolates,\n",
    "                    grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True,\n",
    "                    only_inputs=True,\n",
    "                )\n",
    "                gradients = gradients[0]\n",
    "                gradient_penalty += ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "            return gradient_penalty / ((ms // mt) + (ms % mt != 0)) \n",
    "        \n",
    "        # For balanced batch\n",
    "        alpha = torch.rand(source_data.size(0), 1).to(self.device)\n",
    "        interpolates = (alpha * source_data + ((1 - alpha) * target_data)).requires_grad_(True)\n",
    "        \n",
    "        # Domain critic outputs\n",
    "        dc_output = self.critic(interpolates)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=dc_output,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "\n",
    "        # Compute gradient penalty\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "    def train(self, source_loader: DataLoader, target_loader: DataLoader, num_epochs: int = 100, dc_iter: int = 100) -> List[float]:\n",
    "        self.generator.train()\n",
    "        self.critic.train()\n",
    "        losses = []\n",
    "        source_critic_scores = []\n",
    "        target_critic_scores = []\n",
    "        for epoch in trange(num_epochs, desc='Epoch'):\n",
    "            loss = 0\n",
    "            for (source_data, _), (target_data, _) in zip(source_loader, target_loader):\n",
    "                source_data, target_data = source_data.to(self.device), target_data.to(self.device)\n",
    "\n",
    "                # Train domain critic\n",
    "                for _ in range(dc_iter):\n",
    "                    self.critic_optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        source_features = self.generator(source_data)\n",
    "                        target_features = self.generator(target_data)\n",
    "                    \n",
    "                    # Compute empirical Wasserstein distance\n",
    "                    dc_source = self.critic(source_features)\n",
    "                    dc_target = self.critic(target_features)\n",
    "                    wasserstein_distance = dc_source.mean() - dc_target.mean()\n",
    "\n",
    "                    # Gradient penalty\n",
    "                    gradient_penalty = self.compute_gradient_penalty(source_features, target_features)\n",
    "\n",
    "                    # Domain critic loss\n",
    "                    dc_loss = - wasserstein_distance + self.gamma * gradient_penalty\n",
    "                    dc_loss.backward()\n",
    "                    self.critic_optimizer.step()\n",
    "\n",
    "                # Train feature extractor\n",
    "                self.generator_optimizer.zero_grad()\n",
    "                source_features = self.generator(source_data)\n",
    "                target_features = self.generator(target_data)\n",
    "                dc_source = self.critic(source_features)\n",
    "                dc_target = self.critic(target_features)\n",
    "                wasserstein_distance = dc_source.mean() - dc_target.mean()\n",
    "                wasserstein_distance.backward()\n",
    "                self.generator_optimizer.step()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    loss += wasserstein_distance.item()\n",
    "                    \n",
    "            source_critic_scores.append(self.criticize(source_loader.dataset.tensors[0].to(self.device)))\n",
    "            target_critic_scores.append(self.criticize(target_loader.dataset.tensors[0].to(self.device)))\n",
    "            losses.append(loss/len(source_loader))\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} | Loss: {wasserstein_distance.item()}')\n",
    "        return losses, source_critic_scores, target_critic_scores\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract_feature(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.generator.eval()\n",
    "        return self.generator(x)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def criticize(self, x: torch.Tensor) -> float:\n",
    "        self.generator.eval()\n",
    "        self.critic.eval()\n",
    "        return self.critic(self.generator(x)).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of the WDGRL model (same architecture as before)\n",
    "model = WDGRL(input_dim=1,generator_hidden_dims=[10, 10, 10], critic_hidden_dims=[10])\n",
    "\n",
    "# Load the saved checkpoint\n",
    "checkpoint = torch.load(\"wdgrl.pth\", map_location=model.device, weights_only=True)\n",
    "\n",
    "# Restore the model weights\n",
    "model.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "model.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Create synthetic dataset and dataloaders for domain adaptation.\"\"\"\n",
    "# Create datasets\n",
    "ns, nt, d = 100, 10, 1\n",
    "mu_s, mu_t = 0, 20\n",
    "delta_s, delta_t = [3], [3]\n",
    "xs, ys = gen_data(mu_s, delta_s, ns, d)\n",
    "xt, yt = gen_data(mu_t, delta_t, nt, d)\n",
    "plt.scatter(xs[:, 0], np.zeros_like(xs[:, 0]), c=ys, cmap='viridis', s=2)\n",
    "plt.scatter(xt[:, 0], np.zeros_like(xt[:, 0]), c=yt, cmap='cool', s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.FloatTensor(xs)\n",
    "ys = torch.LongTensor(ys)\n",
    "xt = torch.FloatTensor(xt)\n",
    "yt = torch.LongTensor(yt)\n",
    "xs_hat = model.extract_feature(xs.cuda())\n",
    "xt_hat = model.extract_feature(xt.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_sum(X):\n",
    "    return np.argmax(np.sum(X, axis=1))\n",
    "x_hat = torch.cat([xs_hat, xt_hat], dim=0).cpu().numpy()\n",
    "print(x_hat)\n",
    "O = max_sum(x_hat)\n",
    "O = [O-ns]\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_hat = torch.zeros_like(yt)\n",
    "yt_hat[O[0]] = 1\n",
    "print(yt)\n",
    "yt_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpmath import mp\n",
    "\n",
    "mp.dps = 500\n",
    "def truncated_cdf(etajTy, mu, sigma, left, right):\n",
    "    numerator = mp.ncdf((etajTy - mu) / sigma) - mp.ncdf((left - mu) / sigma)\n",
    "    denominator = mp.ncdf((right - mu) / sigma) - mp.ncdf((left - mu) / sigma)\n",
    "    if denominator <= 1e-16:\n",
    "        true_cdf = 1\n",
    "    else:\n",
    "        true_cdf = numerator / denominator \n",
    "    return true_cdf\n",
    "def intersect(itv1, itv2):\n",
    "    # print(itv1, itv2)\n",
    "    itv = [max(itv1[0], itv2[0]), min(itv1[1], itv2[1])]\n",
    "    if itv[0] > itv[1]:\n",
    "        return None    \n",
    "    return itv\n",
    "\n",
    "def solve_linear_inequality(u, v): #u + vz < 0\n",
    "    if (v > -1e-16 and v < 1e-16):\n",
    "        v = 0\n",
    "        if (u < 0):\n",
    "            return [-np.Inf, np.Inf]\n",
    "        else:\n",
    "            print('error')\n",
    "            return None\n",
    "    if (v < 0):\n",
    "        return [-u/v, np.Inf]\n",
    "    return [np.NINF, -u/v]\n",
    "\n",
    "def get_interval(Xtj, a, b):\n",
    "    layers = []\n",
    "\n",
    "    for name, param in model.generator.named_children():\n",
    "        temp = dict(param._modules)\n",
    "        \n",
    "        for layer_name in temp.values():\n",
    "            if ('Linear' in str(layer_name)):\n",
    "                layers.append('Linear')\n",
    "            elif ('ReLU' in str(layer_name)):\n",
    "                layers.append('ReLU')\n",
    "\n",
    "    ptr = 0\n",
    "    itv = [np.NINF, np.Inf]\n",
    "    u = a\n",
    "    v = b\n",
    "    temp = Xtj\n",
    "    weight = None\n",
    "    bias = None\n",
    "    for name, param in model.generator.named_parameters():\n",
    "        if (layers[ptr] == 'Linear'):\n",
    "            if ('weight' in name):\n",
    "                weight = param.data.cpu().detach().numpy()\n",
    "            elif ('bias' in name):\n",
    "                bias = param.data.cpu().detach().numpy().reshape(-1, 1)\n",
    "                ptr += 1\n",
    "                temp = weight.dot(temp) + bias\n",
    "                u = weight.dot(u) + bias\n",
    "                v = weight.dot(v)\n",
    "\n",
    "        if (ptr < len(layers) and layers[ptr] == 'ReLU'):\n",
    "            ptr += 1\n",
    "            Relu_matrix = np.zeros((temp.shape[0], temp.shape[0]))\n",
    "            sub_itv = [np.NINF, np.inf]\n",
    "            for i in range(temp.shape[0]):\n",
    "                if temp[i] > 0:\n",
    "                    Relu_matrix[i][i] = 1\n",
    "                    sub_itv = intersect(sub_itv, solve_linear_inequality(-u[i][0], -v[i][0]))\n",
    "                else:\n",
    "                    sub_itv = intersect(sub_itv, solve_linear_inequality(u[i][0], v[i][0]))\n",
    "            itv = intersect(itv, sub_itv)\n",
    "            temp = Relu_matrix.dot(temp)\n",
    "            u = Relu_matrix.dot(u)\n",
    "            v = Relu_matrix.dot(v)\n",
    "\n",
    "    return itv, u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tpr():\n",
    "    ns, nt, d = 100, 10, 1\n",
    "    mu_s, mu_t = 0, 20\n",
    "    delta_s, delta_t = [3], [3]\n",
    "    xs, ys = gen_data(mu_s, delta_s, ns, d)\n",
    "    xt, yt = gen_data(mu_t, delta_t, nt, d)\n",
    "\n",
    "    xs = torch.FloatTensor(xs)\n",
    "    ys = torch.LongTensor(ys)\n",
    "    xt = torch.FloatTensor(xt)\n",
    "    yt = torch.LongTensor(yt)\n",
    "\n",
    "    xs_hat = model.extract_feature(xs.cuda())\n",
    "    xt_hat = model.extract_feature(xt.cuda())\n",
    "    x_hat = torch.cat([xs_hat, xt_hat], dim=0)\n",
    "\n",
    "    xs_hat = xs_hat.cpu()\n",
    "    xt_hat = xt_hat.cpu()\n",
    "    x_hat = x_hat.cpu()\n",
    "    xs = xs.cpu()\n",
    "    xt = xt.cpu()\n",
    "    ys = ys.cpu()\n",
    "    yt = yt.cpu()\n",
    "    \n",
    "    O = max_sum(x_hat.numpy())\n",
    "    if (O < ns):\n",
    "        return None\n",
    "    else:\n",
    "        O = [O - ns]   \n",
    "    if yt[O[0]] == 0:\n",
    "        return None\n",
    "    Oc = list(np.where(yt == 0)[0])\n",
    "    j = np.random.choice(O, 1, replace=False)[0]\n",
    "    etj = np.zeros((nt, 1))\n",
    "    etj[j][0] = 1\n",
    "    etOc = np.zeros((nt, 1))\n",
    "    etOc[Oc] = 1\n",
    "    etaj = np.vstack((np.zeros((ns, 1)), etj-(1/len(Oc))*etOc))\n",
    "    X = np.vstack((xs.numpy(), xt.numpy()))\n",
    "    \n",
    "\n",
    "    etajTX = etaj.T.dot(X)\n",
    "    print(f'Anomaly index: {O[0] + ns}')\n",
    "    print(f'etajTX: {etajTX}')\n",
    "    mu = np.vstack((np.full((ns,1), mu_s), np.full((nt,1), mu_t)))\n",
    "    sigma = np.identity(ns+nt)\n",
    "    etajTmu = etaj.T.dot(mu)\n",
    "    etajTsigmaetaj = etaj.T.dot(sigma).dot(etaj)\n",
    "    \n",
    "    b = sigma.dot(etaj).dot(np.linalg.inv(etajTsigmaetaj))\n",
    "    a = (np.identity(ns+nt) - b.dot(etaj.T)).dot(X)\n",
    "    \n",
    "\n",
    "    itv = [np.NINF, np.inf]\n",
    "    for i in range(X.shape[0]):\n",
    "        itv = intersect(itv, get_interval(X[i].reshape(-1, 1), a[i].reshape(-1, 1), b[i].reshape(-1, 1))[0])\n",
    "\n",
    "    sub_itv = [np.NINF, np.inf]\n",
    "    _, uo, vo = get_interval(X[O[0]+100].reshape(-1, 1), a[O[0]+100].reshape(-1, 1), b[O[0]+100].reshape(-1, 1))\n",
    "    I = np.ones((x_hat.shape[1],1))\n",
    "    for i in range(X.shape[0]):\n",
    "        if (i != O[0]+100):\n",
    "            _, ui, vi = get_interval(X[i].reshape(-1, 1), a[i].reshape(-1, 1), b[i].reshape(-1, 1))\n",
    "            u = uo - ui\n",
    "            v = vo - vi \n",
    "            u = I.T.dot(u)[0][0]\n",
    "            v = I.T.dot(v)[0][0]\n",
    "            sub__itv = solve_linear_inequality(-u, -v)\n",
    "            sub_itv = intersect(sub_itv, sub__itv)\n",
    "    itv = intersect(itv, sub_itv)\n",
    "\n",
    "    cdf = truncated_cdf(etajTX[0][0], etajTmu[0][0], np.sqrt(etajTsigmaetaj[0][0]), itv[0], itv[1])\n",
    "    p_value = float(2 * min(cdf, 1 - cdf))\n",
    "    print(f'p-value: {p_value}')\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #0:\n",
      "Anomaly index: 108\n",
      "etajTX: [[2.91529316]]\n",
      "p-value: 0.03798984306237815\n",
      "TPR: 1.0\n",
      "-------------------------------------------------\n",
      "iteration #2:\n",
      "Anomaly index: 109\n",
      "etajTX: [[1.209532]]\n",
      "p-value: 0.8122448200093092\n",
      "TPR: 0.5\n",
      "-------------------------------------------------\n",
      "iteration #3:\n",
      "Anomaly index: 103\n",
      "etajTX: [[2.0845996]]\n",
      "p-value: 0.5820851612862351\n",
      "TPR: 0.3333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #4:\n",
      "Anomaly index: 103\n",
      "etajTX: [[3.08306546]]\n",
      "p-value: 0.278112357976241\n",
      "TPR: 0.25\n",
      "-------------------------------------------------\n",
      "iteration #5:\n",
      "Anomaly index: 102\n",
      "etajTX: [[3.242406]]\n",
      "p-value: 0.02557668686943013\n",
      "TPR: 0.4\n",
      "-------------------------------------------------\n",
      "iteration #6:\n",
      "Anomaly index: 103\n",
      "etajTX: [[2.94435332]]\n",
      "p-value: 0.06943009369166153\n",
      "TPR: 0.3333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #7:\n",
      "Anomaly index: 107\n",
      "etajTX: [[1.74486393]]\n",
      "p-value: 0.4819277022891653\n",
      "TPR: 0.2857142857142857\n",
      "-------------------------------------------------\n",
      "iteration #8:\n",
      "Anomaly index: 107\n",
      "etajTX: [[2.27263048]]\n",
      "p-value: 0.13184819930691036\n",
      "TPR: 0.25\n",
      "-------------------------------------------------\n",
      "iteration #9:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.87079959]]\n",
      "p-value: 0.0035807799581563473\n",
      "TPR: 0.3333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #10:\n",
      "Anomaly index: 108\n",
      "etajTX: [[2.21815406]]\n",
      "p-value: 0.5550211312717878\n",
      "TPR: 0.3\n",
      "-------------------------------------------------\n",
      "iteration #11:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.54083994]]\n",
      "p-value: 0.06331661005312493\n",
      "TPR: 0.2727272727272727\n",
      "-------------------------------------------------\n",
      "iteration #12:\n",
      "Anomaly index: 109\n",
      "etajTX: [[4.11062092]]\n",
      "p-value: 0.0026459798463228505\n",
      "TPR: 0.3333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #13:\n",
      "Anomaly index: 109\n",
      "etajTX: [[3.2584983]]\n",
      "p-value: 0.013675374741929789\n",
      "TPR: 0.38461538461538464\n",
      "-------------------------------------------------\n",
      "iteration #14:\n",
      "Anomaly index: 103\n",
      "etajTX: [[1.80379465]]\n",
      "p-value: 0.47029349199178366\n",
      "TPR: 0.35714285714285715\n",
      "-------------------------------------------------\n",
      "iteration #15:\n",
      "Anomaly index: 101\n",
      "etajTX: [[2.18382539]]\n",
      "p-value: 0.5297128128346941\n",
      "TPR: 0.3333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #16:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.66022619]]\n",
      "p-value: 0.09638932682158802\n",
      "TPR: 0.3125\n",
      "-------------------------------------------------\n",
      "iteration #17:\n",
      "Anomaly index: 101\n",
      "etajTX: [[3.40691566]]\n",
      "p-value: 0.029410449111852626\n",
      "TPR: 0.35294117647058826\n",
      "-------------------------------------------------\n",
      "iteration #18:\n",
      "Anomaly index: 101\n",
      "etajTX: [[2.40958977]]\n",
      "p-value: 0.11367116238578417\n",
      "TPR: 0.3333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #19:\n",
      "Anomaly index: 109\n",
      "etajTX: [[4.14510536]]\n",
      "p-value: 0.0008851899130765387\n",
      "TPR: 0.3684210526315789\n",
      "-------------------------------------------------\n",
      "iteration #20:\n",
      "Anomaly index: 108\n",
      "etajTX: [[3.83983358]]\n",
      "p-value: 0.004945657033433611\n",
      "TPR: 0.4\n",
      "-------------------------------------------------\n",
      "iteration #21:\n",
      "Anomaly index: 103\n",
      "etajTX: [[2.77205065]]\n",
      "p-value: 0.1047222535638845\n",
      "TPR: 0.38095238095238093\n",
      "-------------------------------------------------\n",
      "iteration #22:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.80996916]]\n",
      "p-value: 0.05924970953299098\n",
      "TPR: 0.36363636363636365\n",
      "-------------------------------------------------\n",
      "iteration #23:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.03757244]]\n",
      "p-value: 0.012128634457933034\n",
      "TPR: 0.391304347826087\n",
      "-------------------------------------------------\n",
      "iteration #24:\n",
      "Anomaly index: 102\n",
      "etajTX: [[2.54055044]]\n",
      "p-value: 0.9223087831506872\n",
      "TPR: 0.375\n",
      "-------------------------------------------------\n",
      "iteration #25:\n",
      "Anomaly index: 102\n",
      "etajTX: [[2.28261079]]\n",
      "p-value: 0.33402474328580667\n",
      "TPR: 0.36\n",
      "-------------------------------------------------\n",
      "iteration #26:\n",
      "Anomaly index: 103\n",
      "etajTX: [[2.66808828]]\n",
      "p-value: 0.5351890266669173\n",
      "TPR: 0.34615384615384615\n",
      "-------------------------------------------------\n",
      "iteration #27:\n",
      "Anomaly index: 100\n",
      "etajTX: [[5.94517708]]\n",
      "p-value: 1.2735887808488038e-07\n",
      "TPR: 0.37037037037037035\n",
      "-------------------------------------------------\n",
      "iteration #28:\n",
      "Anomaly index: 105\n",
      "etajTX: [[4.05691507]]\n",
      "p-value: 0.00100725092322995\n",
      "TPR: 0.39285714285714285\n",
      "-------------------------------------------------\n",
      "iteration #29:\n",
      "Anomaly index: 106\n",
      "etajTX: [[3.87073051]]\n",
      "p-value: 0.006077571267930754\n",
      "TPR: 0.41379310344827586\n",
      "-------------------------------------------------\n",
      "iteration #30:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.63581806]]\n",
      "p-value: 0.07725180536968172\n",
      "TPR: 0.4\n",
      "-------------------------------------------------\n",
      "iteration #31:\n",
      "Anomaly index: 106\n",
      "etajTX: [[4.02669695]]\n",
      "p-value: 0.002579074244116024\n",
      "TPR: 0.41935483870967744\n",
      "-------------------------------------------------\n",
      "iteration #32:\n",
      "Anomaly index: 107\n",
      "etajTX: [[5.07935079]]\n",
      "p-value: 1.1265646792609492e-05\n",
      "TPR: 0.4375\n",
      "-------------------------------------------------\n",
      "iteration #33:\n",
      "Anomaly index: 106\n",
      "etajTX: [[4.36526595]]\n",
      "p-value: 0.00046939053072380917\n",
      "TPR: 0.45454545454545453\n",
      "-------------------------------------------------\n",
      "iteration #34:\n",
      "Anomaly index: 102\n",
      "etajTX: [[4.65935432]]\n",
      "p-value: 0.00013560652472356256\n",
      "TPR: 0.47058823529411764\n",
      "-------------------------------------------------\n",
      "iteration #35:\n",
      "Anomaly index: 101\n",
      "etajTX: [[4.37582546]]\n",
      "p-value: 0.00028223116508933405\n",
      "TPR: 0.4857142857142857\n",
      "-------------------------------------------------\n",
      "iteration #36:\n",
      "Anomaly index: 109\n",
      "etajTX: [[2.49602085]]\n",
      "p-value: 0.08916311647399307\n",
      "TPR: 0.4722222222222222\n",
      "-------------------------------------------------\n",
      "iteration #37:\n",
      "Anomaly index: 104\n",
      "etajTX: [[2.92853694]]\n",
      "p-value: 0.3202583252931789\n",
      "TPR: 0.4594594594594595\n",
      "-------------------------------------------------\n",
      "iteration #38:\n",
      "Anomaly index: 101\n",
      "etajTX: [[2.68210157]]\n",
      "p-value: 0.05949771380947177\n",
      "TPR: 0.4473684210526316\n",
      "-------------------------------------------------\n",
      "iteration #39:\n",
      "Anomaly index: 108\n",
      "etajTX: [[3.41717784]]\n",
      "p-value: 0.009835739975032173\n",
      "TPR: 0.46153846153846156\n",
      "-------------------------------------------------\n",
      "iteration #40:\n",
      "Anomaly index: 107\n",
      "etajTX: [[3.43010585]]\n",
      "p-value: 0.010634359750361444\n",
      "TPR: 0.475\n",
      "-------------------------------------------------\n",
      "iteration #41:\n",
      "Anomaly index: 106\n",
      "etajTX: [[2.05995369]]\n",
      "p-value: 0.521117531515616\n",
      "TPR: 0.4634146341463415\n",
      "-------------------------------------------------\n",
      "iteration #42:\n",
      "Anomaly index: 107\n",
      "etajTX: [[2.83700773]]\n",
      "p-value: 0.10116187216273134\n",
      "TPR: 0.4523809523809524\n",
      "-------------------------------------------------\n",
      "iteration #43:\n",
      "Anomaly index: 108\n",
      "etajTX: [[2.15465376]]\n",
      "p-value: 0.19193150217075902\n",
      "TPR: 0.4418604651162791\n",
      "-------------------------------------------------\n",
      "iteration #44:\n",
      "Anomaly index: 100\n",
      "etajTX: [[2.07567088]]\n",
      "p-value: 0.3745164197015306\n",
      "TPR: 0.4318181818181818\n",
      "-------------------------------------------------\n",
      "iteration #45:\n",
      "Anomaly index: 106\n",
      "etajTX: [[3.09183735]]\n",
      "p-value: 0.1594259056893849\n",
      "TPR: 0.4222222222222222\n",
      "-------------------------------------------------\n",
      "iteration #46:\n",
      "Anomaly index: 103\n",
      "etajTX: [[3.65557437]]\n",
      "p-value: 0.0042179537472636286\n",
      "TPR: 0.43478260869565216\n",
      "-------------------------------------------------\n",
      "iteration #47:\n",
      "Anomaly index: 107\n",
      "etajTX: [[4.12461048]]\n",
      "p-value: 0.0013292846722247288\n",
      "TPR: 0.44680851063829785\n",
      "-------------------------------------------------\n",
      "iteration #48:\n",
      "Anomaly index: 100\n",
      "etajTX: [[2.19543881]]\n",
      "p-value: 0.22164398573917976\n",
      "TPR: 0.4375\n",
      "-------------------------------------------------\n",
      "iteration #49:\n",
      "Anomaly index: 105\n",
      "etajTX: [[1.09668562]]\n",
      "p-value: 0.054760364285691504\n",
      "TPR: 0.42857142857142855\n",
      "-------------------------------------------------\n",
      "iteration #50:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.60315069]]\n",
      "p-value: 0.021517071665371073\n",
      "TPR: 0.44\n",
      "-------------------------------------------------\n",
      "iteration #51:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.41154755]]\n",
      "p-value: 0.010271679937876827\n",
      "TPR: 0.45098039215686275\n",
      "-------------------------------------------------\n",
      "iteration #52:\n",
      "Anomaly index: 101\n",
      "etajTX: [[3.94247182]]\n",
      "p-value: 0.006847727445626462\n",
      "TPR: 0.46153846153846156\n",
      "-------------------------------------------------\n",
      "iteration #53:\n",
      "Anomaly index: 100\n",
      "etajTX: [[2.26834488]]\n",
      "p-value: 0.17737642820075147\n",
      "TPR: 0.4528301886792453\n",
      "-------------------------------------------------\n",
      "iteration #54:\n",
      "Anomaly index: 106\n",
      "etajTX: [[5.10957209]]\n",
      "p-value: 4.849130793913556e-05\n",
      "TPR: 0.46296296296296297\n",
      "-------------------------------------------------\n",
      "iteration #55:\n",
      "Anomaly index: 101\n",
      "etajTX: [[2.73420652]]\n",
      "p-value: 0.1527998708637271\n",
      "TPR: 0.45454545454545453\n",
      "-------------------------------------------------\n",
      "iteration #56:\n",
      "Anomaly index: 108\n",
      "etajTX: [[4.27546162]]\n",
      "p-value: 0.00021309160454659613\n",
      "TPR: 0.4642857142857143\n",
      "-------------------------------------------------\n",
      "iteration #57:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.27275043]]\n",
      "p-value: 0.032719531171306\n",
      "TPR: 0.47368421052631576\n",
      "-------------------------------------------------\n",
      "iteration #58:\n",
      "Anomaly index: 102\n",
      "etajTX: [[2.79598469]]\n",
      "p-value: 0.06198224062842315\n",
      "TPR: 0.46551724137931033\n",
      "-------------------------------------------------\n",
      "iteration #59:\n",
      "Anomaly index: 102\n",
      "etajTX: [[2.31498951]]\n",
      "p-value: 0.5010121280757002\n",
      "TPR: 0.4576271186440678\n",
      "-------------------------------------------------\n",
      "iteration #60:\n",
      "Anomaly index: 107\n",
      "etajTX: [[3.16446008]]\n",
      "p-value: 0.05550399692635811\n",
      "TPR: 0.45\n",
      "-------------------------------------------------\n",
      "iteration #61:\n",
      "Anomaly index: 102\n",
      "etajTX: [[2.34120602]]\n",
      "p-value: 0.13069644346324474\n",
      "TPR: 0.4426229508196721\n",
      "-------------------------------------------------\n",
      "iteration #62:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.77154032]]\n",
      "p-value: 0.0031658922640749616\n",
      "TPR: 0.45161290322580644\n",
      "-------------------------------------------------\n",
      "iteration #63:\n",
      "Anomaly index: 103\n",
      "etajTX: [[2.87295575]]\n",
      "p-value: 0.3525622151946571\n",
      "TPR: 0.4444444444444444\n",
      "-------------------------------------------------\n",
      "iteration #64:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.58719444]]\n",
      "p-value: 0.059086654861800765\n",
      "TPR: 0.4375\n",
      "-------------------------------------------------\n",
      "iteration #65:\n",
      "Anomaly index: 109\n",
      "etajTX: [[5.0677149]]\n",
      "p-value: 2.3794072936295563e-05\n",
      "TPR: 0.4461538461538462\n",
      "-------------------------------------------------\n",
      "iteration #66:\n",
      "Anomaly index: 109\n",
      "etajTX: [[2.60342725]]\n",
      "p-value: 0.12458841600205971\n",
      "TPR: 0.4393939393939394\n",
      "-------------------------------------------------\n",
      "iteration #67:\n",
      "Anomaly index: 109\n",
      "etajTX: [[2.45898416]]\n",
      "p-value: 0.14909460628834167\n",
      "TPR: 0.43283582089552236\n",
      "-------------------------------------------------\n",
      "iteration #68:\n",
      "Anomaly index: 101\n",
      "etajTX: [[2.01259338]]\n",
      "p-value: 0.5530596652630299\n",
      "TPR: 0.4264705882352941\n",
      "-------------------------------------------------\n",
      "iteration #69:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.45672841]]\n",
      "p-value: 0.3815917312069131\n",
      "TPR: 0.42028985507246375\n",
      "-------------------------------------------------\n",
      "iteration #70:\n",
      "Anomaly index: 102\n",
      "etajTX: [[2.59995121]]\n",
      "p-value: 0.7150402906941954\n",
      "TPR: 0.4142857142857143\n",
      "-------------------------------------------------\n",
      "iteration #71:\n",
      "Anomaly index: 109\n",
      "etajTX: [[3.1306377]]\n",
      "p-value: 0.022032788627683042\n",
      "TPR: 0.4225352112676056\n",
      "-------------------------------------------------\n",
      "iteration #72:\n",
      "Anomaly index: 104\n",
      "etajTX: [[4.23361863]]\n",
      "p-value: 0.00031777536851344465\n",
      "TPR: 0.4305555555555556\n",
      "-------------------------------------------------\n",
      "iteration #73:\n",
      "Anomaly index: 109\n",
      "etajTX: [[3.5069205]]\n",
      "p-value: 0.009894362270034622\n",
      "TPR: 0.4383561643835616\n",
      "-------------------------------------------------\n",
      "iteration #74:\n",
      "Anomaly index: 109\n",
      "etajTX: [[3.55822584]]\n",
      "p-value: 0.0036342103008113126\n",
      "TPR: 0.44594594594594594\n",
      "-------------------------------------------------\n",
      "iteration #75:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.12814416]]\n",
      "p-value: 0.015668075661098187\n",
      "TPR: 0.4533333333333333\n",
      "-------------------------------------------------\n",
      "iteration #76:\n",
      "Anomaly index: 108\n",
      "etajTX: [[3.21169811]]\n",
      "p-value: 0.012363019289962193\n",
      "TPR: 0.4605263157894737\n",
      "-------------------------------------------------\n",
      "iteration #77:\n",
      "Anomaly index: 109\n",
      "etajTX: [[2.71104749]]\n",
      "p-value: 0.05400085806876581\n",
      "TPR: 0.45454545454545453\n",
      "-------------------------------------------------\n",
      "iteration #78:\n",
      "Anomaly index: 103\n",
      "etajTX: [[2.44589763]]\n",
      "p-value: 0.7085687169849634\n",
      "TPR: 0.44871794871794873\n",
      "-------------------------------------------------\n",
      "iteration #79:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.85342895]]\n",
      "p-value: 0.1360417277915504\n",
      "TPR: 0.4430379746835443\n",
      "-------------------------------------------------\n",
      "iteration #80:\n",
      "Anomaly index: 106\n",
      "etajTX: [[1.75722652]]\n",
      "p-value: 0.1077486629462764\n",
      "TPR: 0.4375\n",
      "-------------------------------------------------\n",
      "iteration #81:\n",
      "Anomaly index: 100\n",
      "etajTX: [[3.57697487]]\n",
      "p-value: 0.01880682729901411\n",
      "TPR: 0.4444444444444444\n",
      "-------------------------------------------------\n",
      "iteration #82:\n",
      "Anomaly index: 103\n",
      "etajTX: [[3.27419133]]\n",
      "p-value: 0.009552710878418675\n",
      "TPR: 0.45121951219512196\n",
      "-------------------------------------------------\n",
      "iteration #83:\n",
      "Anomaly index: 102\n",
      "etajTX: [[3.97281604]]\n",
      "p-value: 0.0006570575124974415\n",
      "TPR: 0.4578313253012048\n",
      "-------------------------------------------------\n",
      "iteration #84:\n",
      "Anomaly index: 106\n",
      "etajTX: [[2.63636038]]\n",
      "p-value: 0.13233814816474473\n",
      "TPR: 0.4523809523809524\n",
      "-------------------------------------------------\n",
      "iteration #85:\n",
      "Anomaly index: 101\n",
      "etajTX: [[3.64522743]]\n",
      "p-value: 0.0065707840971025505\n",
      "TPR: 0.4588235294117647\n",
      "-------------------------------------------------\n",
      "iteration #86:\n",
      "Anomaly index: 109\n",
      "etajTX: [[3.3292745]]\n",
      "p-value: 0.025235133138317607\n",
      "TPR: 0.46511627906976744\n",
      "-------------------------------------------------\n",
      "iteration #87:\n",
      "Anomaly index: 100\n",
      "etajTX: [[3.10164049]]\n",
      "p-value: 0.03621864379956894\n",
      "TPR: 0.47126436781609193\n",
      "-------------------------------------------------\n",
      "iteration #88:\n",
      "Anomaly index: 102\n",
      "etajTX: [[2.08156035]]\n",
      "p-value: 0.3743834255030607\n",
      "TPR: 0.4659090909090909\n",
      "-------------------------------------------------\n",
      "iteration #89:\n",
      "Anomaly index: 109\n",
      "etajTX: [[3.36257617]]\n",
      "p-value: 0.007816757390096959\n",
      "TPR: 0.47191011235955055\n",
      "-------------------------------------------------\n",
      "iteration #90:\n",
      "Anomaly index: 108\n",
      "etajTX: [[3.28058815]]\n",
      "p-value: 0.818627694175369\n",
      "TPR: 0.4666666666666667\n",
      "-------------------------------------------------\n",
      "iteration #91:\n",
      "Anomaly index: 100\n",
      "etajTX: [[3.73851331]]\n",
      "p-value: 0.007496521486449656\n",
      "TPR: 0.4725274725274725\n",
      "-------------------------------------------------\n",
      "iteration #92:\n",
      "Anomaly index: 101\n",
      "etajTX: [[3.75979063]]\n",
      "p-value: 0.001070483670979089\n",
      "TPR: 0.4782608695652174\n",
      "-------------------------------------------------\n",
      "iteration #93:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.55988948]]\n",
      "p-value: 0.003554366105203974\n",
      "TPR: 0.4838709677419355\n",
      "-------------------------------------------------\n",
      "iteration #94:\n",
      "Anomaly index: 108\n",
      "etajTX: [[2.92268774]]\n",
      "p-value: 0.05787357261413066\n",
      "TPR: 0.4787234042553192\n",
      "-------------------------------------------------\n",
      "iteration #95:\n",
      "Anomaly index: 103\n",
      "etajTX: [[2.15830527]]\n",
      "p-value: 0.29734209972185716\n",
      "TPR: 0.47368421052631576\n",
      "-------------------------------------------------\n",
      "iteration #96:\n",
      "Anomaly index: 108\n",
      "etajTX: [[3.36138535]]\n",
      "p-value: 0.011954958100937933\n",
      "TPR: 0.4791666666666667\n",
      "-------------------------------------------------\n",
      "iteration #97:\n",
      "Anomaly index: 109\n",
      "etajTX: [[4.47204145]]\n",
      "p-value: 0.00033192817433797943\n",
      "TPR: 0.4845360824742268\n",
      "-------------------------------------------------\n",
      "iteration #98:\n",
      "Anomaly index: 102\n",
      "etajTX: [[1.93798235]]\n",
      "p-value: 0.9724546305882259\n",
      "TPR: 0.47959183673469385\n",
      "-------------------------------------------------\n",
      "iteration #99:\n",
      "Anomaly index: 101\n",
      "etajTX: [[3.85651864]]\n",
      "p-value: 0.002339810966534643\n",
      "TPR: 0.48484848484848486\n",
      "-------------------------------------------------\n",
      "iteration #100:\n",
      "Anomaly index: 102\n",
      "etajTX: [[3.89199999]]\n",
      "p-value: 0.05494168615373717\n",
      "TPR: 0.48\n",
      "-------------------------------------------------\n",
      "iteration #101:\n",
      "Anomaly index: 106\n",
      "etajTX: [[3.25101407]]\n",
      "p-value: 0.021865515169438462\n",
      "TPR: 0.48514851485148514\n",
      "-------------------------------------------------\n",
      "iteration #102:\n",
      "Anomaly index: 100\n",
      "etajTX: [[2.99919404]]\n",
      "p-value: 0.060667528202442526\n",
      "TPR: 0.4803921568627451\n",
      "-------------------------------------------------\n",
      "iteration #103:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.98400794]]\n",
      "p-value: 0.03509283019115983\n",
      "TPR: 0.4854368932038835\n",
      "-------------------------------------------------\n",
      "iteration #104:\n",
      "Anomaly index: 101\n",
      "etajTX: [[2.18534915]]\n",
      "p-value: 0.18915866397754963\n",
      "TPR: 0.4807692307692308\n",
      "-------------------------------------------------\n",
      "iteration #105:\n",
      "Anomaly index: 101\n",
      "etajTX: [[1.48125161]]\n",
      "p-value: 0.9421088239592139\n",
      "TPR: 0.47619047619047616\n",
      "-------------------------------------------------\n",
      "iteration #106:\n",
      "Anomaly index: 102\n",
      "etajTX: [[3.39540397]]\n",
      "p-value: 0.00751190821631329\n",
      "TPR: 0.4811320754716981\n",
      "-------------------------------------------------\n",
      "iteration #107:\n",
      "Anomaly index: 109\n",
      "etajTX: [[3.42216576]]\n",
      "p-value: 0.012013705767944858\n",
      "TPR: 0.48598130841121495\n",
      "-------------------------------------------------\n",
      "iteration #108:\n",
      "Anomaly index: 108\n",
      "etajTX: [[3.84842936]]\n",
      "p-value: 0.032468434599048905\n",
      "TPR: 0.49074074074074076\n",
      "-------------------------------------------------\n",
      "iteration #109:\n",
      "Anomaly index: 106\n",
      "etajTX: [[3.1970325]]\n",
      "p-value: 0.019532372868895517\n",
      "TPR: 0.4954128440366973\n",
      "-------------------------------------------------\n",
      "iteration #110:\n",
      "Anomaly index: 101\n",
      "etajTX: [[2.73040496]]\n",
      "p-value: 0.18292887051399936\n",
      "TPR: 0.4909090909090909\n",
      "-------------------------------------------------\n",
      "iteration #111:\n",
      "Anomaly index: 100\n",
      "etajTX: [[2.45322461]]\n",
      "p-value: 0.35393891135200894\n",
      "TPR: 0.4864864864864865\n",
      "-------------------------------------------------\n",
      "iteration #112:\n",
      "Anomaly index: 107\n",
      "etajTX: [[4.70409012]]\n",
      "p-value: 5.0537398426343276e-05\n",
      "TPR: 0.49107142857142855\n",
      "-------------------------------------------------\n",
      "iteration #113:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.27021175]]\n",
      "p-value: 0.5808994713177419\n",
      "TPR: 0.48672566371681414\n",
      "-------------------------------------------------\n",
      "iteration #114:\n",
      "Anomaly index: 101\n",
      "etajTX: [[2.91669549]]\n",
      "p-value: 0.22330806450915486\n",
      "TPR: 0.4824561403508772\n",
      "-------------------------------------------------\n",
      "iteration #115:\n",
      "Anomaly index: 104\n",
      "etajTX: [[1.86328549]]\n",
      "p-value: 0.5253028937285221\n",
      "TPR: 0.4782608695652174\n",
      "-------------------------------------------------\n",
      "iteration #116:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.41936853]]\n",
      "p-value: 0.00933927797552652\n",
      "TPR: 0.4827586206896552\n",
      "-------------------------------------------------\n",
      "iteration #117:\n",
      "Anomaly index: 102\n",
      "etajTX: [[3.30435011]]\n",
      "p-value: 0.030316393488666788\n",
      "TPR: 0.48717948717948717\n",
      "-------------------------------------------------\n",
      "iteration #118:\n",
      "Anomaly index: 107\n",
      "etajTX: [[5.07211537]]\n",
      "p-value: 8.919008007182488e-06\n",
      "TPR: 0.4915254237288136\n",
      "-------------------------------------------------\n",
      "iteration #119:\n",
      "Anomaly index: 109\n",
      "etajTX: [[2.81684049]]\n",
      "p-value: 0.04005078730294281\n",
      "TPR: 0.4957983193277311\n",
      "-------------------------------------------------\n",
      "iteration #120:\n",
      "Anomaly index: 103\n",
      "etajTX: [[3.26602427]]\n",
      "p-value: 0.035659081495590314\n",
      "TPR: 0.5\n",
      "-------------------------------------------------\n",
      "iteration #121:\n",
      "Anomaly index: 107\n",
      "etajTX: [[1.71545813]]\n",
      "p-value: 0.6292616764523671\n",
      "TPR: 0.49586776859504134\n",
      "-------------------------------------------------\n",
      "iteration #122:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "max_iteration = 120\n",
    "alpha = 0.05\n",
    "list_p_value = []\n",
    "count = 0\n",
    "print(f'iteration #{0}:')\n",
    "\n",
    "\n",
    "while len(list_p_value) <= max_iteration:\n",
    "    p_value = run_tpr()\n",
    "    if p_value is None:\n",
    "        continue\n",
    "    list_p_value.append(p_value)\n",
    "    if p_value <= alpha:\n",
    "        count += 1\n",
    "    print(f'TPR: {count / len(list_p_value)}')\n",
    "    print('-------------------------------------------------')\n",
    "    print(f'iteration #{len(list_p_value)+1}:')\n",
    "plt.hist(list_p_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fpr():\n",
    "    ns, nt, d = 100, 10, 1\n",
    "    mu_s, mu_t = 0, 20\n",
    "    delta_s, delta_t = [4], [0]\n",
    "    xs, ys = gen_data(mu_s, delta_s, ns, d)\n",
    "    xt, yt = gen_data(mu_s, delta_t, nt, d)\n",
    "\n",
    "    xs = torch.FloatTensor(xs)\n",
    "    ys = torch.LongTensor(ys)\n",
    "    xt = torch.FloatTensor(xt)\n",
    "    yt = torch.LongTensor(yt)\n",
    "\n",
    "    xs_hat = model.extract_feature(xs.cuda())\n",
    "    xt_hat = model.extract_feature(xt.cuda())\n",
    "    x_hat = torch.cat([xs_hat, xt_hat], dim=0)\n",
    "\n",
    "    xs_hat = xs_hat.cpu()\n",
    "    xt_hat = xt_hat.cpu()\n",
    "    x_hat = x_hat.cpu()\n",
    "    xs = xs.cpu()\n",
    "    xt = xt.cpu()\n",
    "    ys = ys.cpu()\n",
    "    yt = yt.cpu()\n",
    "    \n",
    "    O = max_sum(x_hat.numpy())\n",
    "    if (O < 100):\n",
    "        return None\n",
    "    else:\n",
    "        O = [O - 100]   \n",
    "\n",
    "    Oc = list(np.where(yt == 0)[0])\n",
    "    j = np.random.choice(O, 1, replace=False)[0]\n",
    "    etj = np.zeros((nt, 1))\n",
    "    etj[j][0] = 1\n",
    "    etOc = np.zeros((nt, 1))\n",
    "    etOc[Oc] = 1\n",
    "    etaj = np.vstack((np.zeros((ns, 1)), etj-(1/len(Oc))*etOc))\n",
    "    X = np.vstack((xs.numpy(), xt.numpy()))\n",
    "\n",
    "    etajTX = etaj.T.dot(X)\n",
    "    print(f'Anomaly index: {O[0] + ns}')\n",
    "    print(f'etajTX: {etajTX}')\n",
    "    mu = np.vstack((np.full((ns,1), mu_s), np.full((nt,1), mu_t)))\n",
    "    sigma = np.identity(ns+nt)\n",
    "    etajTmu = etaj.T.dot(mu)\n",
    "    etajTsigmaetaj = etaj.T.dot(sigma).dot(etaj)\n",
    "    \n",
    "    b = sigma.dot(etaj).dot(np.linalg.inv(etajTsigmaetaj))\n",
    "    a = (np.identity(ns+nt) - b.dot(etaj.T)).dot(X)\n",
    "    \n",
    "\n",
    "    itv = [np.NINF, np.inf]\n",
    "    for i in range(X.shape[0]):\n",
    "        itv = intersect(itv, get_interval(X[i].reshape(-1, 1), a[i].reshape(-1, 1), b[i].reshape(-1, 1))[0])\n",
    "\n",
    "    sub_itv = [np.NINF, np.inf]\n",
    "    _, uo, vo = get_interval(X[O[0]+100].reshape(-1, 1), a[O[0]+100].reshape(-1, 1), b[O[0]+100].reshape(-1, 1))\n",
    "    I = np.ones((x_hat.shape[1],1))\n",
    "    for i in range(X.shape[0]):\n",
    "        if (i != O[0]+100):\n",
    "            _, ui, vi = get_interval(X[i].reshape(-1, 1), a[i].reshape(-1, 1), b[i].reshape(-1, 1))\n",
    "            u = uo - ui\n",
    "            v = vo - vi \n",
    "            u = I.T.dot(u)[0][0]\n",
    "            v = I.T.dot(v)[0][0]\n",
    "            sub__itv = solve_linear_inequality(-u, -v)\n",
    "            sub_itv = intersect(sub_itv, sub__itv)\n",
    "    itv = intersect(itv, sub_itv)\n",
    "\n",
    "    cdf = truncated_cdf(etajTX[0][0], etajTmu[0][0], np.sqrt(etajTsigmaetaj[0][0]), itv[0], itv[1])\n",
    "    p_value = float(2 * min(cdf, 1 - cdf))\n",
    "    print(f'p-value: {p_value}')\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #0:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-2.01561238]]\n",
      "p-value: 0.9651486068531854\n",
      "FPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #2:\n",
      "Anomaly index: 103\n",
      "etajTX: [[-2.26867177]]\n",
      "p-value: 0.18424115880134945\n",
      "FPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #3:\n",
      "Anomaly index: 103\n",
      "etajTX: [[-2.16556077]]\n",
      "p-value: 0.7381187539354411\n",
      "FPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #4:\n",
      "Anomaly index: 108\n",
      "etajTX: [[-2.08091371]]\n",
      "p-value: 0.5814398450326788\n",
      "FPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #5:\n",
      "Anomaly index: 106\n",
      "etajTX: [[-1.78592258]]\n",
      "p-value: 0.2693260857635579\n",
      "FPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #6:\n",
      "Anomaly index: 100\n",
      "etajTX: [[-2.65401466]]\n",
      "p-value: 0.6542002014877414\n",
      "FPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #7:\n",
      "Anomaly index: 104\n",
      "etajTX: [[-1.89963134]]\n",
      "p-value: 0.5786668422320682\n",
      "FPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #8:\n",
      "Anomaly index: 108\n",
      "etajTX: [[-1.8503005]]\n",
      "p-value: 0.4394121906293858\n",
      "FPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #9:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-1.96790093]]\n",
      "p-value: 0.6452077578092317\n",
      "FPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #10:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-2.13144234]]\n",
      "p-value: 0.589115319302478\n",
      "FPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #11:\n",
      "Anomaly index: 104\n",
      "etajTX: [[-1.6566103]]\n",
      "p-value: 0.3176692584329481\n",
      "FPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #12:\n",
      "Anomaly index: 105\n",
      "etajTX: [[-2.80210297]]\n",
      "p-value: 0.19374855611999262\n",
      "FPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #13:\n",
      "Anomaly index: 107\n",
      "etajTX: [[-2.3049094]]\n",
      "p-value: 0.6217840842170507\n",
      "FPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #14:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-2.37550144]]\n",
      "p-value: 0.002695479337363425\n",
      "FPR: 0.07142857142857142\n",
      "-------------------------------------------------\n",
      "iteration #15:\n",
      "Anomaly index: 109\n",
      "etajTX: [[-2.22969525]]\n",
      "p-value: 0.02933728802699726\n",
      "FPR: 0.13333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #16:\n",
      "Anomaly index: 101\n",
      "etajTX: [[-1.6595511]]\n",
      "p-value: 0.5266673265981194\n",
      "FPR: 0.125\n",
      "-------------------------------------------------\n",
      "iteration #17:\n",
      "Anomaly index: 103\n",
      "etajTX: [[-2.65397513]]\n",
      "p-value: 0.35927168643302276\n",
      "FPR: 0.11764705882352941\n",
      "-------------------------------------------------\n",
      "iteration #18:\n",
      "Anomaly index: 101\n",
      "etajTX: [[-2.42769281]]\n",
      "p-value: 0.7753617544640821\n",
      "FPR: 0.1111111111111111\n",
      "-------------------------------------------------\n",
      "iteration #19:\n",
      "Anomaly index: 101\n",
      "etajTX: [[-2.17472343]]\n",
      "p-value: 0.8507118169068152\n",
      "FPR: 0.10526315789473684\n",
      "-------------------------------------------------\n",
      "iteration #20:\n",
      "Anomaly index: 105\n",
      "etajTX: [[-2.5236]]\n",
      "p-value: 0.6158830432330646\n",
      "FPR: 0.1\n",
      "-------------------------------------------------\n",
      "iteration #21:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-3.08794824]]\n",
      "p-value: 0.06722107604846\n",
      "FPR: 0.09523809523809523\n",
      "-------------------------------------------------\n",
      "iteration #22:\n",
      "Anomaly index: 101\n",
      "etajTX: [[-2.66707352]]\n",
      "p-value: 0.502906635578831\n",
      "FPR: 0.09090909090909091\n",
      "-------------------------------------------------\n",
      "iteration #23:\n",
      "Anomaly index: 101\n",
      "etajTX: [[-2.73345382]]\n",
      "p-value: 0.27569073269767463\n",
      "FPR: 0.08695652173913043\n",
      "-------------------------------------------------\n",
      "iteration #24:\n",
      "Anomaly index: 106\n",
      "etajTX: [[-2.45287272]]\n",
      "p-value: 0.22148462856479392\n",
      "FPR: 0.08333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #25:\n",
      "Anomaly index: 103\n",
      "etajTX: [[-1.97633713]]\n",
      "p-value: 0.8227319584010738\n",
      "FPR: 0.08\n",
      "-------------------------------------------------\n",
      "iteration #26:\n",
      "Anomaly index: 108\n",
      "etajTX: [[-2.23691435]]\n",
      "p-value: 0.8018043353785869\n",
      "FPR: 0.07692307692307693\n",
      "-------------------------------------------------\n",
      "iteration #27:\n",
      "Anomaly index: 106\n",
      "etajTX: [[-1.57558844]]\n",
      "p-value: 0.7566455248436177\n",
      "FPR: 0.07407407407407407\n",
      "-------------------------------------------------\n",
      "iteration #28:\n",
      "Anomaly index: 106\n",
      "etajTX: [[-2.09866562]]\n",
      "p-value: 0.7320669759086262\n",
      "FPR: 0.07142857142857142\n",
      "-------------------------------------------------\n",
      "iteration #29:\n",
      "Anomaly index: 103\n",
      "etajTX: [[-2.90076588]]\n",
      "p-value: 0.19901079978507957\n",
      "FPR: 0.06896551724137931\n",
      "-------------------------------------------------\n",
      "iteration #30:\n",
      "Anomaly index: 106\n",
      "etajTX: [[-3.20320263]]\n",
      "p-value: 0.03373730348918912\n",
      "FPR: 0.1\n",
      "-------------------------------------------------\n",
      "iteration #31:\n",
      "Anomaly index: 109\n",
      "etajTX: [[-2.12462218]]\n",
      "p-value: 0.3496939947934061\n",
      "FPR: 0.0967741935483871\n",
      "-------------------------------------------------\n",
      "iteration #32:\n",
      "Anomaly index: 109\n",
      "etajTX: [[-2.23983054]]\n",
      "p-value: 0.15287631205819072\n",
      "FPR: 0.09375\n",
      "-------------------------------------------------\n",
      "iteration #33:\n",
      "Anomaly index: 107\n",
      "etajTX: [[-1.70986608]]\n",
      "p-value: 0.4966224654955496\n",
      "FPR: 0.09090909090909091\n",
      "-------------------------------------------------\n",
      "iteration #34:\n",
      "Anomaly index: 100\n",
      "etajTX: [[-1.90680924]]\n",
      "p-value: 0.9915739056837138\n",
      "FPR: 0.08823529411764706\n",
      "-------------------------------------------------\n",
      "iteration #35:\n",
      "Anomaly index: 104\n",
      "etajTX: [[-2.40093666]]\n",
      "p-value: 0.9885920960273106\n",
      "FPR: 0.08571428571428572\n",
      "-------------------------------------------------\n",
      "iteration #36:\n",
      "Anomaly index: 100\n",
      "etajTX: [[-2.37409392]]\n",
      "p-value: 0.47460855533751634\n",
      "FPR: 0.08333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #37:\n",
      "Anomaly index: 100\n",
      "etajTX: [[-2.48704281]]\n",
      "p-value: 0.11946630463572942\n",
      "FPR: 0.08108108108108109\n",
      "-------------------------------------------------\n",
      "iteration #38:\n",
      "Anomaly index: 101\n",
      "etajTX: [[-3.85939588]]\n",
      "p-value: 0.1810405019185854\n",
      "FPR: 0.07894736842105263\n",
      "-------------------------------------------------\n",
      "iteration #39:\n",
      "Anomaly index: 104\n",
      "etajTX: [[-2.33183094]]\n",
      "p-value: 0.19490734675565866\n",
      "FPR: 0.07692307692307693\n",
      "-------------------------------------------------\n",
      "iteration #40:\n",
      "Anomaly index: 107\n",
      "etajTX: [[-2.95665705]]\n",
      "p-value: 0.7218416257105403\n",
      "FPR: 0.075\n",
      "-------------------------------------------------\n",
      "iteration #41:\n",
      "Anomaly index: 106\n",
      "etajTX: [[-2.49284345]]\n",
      "p-value: 0.7386437421041258\n",
      "FPR: 0.07317073170731707\n",
      "-------------------------------------------------\n",
      "iteration #42:\n",
      "Anomaly index: 106\n",
      "etajTX: [[-1.96718054]]\n",
      "p-value: 0.1869464057999493\n",
      "FPR: 0.07142857142857142\n",
      "-------------------------------------------------\n",
      "iteration #43:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-2.72531359]]\n",
      "p-value: 0.5102451134771363\n",
      "FPR: 0.06976744186046512\n",
      "-------------------------------------------------\n",
      "iteration #44:\n",
      "Anomaly index: 109\n",
      "etajTX: [[-2.0980291]]\n",
      "p-value: 0.43841702576067265\n",
      "FPR: 0.06818181818181818\n",
      "-------------------------------------------------\n",
      "iteration #45:\n",
      "Anomaly index: 109\n",
      "etajTX: [[-2.29349336]]\n",
      "p-value: 0.6457519406393337\n",
      "FPR: 0.06666666666666667\n",
      "-------------------------------------------------\n",
      "iteration #46:\n",
      "Anomaly index: 105\n",
      "etajTX: [[-2.2497747]]\n",
      "p-value: 0.555029517833083\n",
      "FPR: 0.06521739130434782\n",
      "-------------------------------------------------\n",
      "iteration #47:\n",
      "Anomaly index: 100\n",
      "etajTX: [[-3.07903502]]\n",
      "p-value: 0.5558748720235459\n",
      "FPR: 0.06382978723404255\n",
      "-------------------------------------------------\n",
      "iteration #48:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-2.92582263]]\n",
      "p-value: 0.1862447087216317\n",
      "FPR: 0.0625\n",
      "-------------------------------------------------\n",
      "iteration #49:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-1.86347487]]\n",
      "p-value: 0.5117903340001454\n",
      "FPR: 0.061224489795918366\n",
      "-------------------------------------------------\n",
      "iteration #50:\n",
      "Anomaly index: 101\n",
      "etajTX: [[-2.32187103]]\n",
      "p-value: 0.7004188052815946\n",
      "FPR: 0.06\n",
      "-------------------------------------------------\n",
      "iteration #51:\n",
      "Anomaly index: 104\n",
      "etajTX: [[-3.38344954]]\n",
      "p-value: 0.39148109239365997\n",
      "FPR: 0.058823529411764705\n",
      "-------------------------------------------------\n",
      "iteration #52:\n",
      "Anomaly index: 107\n",
      "etajTX: [[-2.11849257]]\n",
      "p-value: 0.49298411927556934\n",
      "FPR: 0.057692307692307696\n",
      "-------------------------------------------------\n",
      "iteration #53:\n",
      "Anomaly index: 104\n",
      "etajTX: [[-1.65787254]]\n",
      "p-value: 0.61261785354436\n",
      "FPR: 0.05660377358490566\n",
      "-------------------------------------------------\n",
      "iteration #54:\n",
      "Anomaly index: 106\n",
      "etajTX: [[-2.46626767]]\n",
      "p-value: 0.6885527534730872\n",
      "FPR: 0.05555555555555555\n",
      "-------------------------------------------------\n",
      "iteration #55:\n",
      "Anomaly index: 103\n",
      "etajTX: [[-2.14133292]]\n",
      "p-value: 0.7015611358849616\n",
      "FPR: 0.05454545454545454\n",
      "-------------------------------------------------\n",
      "iteration #56:\n",
      "Anomaly index: 101\n",
      "etajTX: [[-2.0630801]]\n",
      "p-value: 0.9640102512740668\n",
      "FPR: 0.05357142857142857\n",
      "-------------------------------------------------\n",
      "iteration #57:\n",
      "Anomaly index: 108\n",
      "etajTX: [[-2.69353861]]\n",
      "p-value: 0.2427266155488933\n",
      "FPR: 0.05263157894736842\n",
      "-------------------------------------------------\n",
      "iteration #58:\n",
      "Anomaly index: 100\n",
      "etajTX: [[-1.72312095]]\n",
      "p-value: 0.7065694132081732\n",
      "FPR: 0.05172413793103448\n",
      "-------------------------------------------------\n",
      "iteration #59:\n",
      "Anomaly index: 103\n",
      "etajTX: [[-2.18056867]]\n",
      "p-value: 0.15658155198003662\n",
      "FPR: 0.05084745762711865\n",
      "-------------------------------------------------\n",
      "iteration #60:\n",
      "Anomaly index: 108\n",
      "etajTX: [[-2.34174785]]\n",
      "p-value: 0.757371819583006\n",
      "FPR: 0.05\n",
      "-------------------------------------------------\n",
      "iteration #61:\n",
      "Anomaly index: 100\n",
      "etajTX: [[-2.099483]]\n",
      "p-value: 0.7942117616444581\n",
      "FPR: 0.04918032786885246\n",
      "-------------------------------------------------\n",
      "iteration #62:\n",
      "Anomaly index: 107\n",
      "etajTX: [[-2.61444079]]\n",
      "p-value: 0.6477831629077906\n",
      "FPR: 0.04838709677419355\n",
      "-------------------------------------------------\n",
      "iteration #63:\n",
      "Anomaly index: 109\n",
      "etajTX: [[-1.69142333]]\n",
      "p-value: 0.2838908017238957\n",
      "FPR: 0.047619047619047616\n",
      "-------------------------------------------------\n",
      "iteration #64:\n",
      "Anomaly index: 104\n",
      "etajTX: [[-2.62828268]]\n",
      "p-value: 0.37193810875975364\n",
      "FPR: 0.046875\n",
      "-------------------------------------------------\n",
      "iteration #65:\n",
      "Anomaly index: 106\n",
      "etajTX: [[-2.49242519]]\n",
      "p-value: 0.9837573554810745\n",
      "FPR: 0.046153846153846156\n",
      "-------------------------------------------------\n",
      "iteration #66:\n",
      "Anomaly index: 103\n",
      "etajTX: [[-2.45621894]]\n",
      "p-value: 0.4603443818200564\n",
      "FPR: 0.045454545454545456\n",
      "-------------------------------------------------\n",
      "iteration #67:\n",
      "Anomaly index: 107\n",
      "etajTX: [[-2.64108387]]\n",
      "p-value: 0.3910564916288083\n",
      "FPR: 0.04477611940298507\n",
      "-------------------------------------------------\n",
      "iteration #68:\n",
      "Anomaly index: 104\n",
      "etajTX: [[-1.67060684]]\n",
      "p-value: 0.9526025170343166\n",
      "FPR: 0.04411764705882353\n",
      "-------------------------------------------------\n",
      "iteration #69:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-2.56430855]]\n",
      "p-value: 0.04989594286493088\n",
      "FPR: 0.057971014492753624\n",
      "-------------------------------------------------\n",
      "iteration #70:\n",
      "Anomaly index: 109\n",
      "etajTX: [[-1.9614719]]\n",
      "p-value: 0.579667316403041\n",
      "FPR: 0.05714285714285714\n",
      "-------------------------------------------------\n",
      "iteration #71:\n",
      "Anomaly index: 101\n",
      "etajTX: [[-2.50864013]]\n",
      "p-value: 0.9524428708643304\n",
      "FPR: 0.056338028169014086\n",
      "-------------------------------------------------\n",
      "iteration #72:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-2.21161635]]\n",
      "p-value: 0.35497865313383703\n",
      "FPR: 0.05555555555555555\n",
      "-------------------------------------------------\n",
      "iteration #73:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-2.5449489]]\n",
      "p-value: 0.6733631588788792\n",
      "FPR: 0.0547945205479452\n",
      "-------------------------------------------------\n",
      "iteration #74:\n",
      "Anomaly index: 104\n",
      "etajTX: [[-2.14460891]]\n",
      "p-value: 0.36268199153246694\n",
      "FPR: 0.05405405405405406\n",
      "-------------------------------------------------\n",
      "iteration #75:\n",
      "Anomaly index: 101\n",
      "etajTX: [[-1.90449341]]\n",
      "p-value: 0.6577012170215951\n",
      "FPR: 0.05333333333333334\n",
      "-------------------------------------------------\n",
      "iteration #76:\n",
      "Anomaly index: 108\n",
      "etajTX: [[-2.58692808]]\n",
      "p-value: 0.6460721252398478\n",
      "FPR: 0.05263157894736842\n",
      "-------------------------------------------------\n",
      "iteration #77:\n",
      "Anomaly index: 104\n",
      "etajTX: [[-1.89581476]]\n",
      "p-value: 0.5148192663760934\n",
      "FPR: 0.05194805194805195\n",
      "-------------------------------------------------\n",
      "iteration #78:\n",
      "Anomaly index: 106\n",
      "etajTX: [[-2.43925601]]\n",
      "p-value: 0.6609786210923578\n",
      "FPR: 0.05128205128205128\n",
      "-------------------------------------------------\n",
      "iteration #79:\n",
      "Anomaly index: 109\n",
      "etajTX: [[-2.52436034]]\n",
      "p-value: 0.5836785306983621\n",
      "FPR: 0.05063291139240506\n",
      "-------------------------------------------------\n",
      "iteration #80:\n",
      "Anomaly index: 106\n",
      "etajTX: [[-2.34087541]]\n",
      "p-value: 0.9218047460528032\n",
      "FPR: 0.05\n",
      "-------------------------------------------------\n",
      "iteration #81:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-1.34414804]]\n",
      "p-value: 0.4952827275690274\n",
      "FPR: 0.04938271604938271\n",
      "-------------------------------------------------\n",
      "iteration #82:\n",
      "Anomaly index: 101\n",
      "etajTX: [[-2.67293674]]\n",
      "p-value: 0.09072372249671662\n",
      "FPR: 0.04878048780487805\n",
      "-------------------------------------------------\n",
      "iteration #83:\n",
      "Anomaly index: 105\n",
      "etajTX: [[-3.22011906]]\n",
      "p-value: 0.29686327840276305\n",
      "FPR: 0.04819277108433735\n",
      "-------------------------------------------------\n",
      "iteration #84:\n",
      "Anomaly index: 105\n",
      "etajTX: [[-2.22217719]]\n",
      "p-value: 0.6764535814376759\n",
      "FPR: 0.047619047619047616\n",
      "-------------------------------------------------\n",
      "iteration #85:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-2.65601829]]\n",
      "p-value: 0.8499030848191631\n",
      "FPR: 0.047058823529411764\n",
      "-------------------------------------------------\n",
      "iteration #86:\n",
      "Anomaly index: 109\n",
      "etajTX: [[-2.73107085]]\n",
      "p-value: 0.7401053113270692\n",
      "FPR: 0.046511627906976744\n",
      "-------------------------------------------------\n",
      "iteration #87:\n",
      "Anomaly index: 105\n",
      "etajTX: [[-1.90341154]]\n",
      "p-value: 0.9844801188414937\n",
      "FPR: 0.04597701149425287\n",
      "-------------------------------------------------\n",
      "iteration #88:\n",
      "Anomaly index: 108\n",
      "etajTX: [[-2.08858281]]\n",
      "p-value: 0.22068808406864096\n",
      "FPR: 0.045454545454545456\n",
      "-------------------------------------------------\n",
      "iteration #89:\n",
      "Anomaly index: 109\n",
      "etajTX: [[-2.608266]]\n",
      "p-value: 0.1211169876668711\n",
      "FPR: 0.0449438202247191\n",
      "-------------------------------------------------\n",
      "iteration #90:\n",
      "Anomaly index: 105\n",
      "etajTX: [[-2.22948347]]\n",
      "p-value: 0.34629596174023386\n",
      "FPR: 0.044444444444444446\n",
      "-------------------------------------------------\n",
      "iteration #91:\n",
      "Anomaly index: 100\n",
      "etajTX: [[-2.88909553]]\n",
      "p-value: 0.9547962795500672\n",
      "FPR: 0.04395604395604396\n",
      "-------------------------------------------------\n",
      "iteration #92:\n",
      "Anomaly index: 101\n",
      "etajTX: [[-1.89230686]]\n",
      "p-value: 0.49896370991493566\n",
      "FPR: 0.043478260869565216\n",
      "-------------------------------------------------\n",
      "iteration #93:\n",
      "Anomaly index: 106\n",
      "etajTX: [[-2.85897212]]\n",
      "p-value: 0.49842625711406446\n",
      "FPR: 0.043010752688172046\n",
      "-------------------------------------------------\n",
      "iteration #94:\n",
      "Anomaly index: 102\n",
      "etajTX: [[-1.39268659]]\n",
      "p-value: 0.21335280302303164\n",
      "FPR: 0.0425531914893617\n",
      "-------------------------------------------------\n",
      "iteration #95:\n",
      "Anomaly index: 101\n",
      "etajTX: [[-3.0035323]]\n",
      "p-value: 0.6358087323810532\n",
      "FPR: 0.042105263157894736\n",
      "-------------------------------------------------\n",
      "iteration #96:\n",
      "Anomaly index: 108\n",
      "etajTX: [[-3.11683563]]\n",
      "p-value: 0.36390687547145034\n",
      "FPR: 0.041666666666666664\n",
      "-------------------------------------------------\n",
      "iteration #97:\n",
      "Anomaly index: 109\n",
      "etajTX: [[-2.36030027]]\n",
      "p-value: 0.8015954504653298\n",
      "FPR: 0.041237113402061855\n",
      "-------------------------------------------------\n",
      "iteration #98:\n",
      "Anomaly index: 106\n",
      "etajTX: [[-2.33275839]]\n",
      "p-value: 0.5383242851334037\n",
      "FPR: 0.04081632653061224\n",
      "-------------------------------------------------\n",
      "iteration #99:\n",
      "Anomaly index: 100\n",
      "etajTX: [[-1.37473906]]\n",
      "p-value: 0.39091169164028106\n",
      "FPR: 0.04040404040404041\n",
      "-------------------------------------------------\n",
      "iteration #100:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miteration #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_p_value) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_iteration:\n\u001b[1;32m---> 10\u001b[0m     p_value \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fpr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[146], line 8\u001b[0m, in \u001b[0;36mrun_fpr\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m xs, ys \u001b[38;5;241m=\u001b[39m gen_data(mu_s, delta_s, ns, d)\n\u001b[0;32m      6\u001b[0m xt, yt \u001b[38;5;241m=\u001b[39m gen_data(mu_s, delta_t, nt, d)\n\u001b[1;32m----> 8\u001b[0m xs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m ys \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(ys)\n\u001b[0;32m     10\u001b[0m xt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(xt)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "max_iteration = 120\n",
    "alpha = 0.05\n",
    "list_p_value = []\n",
    "count = 0\n",
    "print(f'iteration #{0}:')\n",
    "\n",
    "\n",
    "while len(list_p_value) <= max_iteration:\n",
    "    p_value = run_fpr()\n",
    "    if p_value is None:\n",
    "        continue\n",
    "    list_p_value.append(p_value)\n",
    "    if p_value <= alpha:\n",
    "        count += 1\n",
    "    print(f'FPR: {count / len(list_p_value)}')\n",
    "    print('-------------------------------------------------')\n",
    "    print(f'iteration #{len(list_p_value)+1}:')\n",
    "plt.hist(list_p_value)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
