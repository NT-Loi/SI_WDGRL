{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def gen_data(mu, delta, n, d: int = 2):\n",
    "    noise = np.random.normal(loc = 0, scale = 1, size=(n, d))\n",
    "    mu = np.full((n, d), mu, dtype=np.float64)\n",
    "\n",
    "    if len(delta) == 1 and delta[0] == 0:\n",
    "        return mu + noise, np.zeros(n)\n",
    "    \n",
    "    # 10% of the data are abnormal\n",
    "    m = len(delta)\n",
    "    abnormal_idx = np.random.choice(n, int(n/10), replace=False)\n",
    "\n",
    "    ptr = 0\n",
    "    for i in range(m):\n",
    "        for j in range(len(abnormal_idx)//m):\n",
    "            mu[abnormal_idx[ptr], :] += delta[i]\n",
    "            ptr += 1\n",
    "    \n",
    "    X = mu + noise \n",
    "    Y = np.zeros(n)\n",
    "    Y[abnormal_idx] = 1\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"Feature extractor network.\"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"Domain critic network.\"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class WDGRL():\n",
    "    def __init__(self, input_dim: int=2, generator_hidden_dims: List[int]=[32, 16, 8, 4, 2], critic_hidden_dims: List[int]=[32, 16, 8, 4, 2],\n",
    "                 gamma: float = 0.1, _lr_generator: float = 1e-2, _lr_critic: float = 1e-2, \n",
    "                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.generator = Generator(input_dim, generator_hidden_dims).to(self.device)\n",
    "        self.critic = Critic(generator_hidden_dims[-1], critic_hidden_dims).to(self.device)\n",
    "        self.generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=_lr_generator)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=_lr_critic)\n",
    "    \n",
    "    def compute_gradient_penalty(self, source_data: torch.Tensor, target_data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute gradient penalty.\"\"\"\n",
    "        if source_data.size(0) > target_data.size(0):\n",
    "            ms = source_data.size(0)\n",
    "            mt = target_data.size(0)\n",
    "            gradient_penalty = 0\n",
    "            for _ in range(0, ms, mt):\n",
    "                source_chunk = source_data[_:_+mt]\n",
    "                target_chunk = target_data\n",
    "                alpha = torch.rand(target_chunk.size(0), 1).to(self.device)\n",
    "                interpolates = (alpha * source_chunk + ((1 - alpha) * target_chunk)).requires_grad_(True)\n",
    "                \n",
    "                # Domain critic outputs\n",
    "                dc_output = self.critic(interpolates)\n",
    "                \n",
    "                # Compute gradients\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=dc_output,\n",
    "                    inputs=interpolates,\n",
    "                    grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True,\n",
    "                    only_inputs=True,\n",
    "                )\n",
    "                gradients = gradients[0]\n",
    "                gradient_penalty += ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "            if ms % mt != 0:\n",
    "                source_chunk = source_data[ms-mt:]\n",
    "                perm = torch.randperm(mt)\n",
    "                idx = perm[:ms % mt]\n",
    "                target_chunk = target_data[idx]\n",
    "                alpha = torch.rand(target_chunk.size(0), 1).to(self.device)\n",
    "                interpolates = (alpha * source_chunk + ((1 - alpha) * target_chunk)).requires_grad_(True)\n",
    "                \n",
    "                # Domain critic outputs\n",
    "                dc_output = self.critic(interpolates)\n",
    "                \n",
    "                # Compute gradients\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=dc_output,\n",
    "                    inputs=interpolates,\n",
    "                    grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True,\n",
    "                    only_inputs=True,\n",
    "                )\n",
    "                gradients = gradients[0]\n",
    "                gradient_penalty += ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "            return gradient_penalty / ((ms // mt) + (ms % mt != 0)) \n",
    "        \n",
    "        # For balanced batch\n",
    "        alpha = torch.rand(source_data.size(0), 1).to(self.device)\n",
    "        interpolates = (alpha * source_data + ((1 - alpha) * target_data)).requires_grad_(True)\n",
    "        \n",
    "        # Domain critic outputs\n",
    "        dc_output = self.critic(interpolates)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=dc_output,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "\n",
    "        # Compute gradient penalty\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "    def train(self, source_loader: DataLoader, target_loader: DataLoader, num_epochs: int = 100, dc_iter: int = 100) -> List[float]:\n",
    "        self.generator.train()\n",
    "        self.critic.train()\n",
    "        losses = []\n",
    "        source_critic_scores = []\n",
    "        target_critic_scores = []\n",
    "        for epoch in trange(num_epochs, desc='Epoch'):\n",
    "            loss = 0\n",
    "            for (source_data, _), (target_data, _) in zip(source_loader, target_loader):\n",
    "                source_data, target_data = source_data.to(self.device), target_data.to(self.device)\n",
    "\n",
    "                # Train domain critic\n",
    "                for _ in range(dc_iter):\n",
    "                    self.critic_optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        source_features = self.generator(source_data)\n",
    "                        target_features = self.generator(target_data)\n",
    "                    \n",
    "                    # Compute empirical Wasserstein distance\n",
    "                    dc_source = self.critic(source_features)\n",
    "                    dc_target = self.critic(target_features)\n",
    "                    wasserstein_distance = dc_source.mean() - dc_target.mean()\n",
    "\n",
    "                    # Gradient penalty\n",
    "                    gradient_penalty = self.compute_gradient_penalty(source_features, target_features)\n",
    "\n",
    "                    # Domain critic loss\n",
    "                    dc_loss = - wasserstein_distance + self.gamma * gradient_penalty\n",
    "                    dc_loss.backward()\n",
    "                    self.critic_optimizer.step()\n",
    "\n",
    "                # Train feature extractor\n",
    "                self.generator_optimizer.zero_grad()\n",
    "                source_features = self.generator(source_data)\n",
    "                target_features = self.generator(target_data)\n",
    "                dc_source = self.critic(source_features)\n",
    "                dc_target = self.critic(target_features)\n",
    "                wasserstein_distance = dc_source.mean() - dc_target.mean()\n",
    "                wasserstein_distance.backward()\n",
    "                self.generator_optimizer.step()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    loss += wasserstein_distance.item()\n",
    "                    \n",
    "            source_critic_scores.append(self.criticize(source_loader.dataset.tensors[0].to(self.device)))\n",
    "            target_critic_scores.append(self.criticize(target_loader.dataset.tensors[0].to(self.device)))\n",
    "            losses.append(loss/len(source_loader))\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} | Loss: {wasserstein_distance.item()}')\n",
    "        return losses, source_critic_scores, target_critic_scores\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract_feature(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.generator.eval()\n",
    "        return self.generator(x)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def criticize(self, x: torch.Tensor) -> float:\n",
    "        self.generator.eval()\n",
    "        self.critic.eval()\n",
    "        return self.critic(self.generator(x)).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of the WDGRL model (same architecture as before)\n",
    "model = WDGRL(input_dim=1,generator_hidden_dims=[10, 10, 10], critic_hidden_dims=[10])\n",
    "\n",
    "# Load the saved checkpoint\n",
    "checkpoint = torch.load(\"wdgrl.pth\", map_location=model.device, weights_only=True)\n",
    "\n",
    "# Restore the model weights\n",
    "model.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "model.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sum(X):\n",
    "    return np.argmax(np.sum(X, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpmath import mp\n",
    "\n",
    "mp.dps = 500\n",
    "def truncated_cdf(etajTy, mu, sigma, left, right):\n",
    "    numerator = mp.ncdf((etajTy - mu) / sigma) - mp.ncdf((left - mu) / sigma)\n",
    "    denominator = mp.ncdf((right - mu) / sigma) - mp.ncdf((left - mu) / sigma)\n",
    "    if denominator <= 1e-16:\n",
    "        true_cdf = 1\n",
    "    else:\n",
    "        true_cdf = numerator / denominator \n",
    "    return true_cdf\n",
    "def intersect(itv1, itv2):\n",
    "    # print(itv1, itv2)\n",
    "    itv = [max(itv1[0], itv2[0]), min(itv1[1], itv2[1])]\n",
    "    if itv[0] > itv[1]:\n",
    "        return None    \n",
    "    return itv\n",
    "\n",
    "def solve_linear_inequality(u, v): #u + vz < 0\n",
    "    if (v > -1e-16 and v < 1e-16):\n",
    "        v = 0\n",
    "        if (u < 0):\n",
    "            return [-np.Inf, np.Inf]\n",
    "        else:\n",
    "            print('error')\n",
    "            return None\n",
    "    if (v < 0):\n",
    "        return [-u/v, np.Inf]\n",
    "    return [np.NINF, -u/v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tpr():\n",
    "    ns, nt, d = 100, 100, 1\n",
    "    mu_s, mu_t = 0, 20\n",
    "    delta_s, delta_t = [1], [1]\n",
    "    xs, ys = gen_data(mu_s, delta_s, ns, d)\n",
    "    xt, yt = gen_data(mu_t, delta_t, nt, d)\n",
    "\n",
    "    X = np.vstack((xs, xt))\n",
    "    O = max_sum(X)\n",
    "    if O < 100:\n",
    "        return None\n",
    "    O = [O-100]\n",
    "    Oc = list(np.where(yt == 0)[0])\n",
    "    j = np.random.choice(O, 1, replace=False)[0]\n",
    "    etj = np.zeros((nt, 1))\n",
    "    etj[j][0] = 1\n",
    "    etOc = np.zeros((nt, 1))\n",
    "    etOc[Oc] = 1\n",
    "    etaj = np.vstack((np.zeros((ns, 1)), etj-(1/len(Oc))*etOc))\n",
    "\n",
    "    etajTX = etaj.T.dot(X)\n",
    "    print(f'Anomaly index: {O[0] + ns}')\n",
    "    print(f'etajTX: {etajTX}')\n",
    "    mu = np.vstack((np.full((ns,1), mu_s), np.full((nt,1), mu_t)))\n",
    "    sigma = np.identity(ns+nt)\n",
    "    etajTmu = etaj.T.dot(mu)\n",
    "    etajTsigmaetaj = etaj.T.dot(sigma).dot(etaj)\n",
    "\n",
    "    b = sigma.dot(etaj).dot(np.linalg.inv(etajTsigmaetaj))\n",
    "    a = (np.identity(ns+nt) - b.dot(etaj.T)).dot(X)\n",
    "\n",
    "    ao = a[O[0]+100][0]\n",
    "    bo = b[O[0]+100][0]\n",
    "    itv = [np.NINF, np.inf]\n",
    "    for i in range(X.shape[0]):\n",
    "        if (i != O[0]+100):\n",
    "            ai = a[i][0]\n",
    "            bi = b[i][0]\n",
    "            sub_itv = solve_linear_inequality(ai-ao, bi-bo)\n",
    "            itv = intersect(itv, sub_itv)   \n",
    "    cdf = truncated_cdf(etajTX[0][0], etajTmu[0][0], np.sqrt(etajTsigmaetaj[0][0]), itv[0], itv[1])\n",
    "    p_value = float(2 * min(cdf, 1 - cdf))\n",
    "    print(f'p-value: {p_value}')\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #0:\n",
      "Anomaly index: 139\n",
      "etajTX: [[4.46657016]]\n",
      "p-value: 0.0012175723261497553\n",
      "TPR: 1.0\n",
      "-------------------------------------------------\n",
      "iteration #2:\n",
      "Anomaly index: 199\n",
      "etajTX: [[3.48555829]]\n",
      "p-value: 0.6862809717758165\n",
      "TPR: 0.5\n",
      "-------------------------------------------------\n",
      "iteration #3:\n",
      "Anomaly index: 155\n",
      "etajTX: [[3.82406299]]\n",
      "p-value: 0.16998277899536385\n",
      "TPR: 0.3333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #4:\n",
      "Anomaly index: 170\n",
      "etajTX: [[4.64273676]]\n",
      "p-value: 0.0071279174638150975\n",
      "TPR: 0.5\n",
      "-------------------------------------------------\n",
      "iteration #5:\n",
      "Anomaly index: 132\n",
      "etajTX: [[3.12951137]]\n",
      "p-value: 0.1663525135244735\n",
      "TPR: 0.4\n",
      "-------------------------------------------------\n",
      "iteration #6:\n",
      "Anomaly index: 108\n",
      "etajTX: [[2.22864657]]\n",
      "p-value: 0.13130307944998637\n",
      "TPR: 0.3333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #7:\n",
      "Anomaly index: 117\n",
      "etajTX: [[2.25322255]]\n",
      "p-value: 0.5388136706741227\n",
      "TPR: 0.2857142857142857\n",
      "-------------------------------------------------\n",
      "iteration #8:\n",
      "Anomaly index: 134\n",
      "etajTX: [[2.8861666]]\n",
      "p-value: 0.5053863026620036\n",
      "TPR: 0.25\n",
      "-------------------------------------------------\n",
      "iteration #9:\n",
      "Anomaly index: 192\n",
      "etajTX: [[2.73890653]]\n",
      "p-value: 0.45297383927334794\n",
      "TPR: 0.2222222222222222\n",
      "-------------------------------------------------\n",
      "iteration #10:\n",
      "Anomaly index: 119\n",
      "etajTX: [[3.12605058]]\n",
      "p-value: 0.16800373764310214\n",
      "TPR: 0.2\n",
      "-------------------------------------------------\n",
      "iteration #11:\n",
      "Anomaly index: 101\n",
      "etajTX: [[2.74272907]]\n",
      "p-value: 0.6149964080667993\n",
      "TPR: 0.18181818181818182\n",
      "-------------------------------------------------\n",
      "iteration #12:\n",
      "Anomaly index: 141\n",
      "etajTX: [[2.88681665]]\n",
      "p-value: 0.5463458126481325\n",
      "TPR: 0.16666666666666666\n",
      "-------------------------------------------------\n",
      "iteration #13:\n",
      "Anomaly index: 130\n",
      "etajTX: [[3.84183598]]\n",
      "p-value: 0.05667612832772473\n",
      "TPR: 0.15384615384615385\n",
      "-------------------------------------------------\n",
      "iteration #14:\n",
      "Anomaly index: 161\n",
      "etajTX: [[2.4364325]]\n",
      "p-value: 0.5770100932300838\n",
      "TPR: 0.14285714285714285\n",
      "-------------------------------------------------\n",
      "iteration #15:\n",
      "Anomaly index: 162\n",
      "etajTX: [[2.66282066]]\n",
      "p-value: 0.9562064498607886\n",
      "TPR: 0.13333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #16:\n",
      "Anomaly index: 153\n",
      "etajTX: [[2.18603871]]\n",
      "p-value: 0.24017431954510451\n",
      "TPR: 0.125\n",
      "-------------------------------------------------\n",
      "iteration #17:\n",
      "Anomaly index: 174\n",
      "etajTX: [[3.48196098]]\n",
      "p-value: 0.08169619475260798\n",
      "TPR: 0.11764705882352941\n",
      "-------------------------------------------------\n",
      "iteration #18:\n",
      "Anomaly index: 164\n",
      "etajTX: [[3.47116506]]\n",
      "p-value: 0.0462764141663849\n",
      "TPR: 0.16666666666666666\n",
      "-------------------------------------------------\n",
      "iteration #19:\n",
      "Anomaly index: 198\n",
      "etajTX: [[3.10431899]]\n",
      "p-value: 0.5047612388473292\n",
      "TPR: 0.15789473684210525\n",
      "-------------------------------------------------\n",
      "iteration #20:\n",
      "Anomaly index: 159\n",
      "etajTX: [[2.6630397]]\n",
      "p-value: 0.9491257645347965\n",
      "TPR: 0.15\n",
      "-------------------------------------------------\n",
      "iteration #21:\n",
      "Anomaly index: 140\n",
      "etajTX: [[3.01244651]]\n",
      "p-value: 0.20563273011711686\n",
      "TPR: 0.14285714285714285\n",
      "-------------------------------------------------\n",
      "iteration #22:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.55388207]]\n",
      "p-value: 0.9974800769100549\n",
      "TPR: 0.13636363636363635\n",
      "-------------------------------------------------\n",
      "iteration #23:\n",
      "Anomaly index: 151\n",
      "etajTX: [[3.71666386]]\n",
      "p-value: 0.010626363324223601\n",
      "TPR: 0.17391304347826086\n",
      "-------------------------------------------------\n",
      "iteration #24:\n",
      "Anomaly index: 106\n",
      "etajTX: [[2.2082171]]\n",
      "p-value: 0.9929694036807988\n",
      "TPR: 0.16666666666666666\n",
      "-------------------------------------------------\n",
      "iteration #25:\n",
      "Anomaly index: 130\n",
      "etajTX: [[4.29848833]]\n",
      "p-value: 0.2252429369888528\n",
      "TPR: 0.16\n",
      "-------------------------------------------------\n",
      "iteration #26:\n",
      "Anomaly index: 160\n",
      "etajTX: [[2.42648163]]\n",
      "p-value: 0.9775357780435269\n",
      "TPR: 0.15384615384615385\n",
      "-------------------------------------------------\n",
      "iteration #27:\n",
      "Anomaly index: 169\n",
      "etajTX: [[2.78924841]]\n",
      "p-value: 0.799074653302742\n",
      "TPR: 0.14814814814814814\n",
      "-------------------------------------------------\n",
      "iteration #28:\n",
      "Anomaly index: 166\n",
      "etajTX: [[2.45880158]]\n",
      "p-value: 0.395004750924649\n",
      "TPR: 0.14285714285714285\n",
      "-------------------------------------------------\n",
      "iteration #29:\n",
      "Anomaly index: 169\n",
      "etajTX: [[2.60334472]]\n",
      "p-value: 0.6627642231180211\n",
      "TPR: 0.13793103448275862\n",
      "-------------------------------------------------\n",
      "iteration #30:\n",
      "Anomaly index: 165\n",
      "etajTX: [[2.86496007]]\n",
      "p-value: 0.7307241210897819\n",
      "TPR: 0.13333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #31:\n",
      "Anomaly index: 166\n",
      "etajTX: [[2.88038706]]\n",
      "p-value: 0.3985916240638399\n",
      "TPR: 0.12903225806451613\n",
      "-------------------------------------------------\n",
      "iteration #32:\n",
      "Anomaly index: 144\n",
      "etajTX: [[3.22522906]]\n",
      "p-value: 0.31495135879400077\n",
      "TPR: 0.125\n",
      "-------------------------------------------------\n",
      "iteration #33:\n",
      "Anomaly index: 144\n",
      "etajTX: [[3.27315726]]\n",
      "p-value: 0.5680547603346068\n",
      "TPR: 0.12121212121212122\n",
      "-------------------------------------------------\n",
      "iteration #34:\n",
      "Anomaly index: 147\n",
      "etajTX: [[3.62194657]]\n",
      "p-value: 0.03809544586021359\n",
      "TPR: 0.14705882352941177\n",
      "-------------------------------------------------\n",
      "iteration #35:\n",
      "Anomaly index: 143\n",
      "etajTX: [[3.63746404]]\n",
      "p-value: 0.03726899157778276\n",
      "TPR: 0.17142857142857143\n",
      "-------------------------------------------------\n",
      "iteration #36:\n",
      "Anomaly index: 171\n",
      "etajTX: [[2.83121454]]\n",
      "p-value: 0.15707537080454753\n",
      "TPR: 0.16666666666666666\n",
      "-------------------------------------------------\n",
      "iteration #37:\n",
      "Anomaly index: 197\n",
      "etajTX: [[2.77026265]]\n",
      "p-value: 0.9242819011326785\n",
      "TPR: 0.16216216216216217\n",
      "-------------------------------------------------\n",
      "iteration #38:\n",
      "Anomaly index: 109\n",
      "etajTX: [[2.7515459]]\n",
      "p-value: 0.8331577783274476\n",
      "TPR: 0.15789473684210525\n",
      "-------------------------------------------------\n",
      "iteration #39:\n",
      "Anomaly index: 144\n",
      "etajTX: [[2.46997348]]\n",
      "p-value: 0.756764316796995\n",
      "TPR: 0.15384615384615385\n",
      "-------------------------------------------------\n",
      "iteration #40:\n",
      "Anomaly index: 198\n",
      "etajTX: [[2.33515455]]\n",
      "p-value: 0.803212235390941\n",
      "TPR: 0.15\n",
      "-------------------------------------------------\n",
      "iteration #41:\n",
      "Anomaly index: 138\n",
      "etajTX: [[3.54367165]]\n",
      "p-value: 0.028568940835799674\n",
      "TPR: 0.17073170731707318\n",
      "-------------------------------------------------\n",
      "iteration #42:\n",
      "Anomaly index: 184\n",
      "etajTX: [[3.48027655]]\n",
      "p-value: 0.09065125403941689\n",
      "TPR: 0.16666666666666666\n",
      "-------------------------------------------------\n",
      "iteration #43:\n",
      "Anomaly index: 147\n",
      "etajTX: [[3.55943616]]\n",
      "p-value: 0.03442672760797278\n",
      "TPR: 0.18604651162790697\n",
      "-------------------------------------------------\n",
      "iteration #44:\n",
      "Anomaly index: 105\n",
      "etajTX: [[3.0151601]]\n",
      "p-value: 0.3262631092389257\n",
      "TPR: 0.18181818181818182\n",
      "-------------------------------------------------\n",
      "iteration #45:\n",
      "Anomaly index: 170\n",
      "etajTX: [[2.60646428]]\n",
      "p-value: 0.9421922561174693\n",
      "TPR: 0.17777777777777778\n",
      "-------------------------------------------------\n",
      "iteration #46:\n",
      "Anomaly index: 113\n",
      "etajTX: [[2.8667074]]\n",
      "p-value: 0.5688573003925842\n",
      "TPR: 0.17391304347826086\n",
      "-------------------------------------------------\n",
      "iteration #47:\n",
      "Anomaly index: 109\n",
      "etajTX: [[1.84146644]]\n",
      "p-value: 0.3117156377090851\n",
      "TPR: 0.1702127659574468\n",
      "-------------------------------------------------\n",
      "iteration #48:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.52175866]]\n",
      "p-value: 0.9857771410907791\n",
      "TPR: 0.16666666666666666\n",
      "-------------------------------------------------\n",
      "iteration #49:\n",
      "Anomaly index: 164\n",
      "etajTX: [[2.6706395]]\n",
      "p-value: 0.12892498366575003\n",
      "TPR: 0.16326530612244897\n",
      "-------------------------------------------------\n",
      "iteration #50:\n",
      "Anomaly index: 171\n",
      "etajTX: [[2.48807136]]\n",
      "p-value: 0.8753190991473956\n",
      "TPR: 0.16\n",
      "-------------------------------------------------\n",
      "iteration #51:\n",
      "Anomaly index: 163\n",
      "etajTX: [[3.36412362]]\n",
      "p-value: 0.9559141125934163\n",
      "TPR: 0.1568627450980392\n",
      "-------------------------------------------------\n",
      "iteration #52:\n",
      "Anomaly index: 158\n",
      "etajTX: [[2.82812472]]\n",
      "p-value: 0.24784234082087253\n",
      "TPR: 0.15384615384615385\n",
      "-------------------------------------------------\n",
      "iteration #53:\n",
      "Anomaly index: 173\n",
      "etajTX: [[2.14322741]]\n",
      "p-value: 0.6367493379198892\n",
      "TPR: 0.1509433962264151\n",
      "-------------------------------------------------\n",
      "iteration #54:\n",
      "Anomaly index: 120\n",
      "etajTX: [[2.58297386]]\n",
      "p-value: 0.5005870812454383\n",
      "TPR: 0.14814814814814814\n",
      "-------------------------------------------------\n",
      "iteration #55:\n",
      "Anomaly index: 143\n",
      "etajTX: [[2.57479246]]\n",
      "p-value: 0.6439172065292642\n",
      "TPR: 0.14545454545454545\n",
      "-------------------------------------------------\n",
      "iteration #56:\n",
      "Anomaly index: 137\n",
      "etajTX: [[2.67423242]]\n",
      "p-value: 0.4707467982514926\n",
      "TPR: 0.14285714285714285\n",
      "-------------------------------------------------\n",
      "iteration #57:\n",
      "Anomaly index: 156\n",
      "etajTX: [[2.44796834]]\n",
      "p-value: 0.7188793915827512\n",
      "TPR: 0.14035087719298245\n",
      "-------------------------------------------------\n",
      "iteration #58:\n",
      "Anomaly index: 144\n",
      "etajTX: [[3.25248157]]\n",
      "p-value: 0.16903672303955442\n",
      "TPR: 0.13793103448275862\n",
      "-------------------------------------------------\n",
      "iteration #59:\n",
      "Anomaly index: 166\n",
      "etajTX: [[2.19453414]]\n",
      "p-value: 0.9641671317212744\n",
      "TPR: 0.13559322033898305\n",
      "-------------------------------------------------\n",
      "iteration #60:\n",
      "Anomaly index: 126\n",
      "etajTX: [[2.9135005]]\n",
      "p-value: 0.828824229126885\n",
      "TPR: 0.13333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #61:\n",
      "Anomaly index: 135\n",
      "etajTX: [[2.40305296]]\n",
      "p-value: 0.6189629279458255\n",
      "TPR: 0.13114754098360656\n",
      "-------------------------------------------------\n",
      "iteration #62:\n",
      "Anomaly index: 118\n",
      "etajTX: [[2.18930973]]\n",
      "p-value: 0.3487800381361057\n",
      "TPR: 0.12903225806451613\n",
      "-------------------------------------------------\n",
      "iteration #63:\n",
      "Anomaly index: 178\n",
      "etajTX: [[2.59891708]]\n",
      "p-value: 0.9244103517595474\n",
      "TPR: 0.12698412698412698\n",
      "-------------------------------------------------\n",
      "iteration #64:\n",
      "Anomaly index: 111\n",
      "etajTX: [[3.17184505]]\n",
      "p-value: 0.6974244440538474\n",
      "TPR: 0.125\n",
      "-------------------------------------------------\n",
      "iteration #65:\n",
      "Anomaly index: 169\n",
      "etajTX: [[2.94084042]]\n",
      "p-value: 0.3893000240975823\n",
      "TPR: 0.12307692307692308\n",
      "-------------------------------------------------\n",
      "iteration #66:\n",
      "Anomaly index: 160\n",
      "etajTX: [[2.89182128]]\n",
      "p-value: 0.8293611295636646\n",
      "TPR: 0.12121212121212122\n",
      "-------------------------------------------------\n",
      "iteration #67:\n",
      "Anomaly index: 132\n",
      "etajTX: [[2.50688797]]\n",
      "p-value: 0.24168941452430026\n",
      "TPR: 0.11940298507462686\n",
      "-------------------------------------------------\n",
      "iteration #68:\n",
      "Anomaly index: 167\n",
      "etajTX: [[3.00238874]]\n",
      "p-value: 0.4603118356846809\n",
      "TPR: 0.11764705882352941\n",
      "-------------------------------------------------\n",
      "iteration #69:\n",
      "Anomaly index: 130\n",
      "etajTX: [[3.14080575]]\n",
      "p-value: 0.813949345590269\n",
      "TPR: 0.11594202898550725\n",
      "-------------------------------------------------\n",
      "iteration #70:\n",
      "Anomaly index: 146\n",
      "etajTX: [[2.78992342]]\n",
      "p-value: 0.5157736324420548\n",
      "TPR: 0.11428571428571428\n",
      "-------------------------------------------------\n",
      "iteration #71:\n",
      "Anomaly index: 114\n",
      "etajTX: [[2.98441662]]\n",
      "p-value: 0.4148430162729185\n",
      "TPR: 0.11267605633802817\n",
      "-------------------------------------------------\n",
      "iteration #72:\n",
      "Anomaly index: 116\n",
      "etajTX: [[2.5143146]]\n",
      "p-value: 0.7895310551190292\n",
      "TPR: 0.1111111111111111\n",
      "-------------------------------------------------\n",
      "iteration #73:\n",
      "Anomaly index: 116\n",
      "etajTX: [[3.4390622]]\n",
      "p-value: 0.383670257053269\n",
      "TPR: 0.1095890410958904\n",
      "-------------------------------------------------\n",
      "iteration #74:\n",
      "Anomaly index: 141\n",
      "etajTX: [[2.43192698]]\n",
      "p-value: 0.7844850319315329\n",
      "TPR: 0.10810810810810811\n",
      "-------------------------------------------------\n",
      "iteration #75:\n",
      "Anomaly index: 152\n",
      "etajTX: [[2.94246347]]\n",
      "p-value: 0.4885865305003677\n",
      "TPR: 0.10666666666666667\n",
      "-------------------------------------------------\n",
      "iteration #76:\n",
      "Anomaly index: 154\n",
      "etajTX: [[3.13606956]]\n",
      "p-value: 0.4194198980901411\n",
      "TPR: 0.10526315789473684\n",
      "-------------------------------------------------\n",
      "iteration #77:\n",
      "Anomaly index: 164\n",
      "etajTX: [[2.57734818]]\n",
      "p-value: 0.8559355641071056\n",
      "TPR: 0.1038961038961039\n",
      "-------------------------------------------------\n",
      "iteration #78:\n",
      "Anomaly index: 159\n",
      "etajTX: [[2.47527458]]\n",
      "p-value: 0.320995950060305\n",
      "TPR: 0.10256410256410256\n",
      "-------------------------------------------------\n",
      "iteration #79:\n",
      "Anomaly index: 127\n",
      "etajTX: [[2.84331072]]\n",
      "p-value: 0.4114220285365998\n",
      "TPR: 0.10126582278481013\n",
      "-------------------------------------------------\n",
      "iteration #80:\n",
      "Anomaly index: 175\n",
      "etajTX: [[2.57751499]]\n",
      "p-value: 0.8158626564559187\n",
      "TPR: 0.1\n",
      "-------------------------------------------------\n",
      "iteration #81:\n",
      "Anomaly index: 149\n",
      "etajTX: [[3.35273036]]\n",
      "p-value: 0.25688545613572716\n",
      "TPR: 0.09876543209876543\n",
      "-------------------------------------------------\n",
      "iteration #82:\n",
      "Anomaly index: 103\n",
      "etajTX: [[2.69214183]]\n",
      "p-value: 0.596331963781416\n",
      "TPR: 0.0975609756097561\n",
      "-------------------------------------------------\n",
      "iteration #83:\n",
      "Anomaly index: 140\n",
      "etajTX: [[2.88495007]]\n",
      "p-value: 0.5616760222819388\n",
      "TPR: 0.0963855421686747\n",
      "-------------------------------------------------\n",
      "iteration #84:\n",
      "Anomaly index: 147\n",
      "etajTX: [[3.74396476]]\n",
      "p-value: 0.021532187308599928\n",
      "TPR: 0.10714285714285714\n",
      "-------------------------------------------------\n",
      "iteration #85:\n",
      "Anomaly index: 186\n",
      "etajTX: [[2.87033449]]\n",
      "p-value: 0.8877331434851875\n",
      "TPR: 0.10588235294117647\n",
      "-------------------------------------------------\n",
      "iteration #86:\n",
      "Anomaly index: 132\n",
      "etajTX: [[2.83654717]]\n",
      "p-value: 0.22160854373900166\n",
      "TPR: 0.10465116279069768\n",
      "-------------------------------------------------\n",
      "iteration #87:\n",
      "Anomaly index: 169\n",
      "etajTX: [[2.94313017]]\n",
      "p-value: 0.4138449640445168\n",
      "TPR: 0.10344827586206896\n",
      "-------------------------------------------------\n",
      "iteration #88:\n",
      "Anomaly index: 159\n",
      "etajTX: [[2.47994018]]\n",
      "p-value: 0.2420321282741646\n",
      "TPR: 0.10227272727272728\n",
      "-------------------------------------------------\n",
      "iteration #89:\n",
      "Anomaly index: 108\n",
      "etajTX: [[2.42920597]]\n",
      "p-value: 0.18788068881532335\n",
      "TPR: 0.10112359550561797\n",
      "-------------------------------------------------\n",
      "iteration #90:\n",
      "Anomaly index: 119\n",
      "etajTX: [[3.20662199]]\n",
      "p-value: 0.26892638631936006\n",
      "TPR: 0.1\n",
      "-------------------------------------------------\n",
      "iteration #91:\n",
      "Anomaly index: 181\n",
      "etajTX: [[2.58778228]]\n",
      "p-value: 0.743491460386405\n",
      "TPR: 0.0989010989010989\n",
      "-------------------------------------------------\n",
      "iteration #92:\n",
      "Anomaly index: 133\n",
      "etajTX: [[3.37203586]]\n",
      "p-value: 0.24162491486036636\n",
      "TPR: 0.09782608695652174\n",
      "-------------------------------------------------\n",
      "iteration #93:\n",
      "Anomaly index: 179\n",
      "etajTX: [[4.24782844]]\n",
      "p-value: 0.010656623198622601\n",
      "TPR: 0.10752688172043011\n",
      "-------------------------------------------------\n",
      "iteration #94:\n",
      "Anomaly index: 102\n",
      "etajTX: [[3.09968061]]\n",
      "p-value: 0.8735214701261744\n",
      "TPR: 0.10638297872340426\n",
      "-------------------------------------------------\n",
      "iteration #95:\n",
      "Anomaly index: 135\n",
      "etajTX: [[2.50347616]]\n",
      "p-value: 0.7025862945609448\n",
      "TPR: 0.10526315789473684\n",
      "-------------------------------------------------\n",
      "iteration #96:\n",
      "Anomaly index: 141\n",
      "etajTX: [[3.12745599]]\n",
      "p-value: 0.149421495330822\n",
      "TPR: 0.10416666666666667\n",
      "-------------------------------------------------\n",
      "iteration #97:\n",
      "Anomaly index: 133\n",
      "etajTX: [[2.87434367]]\n",
      "p-value: 0.5944476458845419\n",
      "TPR: 0.10309278350515463\n",
      "-------------------------------------------------\n",
      "iteration #98:\n",
      "Anomaly index: 180\n",
      "etajTX: [[3.42089582]]\n",
      "p-value: 0.5931429663378789\n",
      "TPR: 0.10204081632653061\n",
      "-------------------------------------------------\n",
      "iteration #99:\n",
      "Anomaly index: 151\n",
      "etajTX: [[3.05339435]]\n",
      "p-value: 0.39131984848645057\n",
      "TPR: 0.10101010101010101\n",
      "-------------------------------------------------\n",
      "iteration #100:\n",
      "Anomaly index: 174\n",
      "etajTX: [[2.65475708]]\n",
      "p-value: 0.2702865232374901\n",
      "TPR: 0.1\n",
      "-------------------------------------------------\n",
      "iteration #101:\n",
      "Anomaly index: 163\n",
      "etajTX: [[3.15933936]]\n",
      "p-value: 0.17797270763291584\n",
      "TPR: 0.09900990099009901\n",
      "-------------------------------------------------\n",
      "iteration #102:\n",
      "Anomaly index: 107\n",
      "etajTX: [[2.74832007]]\n",
      "p-value: 0.7913354383695541\n",
      "TPR: 0.09803921568627451\n",
      "-------------------------------------------------\n",
      "iteration #103:\n",
      "Anomaly index: 145\n",
      "etajTX: [[3.24402664]]\n",
      "p-value: 0.2915522834944014\n",
      "TPR: 0.0970873786407767\n",
      "-------------------------------------------------\n",
      "iteration #104:\n",
      "Anomaly index: 113\n",
      "etajTX: [[2.80272618]]\n",
      "p-value: 0.2519283571239977\n",
      "TPR: 0.09615384615384616\n",
      "-------------------------------------------------\n",
      "iteration #105:\n",
      "Anomaly index: 190\n",
      "etajTX: [[2.85945402]]\n",
      "p-value: 0.8879421193124518\n",
      "TPR: 0.09523809523809523\n",
      "-------------------------------------------------\n",
      "iteration #106:\n",
      "Anomaly index: 197\n",
      "etajTX: [[2.33390156]]\n",
      "p-value: 0.34659124816871073\n",
      "TPR: 0.09433962264150944\n",
      "-------------------------------------------------\n",
      "iteration #107:\n",
      "Anomaly index: 131\n",
      "etajTX: [[2.43946655]]\n",
      "p-value: 0.9373738935395473\n",
      "TPR: 0.09345794392523364\n",
      "-------------------------------------------------\n",
      "iteration #108:\n",
      "Anomaly index: 198\n",
      "etajTX: [[2.14827052]]\n",
      "p-value: 0.4899882522253883\n",
      "TPR: 0.09259259259259259\n",
      "-------------------------------------------------\n",
      "iteration #109:\n",
      "Anomaly index: 125\n",
      "etajTX: [[2.5869654]]\n",
      "p-value: 0.9220381549414206\n",
      "TPR: 0.09174311926605505\n",
      "-------------------------------------------------\n",
      "iteration #110:\n",
      "Anomaly index: 181\n",
      "etajTX: [[3.08663928]]\n",
      "p-value: 0.1354498168993286\n",
      "TPR: 0.09090909090909091\n",
      "-------------------------------------------------\n",
      "iteration #111:\n",
      "Anomaly index: 196\n",
      "etajTX: [[2.35247935]]\n",
      "p-value: 0.5580027869423307\n",
      "TPR: 0.09009009009009009\n",
      "-------------------------------------------------\n",
      "iteration #112:\n",
      "Anomaly index: 134\n",
      "etajTX: [[2.58244635]]\n",
      "p-value: 0.9793646240192811\n",
      "TPR: 0.08928571428571429\n",
      "-------------------------------------------------\n",
      "iteration #113:\n",
      "Anomaly index: 152\n",
      "etajTX: [[2.8728794]]\n",
      "p-value: 0.9845547895617718\n",
      "TPR: 0.08849557522123894\n",
      "-------------------------------------------------\n",
      "iteration #114:\n",
      "Anomaly index: 119\n",
      "etajTX: [[2.60371321]]\n",
      "p-value: 0.8718690009257178\n",
      "TPR: 0.08771929824561403\n",
      "-------------------------------------------------\n",
      "iteration #115:\n",
      "Anomaly index: 140\n",
      "etajTX: [[2.85767445]]\n",
      "p-value: 0.2798756477370202\n",
      "TPR: 0.08695652173913043\n",
      "-------------------------------------------------\n",
      "iteration #116:\n",
      "Anomaly index: 169\n",
      "etajTX: [[3.75247878]]\n",
      "p-value: 0.04892163353777763\n",
      "TPR: 0.09482758620689655\n",
      "-------------------------------------------------\n",
      "iteration #117:\n",
      "Anomaly index: 180\n",
      "etajTX: [[2.34104374]]\n",
      "p-value: 0.1398049235430836\n",
      "TPR: 0.09401709401709402\n",
      "-------------------------------------------------\n",
      "iteration #118:\n",
      "Anomaly index: 140\n",
      "etajTX: [[2.68491755]]\n",
      "p-value: 0.703642211666031\n",
      "TPR: 0.09322033898305085\n",
      "-------------------------------------------------\n",
      "iteration #119:\n",
      "Anomaly index: 185\n",
      "etajTX: [[2.98139992]]\n",
      "p-value: 0.8926005414173456\n",
      "TPR: 0.09243697478991597\n",
      "-------------------------------------------------\n",
      "iteration #120:\n",
      "Anomaly index: 115\n",
      "etajTX: [[2.94930201]]\n",
      "p-value: 0.34039009282658117\n",
      "TPR: 0.09166666666666666\n",
      "-------------------------------------------------\n",
      "iteration #121:\n",
      "Anomaly index: 174\n",
      "etajTX: [[2.72773041]]\n",
      "p-value: 0.9426301024769604\n",
      "TPR: 0.09090909090909091\n",
      "-------------------------------------------------\n",
      "iteration #122:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdE0lEQVR4nO3df3TV9X348Vck5kI9SRQcP7IGQVdFkeIqykFshY0zDkXU7Wzq6liGW21nWqbZsZJZpPgr6PF4WC2F1a2F7ai024Q5cThHZcyKVX5tblqUgprVBeZZm/BjRiSf/dFjvt9IUC5+7jvc+Hicc/+4n/u+9/PibQ55+rm5pCLLsiwAABI5oa8HAAA+WsQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkVdnXA7xXV1dXvPHGG1FdXR0VFRV9PQ4AcBSyLIu9e/dGXV1dnHDC+1/bOO7i44033oj6+vq+HgMAOAatra3x8Y9//H3XHHfxUV1dHRE/H76mpqaPpwEAjkZHR0fU19d3fx9/P8ddfLz7VktNTY34AIAyczQ/MuEHTgGApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQVNHxsWHDhpg1a1bU1dVFRUVFrF69+rA1L730Ulx22WVRW1sbJ510UlxwwQXx+uuv5zEvAFDmio6P/fv3x/jx42PJkiW9Pv7jH/84Lr744hgzZkysX78+/u3f/i3mz58fAwcO/NDDAgDlryLLsuyYn1xREatWrYorrrii+9jVV18dJ554YvzVX/3VMb1mR0dH1NbWRnt7u18sBwBlopjv37n+zEdXV1esWbMmzjzzzJg+fXoMHTo0Jk6c2OtbM+/q7OyMjo6OHjcAoP+qzPPF9uzZE/v27YtFixbFHXfcEXfffXesXbs2fuM3fiOeeuqpuOSSSw57TktLSyxcuDDPMQAgmVHz1vT1CEV7ddHMPj1/7lc+IiIuv/zyuPHGG+O8886LefPmxaWXXhrLli3r9TnNzc3R3t7efWttbc1zJADgOJPrlY9TTz01Kisr45xzzulx/Oyzz46nn3661+cUCoUoFAp5jgEAHMdyvfJRVVUVF1xwQWzfvr3H8ZdffjlOO+20PE8FAJSpoq987Nu3L3bs2NF9f9euXbFt27YYPHhwjBw5Mm666aa46qqr4jOf+UxMnTo11q5dG3//938f69evz3NuAKBMFR0fmzZtiqlTp3bfb2pqioiIhoaGWL58efz6r/96LFu2LFpaWmLu3Llx1llnxd/+7d/GxRdfnN/UAEDZKjo+pkyZEh/0T4Nce+21ce211x7zUABA/+V3uwAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEkVHR8bNmyIWbNmRV1dXVRUVMTq1auPuPaLX/xiVFRUxOLFiz/EiABAf1J0fOzfvz/Gjx8fS5Ysed91q1atimeffTbq6uqOeTgAoP+pLPYJM2bMiBkzZrzvmp/85Cfx5S9/OZ544omYOXPmMQ8HAPQ/RcfHB+nq6orZs2fHTTfdFGPHjv3A9Z2dndHZ2dl9v6OjI++RAIDjSO7xcffdd0dlZWXMnTv3qNa3tLTEwoUL8x7jiEbNW5PsXHl5dVH5XT2yzxxJOX5tRPj6SKVcvz4oTq6fdtm8eXP86Z/+aSxfvjwqKiqO6jnNzc3R3t7efWttbc1zJADgOJNrfPzLv/xL7NmzJ0aOHBmVlZVRWVkZr732WvzxH/9xjBo1qtfnFAqFqKmp6XEDAPqvXN92mT17dkybNq3HsenTp8fs2bNjzpw5eZ4KAChTRcfHvn37YseOHd33d+3aFdu2bYvBgwfHyJEjY8iQIT3Wn3jiiTF8+PA466yzPvy0AEDZKzo+Nm3aFFOnTu2+39TUFBERDQ0NsXz58twGAwD6p6LjY8qUKZFl2VGvf/XVV4s9BQDQj/ndLgBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFJFx8eGDRti1qxZUVdXFxUVFbF69eruxw4ePBg333xzjBs3Lk466aSoq6uL3/3d34033ngjz5kBgDJWdHzs378/xo8fH0uWLDnssQMHDsSWLVti/vz5sWXLlnjkkUdi+/btcdlll+UyLABQ/iqLfcKMGTNixowZvT5WW1sbTz75ZI9j3/jGN+LCCy+M119/PUaOHHlsUwIA/UbR8VGs9vb2qKioiJNPPrnXxzs7O6Ozs7P7fkdHR6lHAgD6UEnj46233oqbb745fvu3fztqamp6XdPS0hILFy4s5Rhlb9S8NX09wkdCOe7zq4tm9vUIAEUr2addDh48GFdeeWVkWRZLly494rrm5uZob2/vvrW2tpZqJADgOFCSKx/vhsdrr70W3//+94941SMiolAoRKFQKMUYAMBxKPf4eDc8XnnllXjqqadiyJAheZ8CAChjRcfHvn37YseOHd33d+3aFdu2bYvBgwfHiBEj4jd/8zdjy5Yt8dhjj8WhQ4eira0tIiIGDx4cVVVV+U0OAJSlouNj06ZNMXXq1O77TU1NERHR0NAQX/va1+LRRx+NiIjzzjuvx/OeeuqpmDJlyrFPCgD0C0XHx5QpUyLLsiM+/n6PAQD43S4AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBSRcfHhg0bYtasWVFXVxcVFRWxevXqHo9nWRa33nprjBgxIgYNGhTTpk2LV155Ja95AYAyV3R87N+/P8aPHx9Llizp9fF77rknvv71r8eyZcvihz/8YZx00kkxffr0eOuttz70sABA+ass9gkzZsyIGTNm9PpYlmWxePHi+OpXvxqXX355RET85V/+ZQwbNixWr14dV1999YebFgAoe7n+zMeuXbuira0tpk2b1n2strY2Jk6cGBs3buz1OZ2dndHR0dHjBgD0X7nGR1tbW0REDBs2rMfxYcOGdT/2Xi0tLVFbW9t9q6+vz3MkAOA40+efdmlubo729vbuW2tra1+PBACUUK7xMXz48IiI2L17d4/ju3fv7n7svQqFQtTU1PS4AQD9V67xMXr06Bg+fHisW7eu+1hHR0f88Ic/jEmTJuV5KgCgTBX9aZd9+/bFjh07uu/v2rUrtm3bFoMHD46RI0fGDTfcEHfccUd84hOfiNGjR8f8+fOjrq4urrjiijznBgDKVNHxsWnTppg6dWr3/aampoiIaGhoiOXLl8dXvvKV2L9/f1x33XXxs5/9LC6++OJYu3ZtDBw4ML+pAYCyVXR8TJkyJbIsO+LjFRUVcdttt8Vtt932oQYDAPqnPv+0CwDw0SI+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJVfb1AMCxGzVvTV+PwHHM1wfHK1c+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKRyj49Dhw7F/PnzY/To0TFo0KA444wz4vbbb48sy/I+FQBQhirzfsG77747li5dGitWrIixY8fGpk2bYs6cOVFbWxtz587N+3QAQJnJPT6eeeaZuPzyy2PmzJkRETFq1Kh4+OGH47nnnsv7VABAGcr9bZeLLroo1q1bFy+//HJERPzrv/5rPP300zFjxoxe13d2dkZHR0ePGwDQf+V+5WPevHnR0dERY8aMiQEDBsShQ4fizjvvjGuuuabX9S0tLbFw4cK8xwAAjlO5X/n43ve+Fw8++GA89NBDsWXLllixYkXce++9sWLFil7XNzc3R3t7e/ettbU175EAgONI7lc+brrpppg3b15cffXVERExbty4eO2116KlpSUaGhoOW18oFKJQKOQ9BgBwnMr9yseBAwfihBN6vuyAAQOiq6sr71MBAGUo9ysfs2bNijvvvDNGjhwZY8eOja1bt8Z9990X1157bd6nAgDKUO7xcf/998f8+fPj+uuvjz179kRdXV184QtfiFtvvTXvUwEAZSj3+Kiuro7FixfH4sWL835pAKAf8LtdAICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AIKnKvh4AoByMmremr0eAfsOVDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACCpksTHT37yk/id3/mdGDJkSAwaNCjGjRsXmzZtKsWpAIAyU5n3C/70pz+NyZMnx9SpU+Mf/uEf4hd+4RfilVdeiVNOOSXvUwEAZSj3+Lj77rujvr4+vvOd73QfGz16dN6nAQDKVO5vuzz66KMxYcKE+K3f+q0YOnRo/PIv/3I88MADR1zf2dkZHR0dPW4AQP+Ve3zs3Lkzli5dGp/4xCfiiSeeiD/8wz+MuXPnxooVK3pd39LSErW1td23+vr6vEcCAI4jFVmWZXm+YFVVVUyYMCGeeeaZ7mNz586N559/PjZu3HjY+s7Ozujs7Oy+39HREfX19dHe3h41NTV5jhYREaPmrcn9NQGgnLy6aGbur9nR0RG1tbVH9f079ysfI0aMiHPOOafHsbPPPjtef/31XtcXCoWoqanpcQMA+q/c42Py5Mmxffv2HsdefvnlOO200/I+FQBQhnKPjxtvvDGeffbZuOuuu2LHjh3x0EMPxbe+9a1obGzM+1QAQBnKPT4uuOCCWLVqVTz88MNx7rnnxu233x6LFy+Oa665Ju9TAQBlKPd/5yMi4tJLL41LL720FC8NAJQ5v9sFAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBIquTxsWjRoqioqIgbbrih1KcCAMpASePj+eefjz/7sz+LT37yk6U8DQBQRkoWH/v27YtrrrkmHnjggTjllFNKdRoAoMyULD4aGxtj5syZMW3atPdd19nZGR0dHT1uAED/VVmKF125cmVs2bIlnn/++Q9c29LSEgsXLizFGADAcSj3Kx+tra3xR3/0R/Hggw/GwIEDP3B9c3NztLe3d99aW1vzHgkAOI7kfuVj8+bNsWfPnvjUpz7VfezQoUOxYcOG+MY3vhGdnZ0xYMCA7scKhUIUCoW8xwAAjlO5x8ev/uqvxgsvvNDj2Jw5c2LMmDFx88039wgPAOCjJ/f4qK6ujnPPPbfHsZNOOimGDBly2HEA4KPHv3AKACRVkk+7vNf69etTnAYAKAOufAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEnlHh8tLS1xwQUXRHV1dQwdOjSuuOKK2L59e96nAQDKVO7x8c///M/R2NgYzz77bDz55JNx8ODB+LVf+7XYv39/3qcCAMpQZd4vuHbt2h73ly9fHkOHDo3NmzfHZz7zmbxPBwCUmdzj473a29sjImLw4MG9Pt7Z2RmdnZ3d9zs6Oko9EgDQh0r6A6ddXV1xww03xOTJk+Pcc8/tdU1LS0vU1tZ23+rr60s5EgDQx0oaH42NjfHv//7vsXLlyiOuaW5ujvb29u5ba2trKUcCAPpYyd52+dKXvhSPPfZYbNiwIT7+8Y8fcV2hUIhCoVCqMQCA40zu8ZFlWXz5y1+OVatWxfr162P06NF5nwIAKGO5x0djY2M89NBD8Xd/93dRXV0dbW1tERFRW1sbgwYNyvt0AECZyf1nPpYuXRrt7e0xZcqUGDFiRPftu9/9bt6nAgDKUEnedgEAOBK/2wUASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEiqZPGxZMmSGDVqVAwcODAmTpwYzz33XKlOBQCUkZLEx3e/+91oamqKBQsWxJYtW2L8+PExffr02LNnTylOBwCUkZLEx3333Ref//znY86cOXHOOefEsmXL4mMf+1h8+9vfLsXpAIAyUpn3C7799tuxefPmaG5u7j52wgknxLRp02Ljxo2Hre/s7IzOzs7u++3t7RER0dHRkfdoERHR1XmgJK8LAOWiFN9j333NLMs+cG3u8fHmm2/GoUOHYtiwYT2ODxs2LH70ox8dtr6lpSUWLlx42PH6+vq8RwMAIqJ2celee+/evVFbW/u+a3KPj2I1NzdHU1NT9/2urq74n//5nxgyZEhUVFTkdp6Ojo6or6+P1tbWqKmpye11OZy9Tsdep2Gf07HXaZRin7Msi71790ZdXd0Hrs09Pk499dQYMGBA7N69u8fx3bt3x/Dhww9bXygUolAo9Dh28skn5z1Wt5qaGl/QidjrdOx1GvY5HXudRt77/EFXPN6V+w+cVlVVxfnnnx/r1q3rPtbV1RXr1q2LSZMm5X06AKDMlORtl6ampmhoaIgJEybEhRdeGIsXL479+/fHnDlzSnE6AKCMlCQ+rrrqqvjv//7vuPXWW6OtrS3OO++8WLt27WE/hJpSoVCIBQsWHPYWD/mz1+nY6zTsczr2Oo2+3ueK7Gg+EwMAkBO/2wUASEp8AABJiQ8AICnxAQAk1a/iY8mSJTFq1KgYOHBgTJw4MZ577rn3Xf/Xf/3XMWbMmBg4cGCMGzcuHn/88USTlr9i9vqBBx6IT3/603HKKafEKaecEtOmTfvA/zb8XLFf0+9auXJlVFRUxBVXXFHaAfuRYvf6Zz/7WTQ2NsaIESOiUCjEmWee6e+Qo1DsPi9evDjOOuusGDRoUNTX18eNN94Yb731VqJpy9eGDRti1qxZUVdXFxUVFbF69eoPfM769evjU5/6VBQKhfilX/qlWL58eekGzPqJlStXZlVVVdm3v/3t7D/+4z+yz3/+89nJJ5+c7d69u9f1P/jBD7IBAwZk99xzT/biiy9mX/3qV7MTTzwxe+GFFxJPXn6K3evPfe5z2ZIlS7KtW7dmL730UvZ7v/d7WW1tbfaf//mfiScvL8Xu87t27dqV/eIv/mL26U9/Orv88svTDFvmit3rzs7ObMKECdlnP/vZ7Omnn8527dqVrV+/Ptu2bVviyctLsfv84IMPZoVCIXvwwQezXbt2ZU888UQ2YsSI7MYbb0w8efl5/PHHs1tuuSV75JFHsojIVq1a9b7rd+7cmX3sYx/LmpqashdffDG7//77swEDBmRr164tyXz9Jj4uvPDCrLGxsfv+oUOHsrq6uqylpaXX9VdeeWU2c+bMHscmTpyYfeELXyjpnP1BsXv9Xu+8805WXV2drVixolQj9gvHss/vvPNOdtFFF2V//ud/njU0NIiPo1TsXi9dujQ7/fTTs7fffjvViP1Csfvc2NiY/cqv/EqPY01NTdnkyZNLOmd/czTx8ZWvfCUbO3Zsj2NXXXVVNn369JLM1C/ednn77bdj8+bNMW3atO5jJ5xwQkybNi02btzY63M2btzYY31ExPTp04+4np87lr1+rwMHDsTBgwdj8ODBpRqz7B3rPt92220xdOjQ+P3f//0UY/YLx7LXjz76aEyaNCkaGxtj2LBhce6558Zdd90Vhw4dSjV22TmWfb7oooti8+bN3W/N7Ny5Mx5//PH47Gc/m2Tmj5LU3xP7/Lfa5uHNN9+MQ4cOHfYvqA4bNix+9KMf9fqctra2Xte3tbWVbM7+4Fj2+r1uvvnmqKurO+wLnf/nWPb56aefjr/4i7+Ibdu2JZiw/ziWvd65c2d8//vfj2uuuSYef/zx2LFjR1x//fVx8ODBWLBgQYqxy86x7PPnPve5ePPNN+Piiy+OLMvinXfeiS9+8YvxJ3/yJylG/kg50vfEjo6O+N///d8YNGhQrufrF1c+KB+LFi2KlStXxqpVq2LgwIF9PU6/sXfv3pg9e3Y88MADceqpp/b1OP1eV1dXDB06NL71rW/F+eefH1dddVXccsstsWzZsr4erV9Zv3593HXXXfHNb34ztmzZEo888kisWbMmbr/99r4ejQ+pX1z5OPXUU2PAgAGxe/fuHsd3794dw4cP7/U5w4cPL2o9P3cse/2ue++9NxYtWhT/9E//FJ/85CdLOWbZK3aff/zjH8err74as2bN6j7W1dUVERGVlZWxffv2OOOMM0o7dJk6lq/pESNGxIknnhgDBgzoPnb22WdHW1tbvP3221FVVVXSmcvRsezz/PnzY/bs2fEHf/AHERExbty42L9/f1x33XVxyy23xAkn+P/nvBzpe2JNTU3uVz0i+smVj6qqqjj//PNj3bp13ce6urpi3bp1MWnSpF6fM2nSpB7rIyKefPLJI67n545lryMi7rnnnrj99ttj7dq1MWHChBSjlrVi93nMmDHxwgsvxLZt27pvl112WUydOjW2bdsW9fX1KccvK8fyNT158uTYsWNHd+BFRLz88ssxYsQI4XEEx7LPBw4cOCww3g2+zK8ly1Xy74kl+THWPrBy5cqsUChky5cvz1588cXsuuuuy04++eSsra0ty7Ismz17djZv3rzu9T/4wQ+yysrK7N57781eeumlbMGCBT5qe5SK3etFixZlVVVV2d/8zd9k//Vf/9V927t3b1/9EcpCsfv8Xj7tcvSK3evXX389q66uzr70pS9l27dvzx577LFs6NCh2R133NFXf4SyUOw+L1iwIKuurs4efvjhbOfOndk//uM/ZmeccUZ25ZVX9tUfoWzs3bs327p1a7Z169YsIrL77rsv27p1a/baa69lWZZl8+bNy2bPnt29/t2P2t50003ZSy+9lC1ZssRHbY/W/fffn40cOTKrqqrKLrzwwuzZZ5/tfuySSy7JGhoaeqz/3ve+l5155plZVVVVNnbs2GzNmjWJJy5fxez1aaedlkXEYbcFCxakH7zMFPs1/f8TH8Updq+feeaZbOLEiVmhUMhOP/307M4778zeeeedxFOXn2L2+eDBg9nXvva17IwzzsgGDhyY1dfXZ9dff33205/+NP3gZeapp57q9e/dd/e3oaEhu+SSSw57znnnnZdVVVVlp59+evad73ynZPNVZJlrVwBAOv3iZz4AgPIhPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJL6P9DHjOfzT8zQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "max_iteration = 120\n",
    "alpha = 0.05\n",
    "list_p_value = []\n",
    "count = 0\n",
    "print(f'iteration #{0}:')\n",
    "\n",
    "\n",
    "while len(list_p_value) <= max_iteration:\n",
    "    p_value = run_tpr()\n",
    "    if p_value is None:\n",
    "        continue\n",
    "    list_p_value.append(p_value)\n",
    "    if p_value <= alpha:\n",
    "        count += 1\n",
    "    print(f'TPR: {count / len(list_p_value)}')\n",
    "    print('-------------------------------------------------')\n",
    "    print(f'iteration #{len(list_p_value)+1}:')\n",
    "plt.hist(list_p_value)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
