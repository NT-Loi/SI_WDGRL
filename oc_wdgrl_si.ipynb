{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def gen_data(mu, delta, n, d: int = 2):\n",
    "    noise = np.random.normal(loc = 0, scale = 1, size=(n, d))\n",
    "    mu = np.full((n, d), mu, dtype=np.float64)\n",
    "\n",
    "    if len(delta) == 1 and delta[0] == 0:\n",
    "        return mu + noise, np.zeros(n)\n",
    "    \n",
    "    # 10% of the data are abnormal\n",
    "    m = len(delta)\n",
    "    abnormal_idx = np.random.choice(n, int(n/10), replace=False)\n",
    "\n",
    "    ptr = 0\n",
    "    for i in range(m):\n",
    "        for j in range(len(abnormal_idx)//m):\n",
    "            mu[abnormal_idx[ptr], :] += delta[i]\n",
    "            ptr += 1\n",
    "    \n",
    "    X = mu + noise \n",
    "    Y = np.zeros(n)\n",
    "    Y[abnormal_idx] = 1\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"Feature extractor network.\"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"Domain critic network.\"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class WDGRL():\n",
    "    def __init__(self, input_dim: int=2, generator_hidden_dims: List[int]=[32, 16, 8, 4, 2], critic_hidden_dims: List[int]=[32, 16, 8, 4, 2],\n",
    "                 gamma: float = 0.1, _lr_generator: float = 1e-2, _lr_critic: float = 1e-2, \n",
    "                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.generator = Generator(input_dim, generator_hidden_dims).to(self.device)\n",
    "        self.critic = Critic(generator_hidden_dims[-1], critic_hidden_dims).to(self.device)\n",
    "        self.generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=_lr_generator)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=_lr_critic)\n",
    "    \n",
    "    def compute_gradient_penalty(self, source_data: torch.Tensor, target_data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute gradient penalty.\"\"\"\n",
    "        if source_data.size(0) > target_data.size(0):\n",
    "            ms = source_data.size(0)\n",
    "            mt = target_data.size(0)\n",
    "            gradient_penalty = 0\n",
    "            for _ in range(0, ms, mt):\n",
    "                source_chunk = source_data[_:_+mt]\n",
    "                target_chunk = target_data\n",
    "                alpha = torch.rand(target_chunk.size(0), 1).to(self.device)\n",
    "                interpolates = (alpha * source_chunk + ((1 - alpha) * target_chunk)).requires_grad_(True)\n",
    "                \n",
    "                # Domain critic outputs\n",
    "                dc_output = self.critic(interpolates)\n",
    "                \n",
    "                # Compute gradients\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=dc_output,\n",
    "                    inputs=interpolates,\n",
    "                    grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True,\n",
    "                    only_inputs=True,\n",
    "                )\n",
    "                gradients = gradients[0]\n",
    "                gradient_penalty += ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "            if ms % mt != 0:\n",
    "                source_chunk = source_data[ms-mt:]\n",
    "                perm = torch.randperm(mt)\n",
    "                idx = perm[:ms % mt]\n",
    "                target_chunk = target_data[idx]\n",
    "                alpha = torch.rand(target_chunk.size(0), 1).to(self.device)\n",
    "                interpolates = (alpha * source_chunk + ((1 - alpha) * target_chunk)).requires_grad_(True)\n",
    "                \n",
    "                # Domain critic outputs\n",
    "                dc_output = self.critic(interpolates)\n",
    "                \n",
    "                # Compute gradients\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=dc_output,\n",
    "                    inputs=interpolates,\n",
    "                    grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True,\n",
    "                    only_inputs=True,\n",
    "                )\n",
    "                gradients = gradients[0]\n",
    "                gradient_penalty += ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "            return gradient_penalty / ((ms // mt) + (ms % mt != 0)) \n",
    "        \n",
    "        # For balanced batch\n",
    "        alpha = torch.rand(source_data.size(0), 1).to(self.device)\n",
    "        interpolates = (alpha * source_data + ((1 - alpha) * target_data)).requires_grad_(True)\n",
    "        \n",
    "        # Domain critic outputs\n",
    "        dc_output = self.critic(interpolates)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=dc_output,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "\n",
    "        # Compute gradient penalty\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "    def train(self, source_loader: DataLoader, target_loader: DataLoader, num_epochs: int = 100, dc_iter: int = 100) -> List[float]:\n",
    "        self.generator.train()\n",
    "        self.critic.train()\n",
    "        losses = []\n",
    "        source_critic_scores = []\n",
    "        target_critic_scores = []\n",
    "        for epoch in trange(num_epochs, desc='Epoch'):\n",
    "            loss = 0\n",
    "            for (source_data, _), (target_data, _) in zip(source_loader, target_loader):\n",
    "                source_data, target_data = source_data.to(self.device), target_data.to(self.device)\n",
    "\n",
    "                # Train domain critic\n",
    "                for _ in range(dc_iter):\n",
    "                    self.critic_optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        source_features = self.generator(source_data)\n",
    "                        target_features = self.generator(target_data)\n",
    "                    \n",
    "                    # Compute empirical Wasserstein distance\n",
    "                    dc_source = self.critic(source_features)\n",
    "                    dc_target = self.critic(target_features)\n",
    "                    wasserstein_distance = dc_source.mean() - dc_target.mean()\n",
    "\n",
    "                    # Gradient penalty\n",
    "                    gradient_penalty = self.compute_gradient_penalty(source_features, target_features)\n",
    "\n",
    "                    # Domain critic loss\n",
    "                    dc_loss = - wasserstein_distance + self.gamma * gradient_penalty\n",
    "                    dc_loss.backward()\n",
    "                    self.critic_optimizer.step()\n",
    "\n",
    "                # Train feature extractor\n",
    "                self.generator_optimizer.zero_grad()\n",
    "                source_features = self.generator(source_data)\n",
    "                target_features = self.generator(target_data)\n",
    "                dc_source = self.critic(source_features)\n",
    "                dc_target = self.critic(target_features)\n",
    "                wasserstein_distance = dc_source.mean() - dc_target.mean()\n",
    "                wasserstein_distance.backward()\n",
    "                self.generator_optimizer.step()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    loss += wasserstein_distance.item()\n",
    "                    \n",
    "            source_critic_scores.append(self.criticize(source_loader.dataset.tensors[0].to(self.device)))\n",
    "            target_critic_scores.append(self.criticize(target_loader.dataset.tensors[0].to(self.device)))\n",
    "            losses.append(loss/len(source_loader))\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} | Loss: {wasserstein_distance.item()}')\n",
    "        return losses, source_critic_scores, target_critic_scores\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract_feature(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.generator.eval()\n",
    "        return self.generator(x)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def criticize(self, x: torch.Tensor) -> float:\n",
    "        self.generator.eval()\n",
    "        self.critic.eval()\n",
    "        return self.critic(self.generator(x)).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of the WDGRL model (same architecture as before)\n",
    "model = WDGRL(input_dim=1,generator_hidden_dims=[10, 10, 10], critic_hidden_dims=[10])\n",
    "\n",
    "# Load the saved checkpoint\n",
    "checkpoint = torch.load(\"wdgrl.pth\", map_location=model.device, weights_only=True)\n",
    "\n",
    "# Restore the model weights\n",
    "model.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "model.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sum(X):\n",
    "    return np.argmax(np.sum(X, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpmath import mp\n",
    "\n",
    "mp.dps = 500\n",
    "def truncated_cdf(etajTy, mu, sigma, left, right):\n",
    "    numerator = mp.ncdf((etajTy - mu) / sigma) - mp.ncdf((left - mu) / sigma)\n",
    "    denominator = mp.ncdf((right - mu) / sigma) - mp.ncdf((left - mu) / sigma)\n",
    "    if denominator <= 1e-16:\n",
    "        true_cdf = 1\n",
    "    else:\n",
    "        true_cdf = numerator / denominator \n",
    "    return true_cdf\n",
    "def intersect(itv1, itv2):\n",
    "    # print(itv1, itv2)\n",
    "    itv = [max(itv1[0], itv2[0]), min(itv1[1], itv2[1])]\n",
    "    if itv[0] > itv[1]:\n",
    "        return None    \n",
    "    return itv\n",
    "\n",
    "def solve_linear_inequality(u, v): #u + vz < 0\n",
    "    if (v > -1e-16 and v < 1e-16):\n",
    "        v = 0\n",
    "        if (u < 0):\n",
    "            return [-np.Inf, np.Inf]\n",
    "        else:\n",
    "            print('error')\n",
    "            return None\n",
    "    if (v < 0):\n",
    "        return [-u/v, np.Inf]\n",
    "    return [np.NINF, -u/v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tpr():\n",
    "    ns, nt, d = 100, 100, 1\n",
    "    mu_s, mu_t = 0, 20\n",
    "    delta_s, delta_t = [1], [1]\n",
    "    xs, ys = gen_data(mu_s, delta_s, ns, d)\n",
    "    xt, yt = gen_data(mu_t, delta_t, nt, d)\n",
    "\n",
    "    X = np.vstack((xs, xt))\n",
    "    O = max_sum(X)\n",
    "    if yt[O-ns] == 0:\n",
    "        return None\n",
    "    if O < 100:\n",
    "        return None\n",
    "    O = [O-100]\n",
    "    Oc = list(np.where(yt == 0)[0])\n",
    "    j = np.random.choice(O, 1, replace=False)[0]\n",
    "    etj = np.zeros((nt, 1))\n",
    "    etj[j][0] = 1\n",
    "    etOc = np.zeros((nt, 1))\n",
    "    etOc[Oc] = 1\n",
    "    etaj = np.vstack((np.zeros((ns, 1)), etj-(1/len(Oc))*etOc))\n",
    "\n",
    "    etajTX = etaj.T.dot(X)\n",
    "    print(f'Anomaly index: {O[0] + ns}')\n",
    "    print(f'etajTX: {etajTX}')\n",
    "    mu = np.vstack((np.full((ns,1), mu_s), np.full((nt,1), mu_t)))\n",
    "    sigma = np.identity(ns+nt)\n",
    "    etajTmu = etaj.T.dot(mu)\n",
    "    etajTsigmaetaj = etaj.T.dot(sigma).dot(etaj)\n",
    "\n",
    "    b = sigma.dot(etaj).dot(np.linalg.inv(etajTsigmaetaj))\n",
    "    a = (np.identity(ns+nt) - b.dot(etaj.T)).dot(X)\n",
    "\n",
    "    ao = a[O[0]+100][0]\n",
    "    bo = b[O[0]+100][0]\n",
    "    itv = [np.NINF, np.inf]\n",
    "    for i in range(X.shape[0]):\n",
    "        if (i != O[0]+100):\n",
    "            ai = a[i][0]\n",
    "            bi = b[i][0]\n",
    "            sub_itv = solve_linear_inequality(ai-ao, bi-bo)\n",
    "            itv = intersect(itv, sub_itv)   \n",
    "    cdf = truncated_cdf(etajTX[0][0], etajTmu[0][0], np.sqrt(etajTsigmaetaj[0][0]), itv[0], itv[1])\n",
    "    p_value = float(2 * min(cdf, 1 - cdf))\n",
    "    print(f'p-value: {p_value}')\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #0:\n",
      "Anomaly index: 166\n",
      "etajTX: [[2.78752533]]\n",
      "p-value: 0.19974414076215713\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #2:\n",
      "Anomaly index: 116\n",
      "etajTX: [[2.9473039]]\n",
      "p-value: 0.8011840361250182\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #3:\n",
      "Anomaly index: 144\n",
      "etajTX: [[2.72331181]]\n",
      "p-value: 0.7326964210484729\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #4:\n",
      "Anomaly index: 167\n",
      "etajTX: [[2.14711003]]\n",
      "p-value: 0.8262532897223968\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #5:\n",
      "Anomaly index: 164\n",
      "etajTX: [[2.40541904]]\n",
      "p-value: 0.6720795188432941\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #6:\n",
      "Anomaly index: 167\n",
      "etajTX: [[2.57990141]]\n",
      "p-value: 0.6199264563916976\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #7:\n",
      "Anomaly index: 128\n",
      "etajTX: [[3.75418617]]\n",
      "p-value: 0.5525170618455698\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #8:\n",
      "Anomaly index: 199\n",
      "etajTX: [[3.02205437]]\n",
      "p-value: 0.8190386444378464\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #9:\n",
      "Anomaly index: 117\n",
      "etajTX: [[2.58371715]]\n",
      "p-value: 0.578984299311611\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #10:\n",
      "Anomaly index: 169\n",
      "etajTX: [[2.65990557]]\n",
      "p-value: 0.9369668328230585\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #11:\n",
      "Anomaly index: 128\n",
      "etajTX: [[2.87885878]]\n",
      "p-value: 0.12576144548173573\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #12:\n",
      "Anomaly index: 120\n",
      "etajTX: [[2.82480434]]\n",
      "p-value: 0.43528372221845607\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #13:\n",
      "Anomaly index: 117\n",
      "etajTX: [[3.39002029]]\n",
      "p-value: 0.47717566717582305\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #14:\n",
      "Anomaly index: 193\n",
      "etajTX: [[2.70119492]]\n",
      "p-value: 0.5298781541690588\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #15:\n",
      "Anomaly index: 132\n",
      "etajTX: [[3.03190364]]\n",
      "p-value: 0.7565926759733169\n",
      "TPR: 0.0\n",
      "-------------------------------------------------\n",
      "iteration #16:\n",
      "Anomaly index: 139\n",
      "etajTX: [[4.26206412]]\n",
      "p-value: 0.005880761676743484\n",
      "TPR: 0.0625\n",
      "-------------------------------------------------\n",
      "iteration #17:\n",
      "Anomaly index: 141\n",
      "etajTX: [[2.66773169]]\n",
      "p-value: 0.47988949932711095\n",
      "TPR: 0.058823529411764705\n",
      "-------------------------------------------------\n",
      "iteration #18:\n",
      "Anomaly index: 156\n",
      "etajTX: [[3.77747024]]\n",
      "p-value: 0.05298649176742579\n",
      "TPR: 0.05555555555555555\n",
      "-------------------------------------------------\n",
      "iteration #19:\n",
      "Anomaly index: 124\n",
      "etajTX: [[2.83496529]]\n",
      "p-value: 0.8256615131279432\n",
      "TPR: 0.05263157894736842\n",
      "-------------------------------------------------\n",
      "iteration #20:\n",
      "Anomaly index: 198\n",
      "etajTX: [[2.81515336]]\n",
      "p-value: 0.3861254606579119\n",
      "TPR: 0.05\n",
      "-------------------------------------------------\n",
      "iteration #21:\n",
      "Anomaly index: 185\n",
      "etajTX: [[3.78370877]]\n",
      "p-value: 0.21883053336763064\n",
      "TPR: 0.047619047619047616\n",
      "-------------------------------------------------\n",
      "iteration #22:\n",
      "Anomaly index: 103\n",
      "etajTX: [[2.72709077]]\n",
      "p-value: 0.6948212316466007\n",
      "TPR: 0.045454545454545456\n",
      "-------------------------------------------------\n",
      "iteration #23:\n",
      "Anomaly index: 121\n",
      "etajTX: [[3.61431484]]\n",
      "p-value: 0.7401729447219834\n",
      "TPR: 0.043478260869565216\n",
      "-------------------------------------------------\n",
      "iteration #24:\n",
      "Anomaly index: 147\n",
      "etajTX: [[2.59473814]]\n",
      "p-value: 0.36715977524963533\n",
      "TPR: 0.041666666666666664\n",
      "-------------------------------------------------\n",
      "iteration #25:\n",
      "Anomaly index: 121\n",
      "etajTX: [[3.10250199]]\n",
      "p-value: 0.45741650338059336\n",
      "TPR: 0.04\n",
      "-------------------------------------------------\n",
      "iteration #26:\n",
      "Anomaly index: 106\n",
      "etajTX: [[2.5198191]]\n",
      "p-value: 0.8037196706091962\n",
      "TPR: 0.038461538461538464\n",
      "-------------------------------------------------\n",
      "iteration #27:\n",
      "Anomaly index: 127\n",
      "etajTX: [[2.11060528]]\n",
      "p-value: 0.7193780780387666\n",
      "TPR: 0.037037037037037035\n",
      "-------------------------------------------------\n",
      "iteration #28:\n",
      "Anomaly index: 166\n",
      "etajTX: [[3.24253578]]\n",
      "p-value: 0.21373435718436593\n",
      "TPR: 0.03571428571428571\n",
      "-------------------------------------------------\n",
      "iteration #29:\n",
      "Anomaly index: 195\n",
      "etajTX: [[3.37586981]]\n",
      "p-value: 0.15538550961698117\n",
      "TPR: 0.034482758620689655\n",
      "-------------------------------------------------\n",
      "iteration #30:\n",
      "Anomaly index: 179\n",
      "etajTX: [[2.74006375]]\n",
      "p-value: 0.7719005945520554\n",
      "TPR: 0.03333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #31:\n",
      "Anomaly index: 162\n",
      "etajTX: [[2.65028045]]\n",
      "p-value: 0.6263048899378811\n",
      "TPR: 0.03225806451612903\n",
      "-------------------------------------------------\n",
      "iteration #32:\n",
      "Anomaly index: 163\n",
      "etajTX: [[3.13463412]]\n",
      "p-value: 0.28060578099697553\n",
      "TPR: 0.03125\n",
      "-------------------------------------------------\n",
      "iteration #33:\n",
      "Anomaly index: 118\n",
      "etajTX: [[3.25729787]]\n",
      "p-value: 0.38546262102418277\n",
      "TPR: 0.030303030303030304\n",
      "-------------------------------------------------\n",
      "iteration #34:\n",
      "Anomaly index: 190\n",
      "etajTX: [[2.5351679]]\n",
      "p-value: 0.7098944319229831\n",
      "TPR: 0.029411764705882353\n",
      "-------------------------------------------------\n",
      "iteration #35:\n",
      "Anomaly index: 133\n",
      "etajTX: [[3.71299011]]\n",
      "p-value: 0.022537434901672666\n",
      "TPR: 0.05714285714285714\n",
      "-------------------------------------------------\n",
      "iteration #36:\n",
      "Anomaly index: 141\n",
      "etajTX: [[2.22123307]]\n",
      "p-value: 0.27300260854952196\n",
      "TPR: 0.05555555555555555\n",
      "-------------------------------------------------\n",
      "iteration #37:\n",
      "Anomaly index: 100\n",
      "etajTX: [[3.12947724]]\n",
      "p-value: 0.3354750459522459\n",
      "TPR: 0.05405405405405406\n",
      "-------------------------------------------------\n",
      "iteration #38:\n",
      "Anomaly index: 129\n",
      "etajTX: [[3.42996087]]\n",
      "p-value: 0.16843374628674507\n",
      "TPR: 0.05263157894736842\n",
      "-------------------------------------------------\n",
      "iteration #39:\n",
      "Anomaly index: 138\n",
      "etajTX: [[2.84052228]]\n",
      "p-value: 0.9767910575077534\n",
      "TPR: 0.05128205128205128\n",
      "-------------------------------------------------\n",
      "iteration #40:\n",
      "Anomaly index: 172\n",
      "etajTX: [[2.87222022]]\n",
      "p-value: 0.1425216804422787\n",
      "TPR: 0.05\n",
      "-------------------------------------------------\n",
      "iteration #41:\n",
      "Anomaly index: 131\n",
      "etajTX: [[2.68850192]]\n",
      "p-value: 0.4155366721685475\n",
      "TPR: 0.04878048780487805\n",
      "-------------------------------------------------\n",
      "iteration #42:\n",
      "Anomaly index: 160\n",
      "etajTX: [[2.56483284]]\n",
      "p-value: 0.7945317814334053\n",
      "TPR: 0.047619047619047616\n",
      "-------------------------------------------------\n",
      "iteration #43:\n",
      "Anomaly index: 151\n",
      "etajTX: [[2.79623642]]\n",
      "p-value: 0.7851532574071867\n",
      "TPR: 0.046511627906976744\n",
      "-------------------------------------------------\n",
      "iteration #44:\n",
      "Anomaly index: 199\n",
      "etajTX: [[2.60221842]]\n",
      "p-value: 0.9263826461432334\n",
      "TPR: 0.045454545454545456\n",
      "-------------------------------------------------\n",
      "iteration #45:\n",
      "Anomaly index: 139\n",
      "etajTX: [[3.17153964]]\n",
      "p-value: 0.4386515943202698\n",
      "TPR: 0.044444444444444446\n",
      "-------------------------------------------------\n",
      "iteration #46:\n",
      "Anomaly index: 117\n",
      "etajTX: [[3.18472396]]\n",
      "p-value: 0.2189257954377313\n",
      "TPR: 0.043478260869565216\n",
      "-------------------------------------------------\n",
      "iteration #47:\n",
      "Anomaly index: 170\n",
      "etajTX: [[3.47169031]]\n",
      "p-value: 0.1113827810792163\n",
      "TPR: 0.0425531914893617\n",
      "-------------------------------------------------\n",
      "iteration #48:\n",
      "Anomaly index: 117\n",
      "etajTX: [[2.96458137]]\n",
      "p-value: 0.6268000091668741\n",
      "TPR: 0.041666666666666664\n",
      "-------------------------------------------------\n",
      "iteration #49:\n",
      "Anomaly index: 186\n",
      "etajTX: [[2.69512152]]\n",
      "p-value: 0.24595468056064654\n",
      "TPR: 0.04081632653061224\n",
      "-------------------------------------------------\n",
      "iteration #50:\n",
      "Anomaly index: 145\n",
      "etajTX: [[4.00881011]]\n",
      "p-value: 0.05405305707265765\n",
      "TPR: 0.04\n",
      "-------------------------------------------------\n",
      "iteration #51:\n",
      "Anomaly index: 191\n",
      "etajTX: [[2.45409284]]\n",
      "p-value: 0.41286353723622154\n",
      "TPR: 0.0392156862745098\n",
      "-------------------------------------------------\n",
      "iteration #52:\n",
      "Anomaly index: 133\n",
      "etajTX: [[2.6697213]]\n",
      "p-value: 0.27472487334524565\n",
      "TPR: 0.038461538461538464\n",
      "-------------------------------------------------\n",
      "iteration #53:\n",
      "Anomaly index: 166\n",
      "etajTX: [[2.99564032]]\n",
      "p-value: 0.8551594146364222\n",
      "TPR: 0.03773584905660377\n",
      "-------------------------------------------------\n",
      "iteration #54:\n",
      "Anomaly index: 104\n",
      "etajTX: [[2.50556768]]\n",
      "p-value: 0.4729279878560536\n",
      "TPR: 0.037037037037037035\n",
      "-------------------------------------------------\n",
      "iteration #55:\n",
      "Anomaly index: 110\n",
      "etajTX: [[1.98275628]]\n",
      "p-value: 0.2831386467003112\n",
      "TPR: 0.03636363636363636\n",
      "-------------------------------------------------\n",
      "iteration #56:\n",
      "Anomaly index: 191\n",
      "etajTX: [[3.41898379]]\n",
      "p-value: 0.6213252552803028\n",
      "TPR: 0.03571428571428571\n",
      "-------------------------------------------------\n",
      "iteration #57:\n",
      "Anomaly index: 177\n",
      "etajTX: [[3.05964459]]\n",
      "p-value: 0.4748365053878428\n",
      "TPR: 0.03508771929824561\n",
      "-------------------------------------------------\n",
      "iteration #58:\n",
      "Anomaly index: 198\n",
      "etajTX: [[2.45370897]]\n",
      "p-value: 0.31326410211998135\n",
      "TPR: 0.034482758620689655\n",
      "-------------------------------------------------\n",
      "iteration #59:\n",
      "Anomaly index: 161\n",
      "etajTX: [[2.22280583]]\n",
      "p-value: 0.2795808494384851\n",
      "TPR: 0.03389830508474576\n",
      "-------------------------------------------------\n",
      "iteration #60:\n",
      "Anomaly index: 126\n",
      "etajTX: [[2.76261937]]\n",
      "p-value: 0.4881269511232821\n",
      "TPR: 0.03333333333333333\n",
      "-------------------------------------------------\n",
      "iteration #61:\n",
      "Anomaly index: 140\n",
      "etajTX: [[3.62148999]]\n",
      "p-value: 0.06086655086892957\n",
      "TPR: 0.03278688524590164\n",
      "-------------------------------------------------\n",
      "iteration #62:\n",
      "Anomaly index: 189\n",
      "etajTX: [[4.08887333]]\n",
      "p-value: 0.009574131792558862\n",
      "TPR: 0.04838709677419355\n",
      "-------------------------------------------------\n",
      "iteration #63:\n",
      "Anomaly index: 124\n",
      "etajTX: [[3.50969605]]\n",
      "p-value: 0.07512848944428539\n",
      "TPR: 0.047619047619047616\n",
      "-------------------------------------------------\n",
      "iteration #64:\n",
      "Anomaly index: 150\n",
      "etajTX: [[3.15512712]]\n",
      "p-value: 0.5502315263008797\n",
      "TPR: 0.046875\n",
      "-------------------------------------------------\n",
      "iteration #65:\n",
      "Anomaly index: 106\n",
      "etajTX: [[2.79751804]]\n",
      "p-value: 0.26782111816791565\n",
      "TPR: 0.046153846153846156\n",
      "-------------------------------------------------\n",
      "iteration #66:\n",
      "Anomaly index: 123\n",
      "etajTX: [[2.98100233]]\n",
      "p-value: 0.25231069922510463\n",
      "TPR: 0.045454545454545456\n",
      "-------------------------------------------------\n",
      "iteration #67:\n",
      "Anomaly index: 198\n",
      "etajTX: [[3.22422172]]\n",
      "p-value: 0.05909064393090635\n",
      "TPR: 0.04477611940298507\n",
      "-------------------------------------------------\n",
      "iteration #68:\n",
      "Anomaly index: 118\n",
      "etajTX: [[3.27089886]]\n",
      "p-value: 0.11815774247094447\n",
      "TPR: 0.04411764705882353\n",
      "-------------------------------------------------\n",
      "iteration #69:\n",
      "Anomaly index: 132\n",
      "etajTX: [[2.4410193]]\n",
      "p-value: 0.6642860549879142\n",
      "TPR: 0.043478260869565216\n",
      "-------------------------------------------------\n",
      "iteration #70:\n",
      "Anomaly index: 180\n",
      "etajTX: [[2.76194503]]\n",
      "p-value: 0.2563933429222447\n",
      "TPR: 0.04285714285714286\n",
      "-------------------------------------------------\n",
      "iteration #71:\n",
      "Anomaly index: 196\n",
      "etajTX: [[2.33464848]]\n",
      "p-value: 0.8387123367723227\n",
      "TPR: 0.04225352112676056\n",
      "-------------------------------------------------\n",
      "iteration #72:\n",
      "Anomaly index: 180\n",
      "etajTX: [[2.48063434]]\n",
      "p-value: 0.3339221753932548\n",
      "TPR: 0.041666666666666664\n",
      "-------------------------------------------------\n",
      "iteration #73:\n",
      "Anomaly index: 156\n",
      "etajTX: [[3.21697305]]\n",
      "p-value: 0.18835436723875745\n",
      "TPR: 0.0410958904109589\n",
      "-------------------------------------------------\n",
      "iteration #74:\n",
      "Anomaly index: 102\n",
      "etajTX: [[2.64440631]]\n",
      "p-value: 0.4186260219381769\n",
      "TPR: 0.04054054054054054\n",
      "-------------------------------------------------\n",
      "iteration #75:\n",
      "Anomaly index: 151\n",
      "etajTX: [[2.08269862]]\n",
      "p-value: 0.7435082464678205\n",
      "TPR: 0.04\n",
      "-------------------------------------------------\n",
      "iteration #76:\n",
      "Anomaly index: 106\n",
      "etajTX: [[3.63705143]]\n",
      "p-value: 0.08977873572719187\n",
      "TPR: 0.039473684210526314\n",
      "-------------------------------------------------\n",
      "iteration #77:\n",
      "Anomaly index: 169\n",
      "etajTX: [[2.54725018]]\n",
      "p-value: 0.33360756329395364\n",
      "TPR: 0.03896103896103896\n",
      "-------------------------------------------------\n",
      "iteration #78:\n",
      "Anomaly index: 113\n",
      "etajTX: [[2.42912577]]\n",
      "p-value: 0.47483495711920637\n",
      "TPR: 0.038461538461538464\n",
      "-------------------------------------------------\n",
      "iteration #79:\n",
      "Anomaly index: 132\n",
      "etajTX: [[3.82611789]]\n",
      "p-value: 0.015756969090883848\n",
      "TPR: 0.05063291139240506\n",
      "-------------------------------------------------\n",
      "iteration #80:\n",
      "Anomaly index: 197\n",
      "etajTX: [[2.86991888]]\n",
      "p-value: 0.5263504936520862\n",
      "TPR: 0.05\n",
      "-------------------------------------------------\n",
      "iteration #81:\n",
      "Anomaly index: 151\n",
      "etajTX: [[2.62475667]]\n",
      "p-value: 0.9896389524839144\n",
      "TPR: 0.04938271604938271\n",
      "-------------------------------------------------\n",
      "iteration #82:\n",
      "Anomaly index: 140\n",
      "etajTX: [[2.27329975]]\n",
      "p-value: 0.3021440273233801\n",
      "TPR: 0.04878048780487805\n",
      "-------------------------------------------------\n",
      "iteration #83:\n",
      "Anomaly index: 165\n",
      "etajTX: [[2.70162771]]\n",
      "p-value: 0.7605737330595347\n",
      "TPR: 0.04819277108433735\n",
      "-------------------------------------------------\n",
      "iteration #84:\n",
      "Anomaly index: 189\n",
      "etajTX: [[3.16554737]]\n",
      "p-value: 0.49495973721195696\n",
      "TPR: 0.047619047619047616\n",
      "-------------------------------------------------\n",
      "iteration #85:\n",
      "Anomaly index: 139\n",
      "etajTX: [[4.06164033]]\n",
      "p-value: 0.06233103938949036\n",
      "TPR: 0.047058823529411764\n",
      "-------------------------------------------------\n",
      "iteration #86:\n",
      "Anomaly index: 123\n",
      "etajTX: [[2.69363573]]\n",
      "p-value: 0.08549488971186453\n",
      "TPR: 0.046511627906976744\n",
      "-------------------------------------------------\n",
      "iteration #87:\n",
      "Anomaly index: 180\n",
      "etajTX: [[3.24781466]]\n",
      "p-value: 0.6331421578085245\n",
      "TPR: 0.04597701149425287\n",
      "-------------------------------------------------\n",
      "iteration #88:\n",
      "Anomaly index: 121\n",
      "etajTX: [[2.48874038]]\n",
      "p-value: 0.9224516764444434\n",
      "TPR: 0.045454545454545456\n",
      "-------------------------------------------------\n",
      "iteration #89:\n",
      "Anomaly index: 153\n",
      "etajTX: [[3.62166737]]\n",
      "p-value: 0.019757076034269426\n",
      "TPR: 0.056179775280898875\n",
      "-------------------------------------------------\n",
      "iteration #90:\n",
      "Anomaly index: 142\n",
      "etajTX: [[3.65669665]]\n",
      "p-value: 0.03591918007622341\n",
      "TPR: 0.06666666666666667\n",
      "-------------------------------------------------\n",
      "iteration #91:\n",
      "Anomaly index: 169\n",
      "etajTX: [[4.49143358]]\n",
      "p-value: 0.007376851704732099\n",
      "TPR: 0.07692307692307693\n",
      "-------------------------------------------------\n",
      "iteration #92:\n",
      "Anomaly index: 131\n",
      "etajTX: [[3.41702258]]\n",
      "p-value: 0.12346371567879157\n",
      "TPR: 0.07608695652173914\n",
      "-------------------------------------------------\n",
      "iteration #93:\n",
      "Anomaly index: 145\n",
      "etajTX: [[3.04959453]]\n",
      "p-value: 0.297957286103985\n",
      "TPR: 0.07526881720430108\n",
      "-------------------------------------------------\n",
      "iteration #94:\n",
      "Anomaly index: 156\n",
      "etajTX: [[2.13283797]]\n",
      "p-value: 0.6731878671164966\n",
      "TPR: 0.07446808510638298\n",
      "-------------------------------------------------\n",
      "iteration #95:\n",
      "Anomaly index: 101\n",
      "etajTX: [[2.35278926]]\n",
      "p-value: 0.6921118998537015\n",
      "TPR: 0.07368421052631578\n",
      "-------------------------------------------------\n",
      "iteration #96:\n",
      "Anomaly index: 182\n",
      "etajTX: [[3.42200753]]\n",
      "p-value: 0.08897758274417712\n",
      "TPR: 0.07291666666666667\n",
      "-------------------------------------------------\n",
      "iteration #97:\n",
      "Anomaly index: 175\n",
      "etajTX: [[2.36013236]]\n",
      "p-value: 0.5145223331189528\n",
      "TPR: 0.07216494845360824\n",
      "-------------------------------------------------\n",
      "iteration #98:\n",
      "Anomaly index: 192\n",
      "etajTX: [[2.45783132]]\n",
      "p-value: 0.9806245199684601\n",
      "TPR: 0.07142857142857142\n",
      "-------------------------------------------------\n",
      "iteration #99:\n",
      "Anomaly index: 113\n",
      "etajTX: [[2.47503397]]\n",
      "p-value: 0.6642045413560229\n",
      "TPR: 0.0707070707070707\n",
      "-------------------------------------------------\n",
      "iteration #100:\n",
      "Anomaly index: 176\n",
      "etajTX: [[2.92896993]]\n",
      "p-value: 0.7766292878319258\n",
      "TPR: 0.07\n",
      "-------------------------------------------------\n",
      "iteration #101:\n",
      "Anomaly index: 111\n",
      "etajTX: [[3.32427989]]\n",
      "p-value: 0.03958707598258837\n",
      "TPR: 0.07920792079207921\n",
      "-------------------------------------------------\n",
      "iteration #102:\n",
      "Anomaly index: 111\n",
      "etajTX: [[2.90313462]]\n",
      "p-value: 0.27233477157025876\n",
      "TPR: 0.0784313725490196\n",
      "-------------------------------------------------\n",
      "iteration #103:\n",
      "Anomaly index: 120\n",
      "etajTX: [[2.73712277]]\n",
      "p-value: 0.47779693849652466\n",
      "TPR: 0.07766990291262135\n",
      "-------------------------------------------------\n",
      "iteration #104:\n",
      "Anomaly index: 187\n",
      "etajTX: [[2.89687041]]\n",
      "p-value: 0.6655554034895348\n",
      "TPR: 0.07692307692307693\n",
      "-------------------------------------------------\n",
      "iteration #105:\n",
      "Anomaly index: 165\n",
      "etajTX: [[2.39525832]]\n",
      "p-value: 0.8412435194475412\n",
      "TPR: 0.0761904761904762\n",
      "-------------------------------------------------\n",
      "iteration #106:\n",
      "Anomaly index: 123\n",
      "etajTX: [[2.85548947]]\n",
      "p-value: 0.8939975189258912\n",
      "TPR: 0.07547169811320754\n",
      "-------------------------------------------------\n",
      "iteration #107:\n",
      "Anomaly index: 125\n",
      "etajTX: [[3.88817628]]\n",
      "p-value: 0.25104352324229456\n",
      "TPR: 0.07476635514018691\n",
      "-------------------------------------------------\n",
      "iteration #108:\n",
      "Anomaly index: 125\n",
      "etajTX: [[3.37191466]]\n",
      "p-value: 0.9217431059395343\n",
      "TPR: 0.07407407407407407\n",
      "-------------------------------------------------\n",
      "iteration #109:\n",
      "Anomaly index: 159\n",
      "etajTX: [[2.57866236]]\n",
      "p-value: 0.07271404647296358\n",
      "TPR: 0.07339449541284404\n",
      "-------------------------------------------------\n",
      "iteration #110:\n",
      "Anomaly index: 183\n",
      "etajTX: [[3.03095556]]\n",
      "p-value: 0.5466949102349805\n",
      "TPR: 0.07272727272727272\n",
      "-------------------------------------------------\n",
      "iteration #111:\n",
      "Anomaly index: 134\n",
      "etajTX: [[2.41389402]]\n",
      "p-value: 0.5207591578657585\n",
      "TPR: 0.07207207207207207\n",
      "-------------------------------------------------\n",
      "iteration #112:\n",
      "Anomaly index: 107\n",
      "etajTX: [[3.36866485]]\n",
      "p-value: 0.7100681388013378\n",
      "TPR: 0.07142857142857142\n",
      "-------------------------------------------------\n",
      "iteration #113:\n",
      "Anomaly index: 125\n",
      "etajTX: [[3.19887544]]\n",
      "p-value: 0.4722835696228233\n",
      "TPR: 0.07079646017699115\n",
      "-------------------------------------------------\n",
      "iteration #114:\n",
      "Anomaly index: 106\n",
      "etajTX: [[2.81484025]]\n",
      "p-value: 0.8846512904809277\n",
      "TPR: 0.07017543859649122\n",
      "-------------------------------------------------\n",
      "iteration #115:\n",
      "Anomaly index: 191\n",
      "etajTX: [[3.39610102]]\n",
      "p-value: 0.6622319706054106\n",
      "TPR: 0.06956521739130435\n",
      "-------------------------------------------------\n",
      "iteration #116:\n",
      "Anomaly index: 125\n",
      "etajTX: [[2.50973041]]\n",
      "p-value: 0.6851257398040294\n",
      "TPR: 0.06896551724137931\n",
      "-------------------------------------------------\n",
      "iteration #117:\n",
      "Anomaly index: 165\n",
      "etajTX: [[3.29615426]]\n",
      "p-value: 0.15844559407977055\n",
      "TPR: 0.06837606837606838\n",
      "-------------------------------------------------\n",
      "iteration #118:\n",
      "Anomaly index: 126\n",
      "etajTX: [[2.84544636]]\n",
      "p-value: 0.9899836391378737\n",
      "TPR: 0.06779661016949153\n",
      "-------------------------------------------------\n",
      "iteration #119:\n",
      "Anomaly index: 194\n",
      "etajTX: [[2.8352523]]\n",
      "p-value: 0.33398389572035986\n",
      "TPR: 0.06722689075630252\n",
      "-------------------------------------------------\n",
      "iteration #120:\n",
      "Anomaly index: 153\n",
      "etajTX: [[3.03218296]]\n",
      "p-value: 0.17742495525379312\n",
      "TPR: 0.06666666666666667\n",
      "-------------------------------------------------\n",
      "iteration #121:\n",
      "Anomaly index: 153\n",
      "etajTX: [[2.31616914]]\n",
      "p-value: 0.6190039116335194\n",
      "TPR: 0.06611570247933884\n",
      "-------------------------------------------------\n",
      "iteration #122:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "max_iteration = 120\n",
    "alpha = 0.05\n",
    "list_p_value = []\n",
    "count = 0\n",
    "print(f'iteration #{0}:')\n",
    "\n",
    "\n",
    "while len(list_p_value) <= max_iteration:\n",
    "    p_value = run_tpr()\n",
    "    if p_value is None:\n",
    "        continue\n",
    "    list_p_value.append(p_value)\n",
    "    if p_value <= alpha:\n",
    "        count += 1\n",
    "    print(f'TPR: {count / len(list_p_value)}')\n",
    "    print('-------------------------------------------------')\n",
    "    print(f'iteration #{len(list_p_value)+1}:')\n",
    "plt.hist(list_p_value)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
