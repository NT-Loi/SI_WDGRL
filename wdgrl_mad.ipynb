{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def gen_data(mu, delta, n, d: int = 2):\n",
    "    noise = np.random.normal(loc = 0, scale = 1, size=(n, d))\n",
    "    mu = np.full((n, d), mu, dtype=np.float64)\n",
    "\n",
    "    if len(delta) == 1 and delta[0] == 0:\n",
    "        return mu + noise, np.zeros(n)\n",
    "    \n",
    "    # 10% of the data are abnormal\n",
    "    m = len(delta)\n",
    "    abnormal_idx = np.random.choice(n, int(n/10), replace=False)\n",
    "\n",
    "    ptr = 0\n",
    "    for i in range(m):\n",
    "        for j in range(len(abnormal_idx)//m):\n",
    "            mu[abnormal_idx[ptr], :] += delta[i]\n",
    "            ptr += 1\n",
    "    \n",
    "    X = mu + noise \n",
    "    Y = np.zeros(n)\n",
    "    Y[abnormal_idx] = 1\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"Feature extractor network.\"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"Domain critic network.\"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class WDGRL():\n",
    "    def __init__(self, input_dim: int=2, generator_hidden_dims: List[int]=[32, 16, 8, 4, 2], critic_hidden_dims: List[int]=[32, 16, 8, 4, 2],\n",
    "                 gamma: float = 0.1, _lr_generator: float = 1e-2, _lr_critic: float = 1e-2, \n",
    "                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.generator = Generator(input_dim, generator_hidden_dims).to(self.device)\n",
    "        self.critic = Critic(generator_hidden_dims[-1], critic_hidden_dims).to(self.device)\n",
    "        self.generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=_lr_generator)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=_lr_critic)\n",
    "    \n",
    "    def compute_gradient_penalty(self, source_data: torch.Tensor, target_data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute gradient penalty.\"\"\"\n",
    "        if source_data.size(0) > target_data.size(0):\n",
    "            ms = source_data.size(0)\n",
    "            mt = target_data.size(0)\n",
    "            gradient_penalty = 0\n",
    "            for _ in range(0, ms, mt):\n",
    "                source_chunk = source_data[_:_+mt]\n",
    "                target_chunk = target_data\n",
    "                alpha = torch.rand(target_chunk.size(0), 1).to(self.device)\n",
    "                interpolates = (alpha * source_chunk + ((1 - alpha) * target_chunk)).requires_grad_(True)\n",
    "                \n",
    "                # Domain critic outputs\n",
    "                dc_output = self.critic(interpolates)\n",
    "                \n",
    "                # Compute gradients\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=dc_output,\n",
    "                    inputs=interpolates,\n",
    "                    grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True,\n",
    "                    only_inputs=True,\n",
    "                )\n",
    "                gradients = gradients[0]\n",
    "                gradient_penalty += ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "            if ms % mt != 0:\n",
    "                source_chunk = source_data[ms-mt:]\n",
    "                perm = torch.randperm(mt)\n",
    "                idx = perm[:ms % mt]\n",
    "                target_chunk = target_data[idx]\n",
    "                alpha = torch.rand(target_chunk.size(0), 1).to(self.device)\n",
    "                interpolates = (alpha * source_chunk + ((1 - alpha) * target_chunk)).requires_grad_(True)\n",
    "                \n",
    "                # Domain critic outputs\n",
    "                dc_output = self.critic(interpolates)\n",
    "                \n",
    "                # Compute gradients\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=dc_output,\n",
    "                    inputs=interpolates,\n",
    "                    grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True,\n",
    "                    only_inputs=True,\n",
    "                )\n",
    "                gradients = gradients[0]\n",
    "                gradient_penalty += ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "            return gradient_penalty / ((ms // mt) + (ms % mt != 0)) \n",
    "        \n",
    "        # For balanced batch\n",
    "        alpha = torch.rand(source_data.size(0), 1).to(self.device)\n",
    "        interpolates = (alpha * source_data + ((1 - alpha) * target_data)).requires_grad_(True)\n",
    "        \n",
    "        # Domain critic outputs\n",
    "        dc_output = self.critic(interpolates)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=dc_output,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "\n",
    "        # Compute gradient penalty\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "    def train(self, source_loader: DataLoader, target_loader: DataLoader, num_epochs: int = 100, dc_iter: int = 100) -> List[float]:\n",
    "        self.generator.train()\n",
    "        self.critic.train()\n",
    "        losses = []\n",
    "        source_critic_scores = []\n",
    "        target_critic_scores = []\n",
    "        for epoch in trange(num_epochs, desc='Epoch'):\n",
    "            loss = 0\n",
    "            for (source_data, _), (target_data, _) in zip(source_loader, target_loader):\n",
    "                source_data, target_data = source_data.to(self.device), target_data.to(self.device)\n",
    "\n",
    "                # Train domain critic\n",
    "                for _ in range(dc_iter):\n",
    "                    self.critic_optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        source_features = self.generator(source_data)\n",
    "                        target_features = self.generator(target_data)\n",
    "                    \n",
    "                    # Compute empirical Wasserstein distance\n",
    "                    dc_source = self.critic(source_features)\n",
    "                    dc_target = self.critic(target_features)\n",
    "                    wasserstein_distance = dc_source.mean() - dc_target.mean()\n",
    "\n",
    "                    # Gradient penalty\n",
    "                    gradient_penalty = self.compute_gradient_penalty(source_features, target_features)\n",
    "\n",
    "                    # Domain critic loss\n",
    "                    dc_loss = - wasserstein_distance + self.gamma * gradient_penalty\n",
    "                    dc_loss.backward()\n",
    "                    self.critic_optimizer.step()\n",
    "\n",
    "                # Train feature extractor\n",
    "                self.generator_optimizer.zero_grad()\n",
    "                source_features = self.generator(source_data)\n",
    "                target_features = self.generator(target_data)\n",
    "                dc_source = self.critic(source_features)\n",
    "                dc_target = self.critic(target_features)\n",
    "                wasserstein_distance = dc_source.mean() - dc_target.mean()\n",
    "                wasserstein_distance.backward()\n",
    "                self.generator_optimizer.step()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    loss += wasserstein_distance.item()\n",
    "                    \n",
    "            source_critic_scores.append(self.criticize(source_loader.dataset.tensors[0].to(self.device)))\n",
    "            target_critic_scores.append(self.criticize(target_loader.dataset.tensors[0].to(self.device)))\n",
    "            losses.append(loss/len(source_loader))\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} | Loss: {wasserstein_distance.item()}')\n",
    "        return losses, source_critic_scores, target_critic_scores\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract_feature(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.generator.eval()\n",
    "        return self.generator(x)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def criticize(self, x: torch.Tensor) -> float:\n",
    "        self.generator.eval()\n",
    "        self.critic.eval()\n",
    "        return self.critic(self.generator(x)).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of the WDGRL model (same architecture as before)\n",
    "model = WDGRL(input_dim=1,generator_hidden_dims=[10, 10, 10], critic_hidden_dims=[10])\n",
    "\n",
    "# Load the saved checkpoint\n",
    "checkpoint = torch.load(\"wdgrl.pth\", map_location=model.device, weights_only=True)\n",
    "\n",
    "# Restore the model weights\n",
    "model.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "model.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Create synthetic dataset and dataloaders for domain adaptation.\"\"\"\n",
    "# Create datasets\n",
    "ns, nt, d = 100, 10, 1\n",
    "mu_s, mu_t = 0, 20\n",
    "delta_s, delta_t = [4], [0]\n",
    "xs, ys = gen_data(mu_s, delta_s, ns, d)\n",
    "xt, yt = gen_data(mu_t, delta_t, nt, d)\n",
    "\n",
    "plt.scatter(xs[:, 0], np.zeros_like(xs[:, 0]), c=ys, cmap='viridis', s=2)\n",
    "plt.scatter(xt[:, 0], np.zeros_like(xt[:, 0]), c=yt, cmap='cool', s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.FloatTensor(xs)\n",
    "ys = torch.LongTensor(ys)\n",
    "xt = torch.FloatTensor(xt)\n",
    "yt = torch.LongTensor(yt)\n",
    "xs_hat = model.extract_feature(xs.cuda())\n",
    "xt_hat = model.extract_feature(xt.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat = torch.cat([xs_hat, xt_hat], dim=0)\n",
    "O = []\n",
    "alpha = 0.1\n",
    "for i in range(x_hat.shape[1]):\n",
    "    x_hat_i = x_hat[:, i]\n",
    "    median = x_hat_i.median().item()\n",
    "    mad = torch.abs(x_hat_i - median).median().item()\n",
    "    lower = median - alpha * mad\n",
    "    upper = median + alpha * mad\n",
    "    for j in range(nt):\n",
    "        value = xt_hat[j, i].item()\n",
    "        if (value < lower or value > upper) and j not in O:\n",
    "            O.append(j)\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpmath import mp\n",
    "\n",
    "mp.dps = 500\n",
    "def truncated_cdf(etajTy, mu, sigma, left, right):\n",
    "    numerator = mp.ncdf((etajTy - mu) / sigma) - mp.ncdf((left - mu) / sigma)\n",
    "    denominator = mp.ncdf((right - mu) / sigma) - mp.ncdf((left - mu) / sigma)\n",
    "    if denominator <= 1e-16:\n",
    "        true_cdf = 1\n",
    "    else:\n",
    "        true_cdf = numerator / denominator \n",
    "    return true_cdf\n",
    "\n",
    "def mad_ad(xs_hat, xt_hat, alpha):\n",
    "    x_hat = torch.cat([xs_hat, xt_hat], dim=0)\n",
    "    O = []\n",
    "    for i in range(x_hat.shape[1]):\n",
    "        x_hat_i = x_hat[:, i]\n",
    "        median = x_hat_i.median().item()\n",
    "        mad = torch.abs(x_hat_i - median).median().item()\n",
    "        lower = median - alpha * mad\n",
    "        upper = median + alpha * mad\n",
    "        for j in range(nt):\n",
    "            value = xt_hat[j, i].item()\n",
    "            if (value < lower or value > upper) and j not in O:\n",
    "                O.append(j)\n",
    "    return O\n",
    "\n",
    "def intersect(itv1, itv2):\n",
    "    # print(itv1, itv2)\n",
    "    itv = [max(itv1[0], itv2[0]), min(itv1[1], itv2[1])]\n",
    "    if itv[0] > itv[1]:\n",
    "        return None    \n",
    "    return itv\n",
    "\n",
    "def solve_linear_inequality(u, v): #u + vz < 0\n",
    "    if (v > -1e-16 and v < 1e-16):\n",
    "        v = 0\n",
    "        if (u < 0):\n",
    "            return [-np.Inf, np.Inf]\n",
    "        else:\n",
    "            print('error')\n",
    "            return None\n",
    "    if (v < 0):\n",
    "        return [-u/v, np.Inf]\n",
    "    return [np.NINF, -u/v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fpr():\n",
    "    ns, nt, d = 100, 10, 1\n",
    "    mu_s, mu_t = 0, 20\n",
    "    delta_s, delta_t = [4], [0]\n",
    "    xs, ys = gen_data(mu_s, delta_s, ns, d)\n",
    "    xt, yt = gen_data(mu_t, delta_t, nt, d)\n",
    "\n",
    "    xs = torch.FloatTensor(xs)\n",
    "    ys = torch.LongTensor(ys)\n",
    "    xt = torch.FloatTensor(xt)\n",
    "    yt = torch.LongTensor(yt)\n",
    "\n",
    "    xs = xs.cuda()\n",
    "    xt = xt.cuda()\n",
    "    ys = ys.cuda()\n",
    "    yt = yt.cuda()\n",
    "\n",
    "    xs_hat = model.extract_feature(xs)\n",
    "    xt_hat = model.extract_feature(xt)\n",
    "    x_hat = torch.cat([xs_hat, xt_hat], dim=0)\n",
    "\n",
    "    xs_hat = xs_hat.cpu()\n",
    "    xt_hat = xt_hat.cpu()\n",
    "    x_hat = x_hat.cpu()\n",
    "    xs = xs.cpu()\n",
    "    xt = xt.cpu()\n",
    "    ys = ys.cpu()\n",
    "    yt = yt.cpu()\n",
    "\n",
    "    X = np.vstack((xs, xt))\n",
    "    O = mad_ad(xs_hat, xt_hat, 1.5)\n",
    "    Oc = list(np.where(yt == 0)[0])\n",
    "    j = np.random.choice(O, 1, replace=False)[0]\n",
    "    etj = np.zeros((nt, 1))\n",
    "    etj[j][0] = 1\n",
    "    etOc = np.zeros((nt, 1))\n",
    "    etOc[Oc] = 1\n",
    "    etaj = np.vstack((np.zeros((ns, 1)), etj-(1/len(Oc))*etOc))\n",
    "\n",
    "    etajTX = etaj.T.dot(X)\n",
    "    print(f'Anomaly index: {j}')\n",
    "    print(f'etajTX: {etajTX}')\n",
    "    mu = np.vstack((np.full((ns,1), mu_s), np.full((nt,1), mu_t)))\n",
    "    sigma = np.identity(ns+nt)\n",
    "    etajTmu = etaj.T.dot(mu)\n",
    "    etajTsigmaetaj = etaj.T.dot(sigma).dot(etaj)\n",
    "\n",
    "    b = sigma.dot(etaj).dot(np.linalg.inv(etajTsigmaetaj))\n",
    "    a = (np.identity(ns+nt) - b.dot(etaj.T)).dot(X)\n",
    "\n",
    "    itv = [np.NINF, np.Inf]\n",
    "\n",
    "    for i in range(x_hat.shape[1]):\n",
    "        x_hat_i = x_hat[:, i].numpy()\n",
    "        k1 = np.median(x_hat_i)\n",
    "        for j in range(x_hat_i.shape[0]):\n",
    "\n",
    "    return 0\n",
    "    cdf = truncated_cdf(etajTX[0][0], etajTmu[0][0], np.sqrt(etajTsigmaetaj[0][0]), itv[0], itv[1])\n",
    "    p_value = float(2 * min(cdf, 1 - cdf))\n",
    "    print(f'p-value: {p_value}')\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #0:\n",
      "Anomaly index: 7\n",
      "etajTX: [[1.48454266]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.34470963 0.33251452 0.42428955 0.33919615 0.38300157 0.3134258\n",
      " 0.34727937 0.3117364  0.31154403 0.47126177 0.48426217 0.36915165\n",
      " 0.32545385 0.45983952 0.4208119  0.31117064 0.4702609  0.33028886\n",
      " 0.3734065  0.30964047 0.39276806 0.44170117 0.41585505 0.32167092\n",
      " 0.3102232  0.36730075 0.32448947 0.48249796 0.36895987 0.431384\n",
      " 0.4298039  0.39364535 0.38298804 0.31686124 0.32205674 0.45369792\n",
      " 0.37934673 0.30967927 0.42772183 0.38115165 0.40197808 0.40731692\n",
      " 0.32707542 0.39763474 0.5129658  0.41312563 0.33776537 0.34186232\n",
      " 0.3965057  0.33386412 0.32106838 0.44003302 0.33375296 0.31047508\n",
      " 0.33731645 0.3551898  0.31121856 0.36859983 0.48649114 0.3436435\n",
      " 0.35184923 0.46029654 0.4456673  0.52196026 0.31175655 0.3601199\n",
      " 0.44569862 0.31850877 0.3468506  0.336983   0.34654447 0.53387934\n",
      " 0.5626147  0.36460626 0.31101283 0.35861537 0.4297849  0.39644367\n",
      " 0.46156442 0.4498163  0.48901552 0.41566795 0.45834106 0.41858587\n",
      " 0.42786008 0.47335377 0.3265501  0.49107578 0.40514162 0.3096357\n",
      " 0.36042655 0.34172124 0.39536563 0.5424883  0.3604759  0.44642273\n",
      " 0.31779876 0.31476805 0.4546628  0.36320493 0.39437515 0.3948756\n",
      " 0.3988009  0.39831874 0.39552957 0.39801884 0.39565593 0.40433627\n",
      " 0.39726654 0.39942998]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.40812638 0.4068616  0.43960646 0.4073044  0.42071438 0.4073555\n",
      " 0.40852    0.40126815 0.40147933 0.4542366  0.45811805 0.41458368\n",
      " 0.4095718  0.45082632 0.43802857 0.4026008  0.45393777 0.40691918\n",
      " 0.41624668 0.4064517  0.42530465 0.44541085 0.43577957 0.40714216\n",
      " 0.4044666  0.41386023 0.4094343  0.45759133 0.4145087  0.4423305\n",
      " 0.44185877 0.4257027  0.42070764 0.40834653 0.4090874  0.44899267\n",
      " 0.4188915  0.40553778 0.44116372 0.41979173 0.42948338 0.4319057\n",
      " 0.40700233 0.42751274 0.46668795 0.4345412  0.4070911  0.4077019\n",
      " 0.42700046 0.4068267  0.40715775 0.4449128  0.40682957 0.40397054\n",
      " 0.40702417 0.4102448  0.40250644 0.41436797 0.45878354 0.40796745\n",
      " 0.40951642 0.45096278 0.44659498 0.46937338 0.40125486 0.41131973\n",
      " 0.44660437 0.40858147 0.40844557 0.40697446 0.40839994 0.47293198\n",
      " 0.48151132 0.41280708 0.40741792 0.4109917  0.44185308 0.42697233\n",
      " 0.4513413  0.44783372 0.4595372  0.4356947  0.45037895 0.4370186\n",
      " 0.44122645 0.4548612  0.4097281  0.46015236 0.43091872 0.4063977\n",
      " 0.4113866  0.40768087 0.4264832  0.47550228 0.41139737 0.44682053\n",
      " 0.40848023 0.40770128 0.44928074 0.41225937 0.43577784 0.4360319\n",
      " 0.4380246  0.4377798  0.4363639  0.4376276  0.43642804 0.44083467\n",
      " 0.43724567 0.43834394]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.05807693 0.05280744 0.06147812 0.05529998 0.06801935 0.04846077\n",
      " 0.05936319 0.06029079 0.06117062 0.07226072 0.07567799 0.06824305\n",
      " 0.05954258 0.06925828 0.06195719 0.05829631 0.07199763 0.05230064\n",
      " 0.06960195 0.05183359 0.06582023 0.06449044 0.06264    0.05033825\n",
      " 0.05654097 0.06765191 0.05967227 0.07521427 0.0681818  0.06177848\n",
      " 0.06136315 0.06569938 0.0680245  0.06069814 0.05999943 0.0676439\n",
      " 0.06940669 0.0555332  0.06100534 0.06872156 0.06455155 0.06381613\n",
      " 0.05156891 0.06514986 0.083223   0.06301598 0.05457936 0.05664285\n",
      " 0.06530537 0.05311475 0.05020105 0.06405196 0.05308944 0.05700765\n",
      " 0.05435326 0.06293163 0.05838507 0.06806681 0.07626391 0.05753995\n",
      " 0.06142468 0.06937841 0.06553297 0.08558729 0.0602138  0.06515563\n",
      " 0.06554122 0.06047657 0.05915526 0.0541853  0.05900107 0.08872031\n",
      " 0.09627366 0.06679135 0.04791131 0.06447692 0.06135815 0.06531392\n",
      " 0.06971167 0.06662358 0.07692748 0.06266577 0.06886439 0.06226383\n",
      " 0.06098629 0.07281063 0.05939515 0.07746901 0.06411578 0.05209918\n",
      " 0.06529395 0.05657179 0.06546242 0.09098328 0.06531622 0.06573155\n",
      " 0.06057206 0.06093248 0.06789751 0.0663438  0.07029772 0.07041804\n",
      " 0.07136171 0.07124577 0.07057524 0.07117367 0.07060563 0.07269243\n",
      " 0.07099284 0.0715129 ]\n",
      "FPR: 1.0\n",
      "-------------------------------------------------\n",
      "iteration #2:\n",
      "Anomaly index: 0\n",
      "etajTX: [[1.53383198]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.5877305  0.44109666 0.4159208  0.35868815 0.34655133 0.40487114\n",
      " 0.35536203 0.3096095  0.34301358 0.30988526 0.54330176 0.4673894\n",
      " 0.38894132 0.49906415 0.37249064 0.3369767  0.42301804 0.38551167\n",
      " 0.47354764 0.41811508 0.54683185 0.41044253 0.32008693 0.3672805\n",
      " 0.38035747 0.40508655 0.31853434 0.34831578 0.31175154 0.40229955\n",
      " 0.31313896 0.37725654 0.30998147 0.40144274 0.41492057 0.35372192\n",
      " 0.33050168 0.37113562 0.46529394 0.42097434 0.5291457  0.36014926\n",
      " 0.31780538 0.36637235 0.3125109  0.36077031 0.4073025  0.44102496\n",
      " 0.3926409  0.5015532  0.31338528 0.30986458 0.30992848 0.5053631\n",
      " 0.46484005 0.40157276 0.4231388  0.31923673 0.36719    0.4667664\n",
      " 0.36221325 0.531178   0.39022252 0.35738212 0.32096782 0.35470736\n",
      " 0.42779094 0.3928402  0.31172824 0.47607225 0.3673829  0.50137854\n",
      " 0.41465306 0.35285425 0.3411615  0.44915348 0.46147197 0.51012546\n",
      " 0.36244524 0.47744063 0.5272118  0.3148747  0.3804416  0.37485176\n",
      " 0.3434913  0.30966255 0.40193197 0.3107088  0.47271246 0.33450615\n",
      " 0.37580723 0.32399434 0.3808891  0.37265858 0.3847894  0.36289388\n",
      " 0.5002052  0.3596821  0.39600214 0.37819958 0.4049446  0.40105754\n",
      " 0.39562324 0.3990316  0.39547002 0.3915549  0.39840847 0.39825642\n",
      " 0.39879063 0.3973369 ]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.48901004 0.44523036 0.4358094  0.41100758 0.40840095 0.430796\n",
      " 0.41028234 0.4056752  0.40787354 0.4074471  0.4757452  0.45308045\n",
      " 0.4235684  0.46253738 0.41588873 0.40697354 0.43902954 0.42196634\n",
      " 0.45491907 0.436805   0.47679913 0.43332386 0.40718314 0.41385233\n",
      " 0.41939563 0.43089375 0.4085851  0.408746   0.40145683 0.42962924\n",
      " 0.4036788  0.41784897 0.40494266 0.42924047 0.4353556  0.40992475\n",
      " 0.4069137  0.4153591  0.4524548  0.4381023  0.4715187  0.41132614\n",
      " 0.40848115 0.41349736 0.40280202 0.41146156 0.43189916 0.44520897\n",
      " 0.42524695 0.46328056 0.40735653 0.40517285 0.405047   0.46441805\n",
      " 0.45231932 0.42929947 0.43908435 0.40868527 0.41381696 0.45289445\n",
      " 0.41187176 0.47212544 0.4241497  0.4107228  0.40893212 0.41013962\n",
      " 0.4411951  0.42533737 0.40173134 0.45567283 0.41389236 0.4632284\n",
      " 0.43523422 0.40973556 0.40759742 0.44763583 0.45131373 0.46583992\n",
      " 0.41196245 0.4560814  0.4709413  0.4079646  0.4194376  0.4168116\n",
      " 0.40794474 0.40711445 0.42946246 0.4035103  0.4546697  0.4108626\n",
      " 0.417185   0.4093637  0.41966075 0.41595435 0.4216061  0.4121378\n",
      " 0.46287808 0.41122428 0.426772   0.41831934 0.44114345 0.43917018\n",
      " 0.43641144 0.43814173 0.43633366 0.43434614 0.43782538 0.43774816\n",
      " 0.4380194  0.43728137]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.10287561 0.06433155 0.06263092 0.06450975 0.05900453 0.06415302\n",
      " 0.06300933 0.05540393 0.05722269 0.04765455 0.09119707 0.07124284\n",
      " 0.06634737 0.07956885 0.06930945 0.05418214 0.06165329 0.06706655\n",
      " 0.07286158 0.06232866 0.092125   0.06338557 0.04997756 0.06764545\n",
      " 0.06902303 0.06412335 0.06047314 0.05983072 0.05937256 0.06450728\n",
      " 0.06063604 0.07020011 0.05609309 0.0646253  0.06276872 0.06226946\n",
      " 0.0523491  0.06887668 0.07069203 0.0619348  0.08747603 0.06516886\n",
      " 0.06057117 0.0673554  0.06084147 0.06544903 0.06381811 0.06431269\n",
      " 0.06583775 0.0802231  0.04845154 0.05587652 0.05599492 0.08122457\n",
      " 0.07057271 0.06460738 0.06163664 0.06037869 0.06761654 0.07107907\n",
      " 0.06602707 0.08801027 0.06617089 0.06392059 0.06014587 0.062714\n",
      " 0.06099581 0.06581029 0.06110791 0.07352521 0.06767815 0.0801772\n",
      " 0.06280556 0.06187806 0.05628986 0.06644936 0.06968738 0.08247639\n",
      " 0.06610117 0.07388489 0.0869677  0.06095188 0.06899109 0.07006355\n",
      " 0.05746329 0.04881525 0.0645579  0.05744064 0.07264204 0.05832519\n",
      " 0.07036869 0.05973886 0.06882123 0.06936309 0.0673407  0.06624445\n",
      " 0.07986877 0.06495813 0.06537473 0.06984213 0.07283865 0.07190418\n",
      " 0.07059778 0.07141714 0.07056092 0.06961974 0.07126734 0.07123079\n",
      " 0.07135922 0.07100973]\n",
      "FPR: 1.0\n",
      "-------------------------------------------------\n",
      "iteration #3:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "max_iteration = 1\n",
    "alpha = 0.05\n",
    "list_p_value = []\n",
    "count = 0\n",
    "print(f'iteration #{0}:')\n",
    "\n",
    "\n",
    "while len(list_p_value) <= max_iteration:\n",
    "    p_value = run_fpr()\n",
    "    if p_value is None:\n",
    "        continue\n",
    "    list_p_value.append(p_value)\n",
    "    if p_value <= alpha:\n",
    "        count += 1\n",
    "    print(f'FPR: {count / len(list_p_value)}')\n",
    "    print('-------------------------------------------------')\n",
    "    print(f'iteration #{len(list_p_value)+1}:')\n",
    "plt.hist(list_p_value)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
