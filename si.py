import numpy as np
from typing import List
import matplotlib.pyplot as plt
from models.train import WDGRL, run, get_next_index
from mpmath import mp
mp.dps = 500
import torch
import os
import scipy.stats as stats
from multiprocessing import Pool

def gen_data(n: int, d: int, mu: float, delta: List[int]):
    mu = np.full((n, d), mu, dtype=np.float64)
    noise = np.random.normal(loc = 0, scale = 1, size=(n, d))
    X = mu + noise
    labels = np.zeros(n)
    if len(delta) == 1:
        # In this case, we generate data for target domain.
        # For simplicity in experiment with varying deltas, 
        # we assume that delta is the same for all anomalies.
        X = X + delta[0]
        if delta[0] != 0:
            labels = np.ones(n)
    else:
        # In this case, we generate data for source domain.
        # 5% of the data is abnormal.
        # Anomalies are generated by randomly adding deltas to the data.
        n_anomalies = int(n * 0.05)
        idx = np.random.choice(n, n_anomalies, replace=False)
        if 0 in delta: 
            delta.pop(delta.index(0))
        if len(delta) != 0:
            split_points = sorted(np.random.choice(range(1, len(idx)), len(delta) - 1, replace=False))
            segments = np.split(idx, split_points)
            for i, segment in enumerate(segments):
                X[segment] = X[segment] + delta[i]
            labels[idx] = 1
    return X, labels

def model_parameters_match(model, checkpoint): 
    try:
        # Extract model and checkpoint state dictionaries
        model_gen_state = model.generator.state_dict()
        model_crit_state = model.critic.state_dict()
        checkpoint_gen_state = checkpoint['generator_state_dict']
        checkpoint_crit_state = checkpoint['critic_state_dict']

        # Check if layer names match
        generator_match = model_gen_state.keys() == checkpoint_gen_state.keys()
        critic_match = model_crit_state.keys() == checkpoint_crit_state.keys()

        if not (generator_match and critic_match):
            print("Model layer names do not match.")
            return False

        # Check if layer shapes (node counts) match
        for key in model_gen_state:
            if model_gen_state[key].shape != checkpoint_gen_state[key].shape:
                print(f"Mismatch in generator layer {key}: {model_gen_state[key].shape} vs {checkpoint_gen_state[key].shape}")
                return False

        for key in model_crit_state:
            if model_crit_state[key].shape != checkpoint_crit_state[key].shape:
                print(f"Mismatch in critic layer {key}: {model_crit_state[key].shape} vs {checkpoint_crit_state[key].shape}")
                return False

        return True  # Both names and dimensions match

    except KeyError:
        print("KeyError: Model and checkpoint structure do not match.")
        return False

def basic_anomaly_detection(X):
    O = np.argmax(np.sum(X, axis=1))
    Y_hat = np.zeros(X.shape[0])
    Y_hat[O] = 1
    return O, Y_hat

def intersect(itv1, itv2):
    # print(itv1, itv2)
    itv = [max(itv1[0], itv2[0]), min(itv1[1], itv2[1])]
    if itv[0] > itv[1]:
        return None    
    return itv

def solve_linear_inequality(u, v): #u + vz < 0
    if v == 0:
        if (u < 0):
            return [-mp.inf, mp.inf]
        else:
            print('error')
            return None
    if (v < 0):
        return [-u/v, mp.inf]
    return [-mp.inf, -u/v]

def get_dnn_interval(model, Xtj, a, b):
    layers = []

    for name, param in model.generator.named_children():
        temp = dict(param._modules)
        
        for layer_name in temp.values():
            if ('Linear' in str(layer_name)):
                layers.append('Linear')
            elif ('ReLU' in str(layer_name)):
                layers.append('ReLU')

    ptr = 0
    itv = [-mp.inf, mp.inf]
    u = a
    v = b
    temp = Xtj
    weight = None
    bias = None
    for name, param in model.generator.named_parameters():
        if (layers[ptr] == 'Linear'):
            if ('weight' in name):
                weight = param.data.cpu().detach().numpy()
            elif ('bias' in name):
                bias = param.data.cpu().detach().numpy().reshape(-1, 1)
                ptr += 1
                temp = weight.dot(temp) + bias
                u = weight.dot(u) + bias
                v = weight.dot(v)

        if (ptr < len(layers) and layers[ptr] == 'ReLU'):
            ptr += 1
            Relu_matrix = np.zeros((temp.shape[0], temp.shape[0]))
            sub_itv = [-mp.inf, mp.inf]
            for i in range(temp.shape[0]):
                if temp[i] > 0:
                    Relu_matrix[i][i] = 1
                    sub_itv = intersect(sub_itv, solve_linear_inequality(mp.mpf(-u[i][0]), mp.mpf(-v[i][0])))
                else:
                    sub_itv = intersect(sub_itv, solve_linear_inequality(mp.mpf(u[i][0]), mp.mpf(v[i][0])))
            itv = intersect(itv, sub_itv)
            temp = Relu_matrix.dot(temp)
            u = Relu_matrix.dot(u)
            v = Relu_matrix.dot(v)

    return itv, u, v

def tn_cdf(tt, mu, sigma, list_intervals):
    if len(list_intervals) == 0:
        print('Error no interval')
        return None

    list_tn_cdf = []
    for interval in list_intervals:
        temp = mp.ncdf((interval[1] - mu) / sigma) - mp.ncdf((interval[0] - mu) / sigma)
        list_tn_cdf.append(temp)

    numerator = 0
    for i in range(len(list_intervals)):
        interval = list_intervals[i]

        if tt > interval[1]:
            numerator += list_tn_cdf[i]
        else:
            numerator += mp.ncdf((tt - mu) / sigma) - mp.ncdf((interval[0] - mu) / sigma)
            break

    denominator = sum(list_tn_cdf)

    if denominator == 0.0:
        print('Numerical error')
        return None

    cdf = float(numerator / denominator)
    return cdf

def run_oc(model):
    ns, nt = 100, 100
    d = 1
    mu_s, mu_t = 0, 2
    delta_s = [0, 1, 2, 3, 4]
    delta_t = [0]
    Xs, _ = gen_data(ns, d, mu_s, delta_s)
    Xt, __ = gen_data(nt, d, mu_t, delta_t)

    Xs_hat = model.generator(torch.tensor(Xs, dtype=torch.float32)).detach().numpy()
    Xt_hat = model.generator(torch.tensor(Xt, dtype=torch.float32)).detach().numpy()
    X_hat = np.vstack((Xs_hat, Xt_hat))
    O, Y_hat = basic_anomaly_detection(X_hat)
    if O < ns:
        # print("Anomaly detected in source domain.")
        return None
    O = [O-ns]
    Oc = list(np.where(Y_hat[ns:ns+nt] == 0)[0])
    j = np.random.choice(O, 1, replace=False)[0]
    etj = np.zeros((nt, 1))
    etj[j][0] = 1
    etOc = np.zeros((nt, 1))
    etOc[Oc] = 1
    etaj = np.vstack((np.zeros((ns, 1)), etj-(1/len(Oc))*etOc))
    X = np.vstack((Xs, Xt))

    etajTX = etaj.T.dot(X)
    mu = np.vstack((np.full((ns,1), mu_s), np.full((nt,1), mu_t)))
    sigma = np.identity(ns+nt)
    etajTmu = etaj.T.dot(mu)
    etajTsigmaetaj = etaj.T.dot(sigma).dot(etaj)
    
    b = sigma.dot(etaj).dot(np.linalg.inv(etajTsigmaetaj))
    a = (np.identity(ns+nt) - b.dot(etaj.T)).dot(X)

    itv1 = [-mp.inf, mp.inf]
    for i in range(X.shape[0]):
        itv1 = intersect(itv1, get_dnn_interval(model, X[i].reshape(-1, 1), a[i].reshape(-1, 1), b[i].reshape(-1, 1))[0])
    itv2 = [-mp.inf, mp.inf]
    _, uo, vo = get_dnn_interval(model, X[O[0]+ns].reshape(-1, 1), a[O[0]+ns].reshape(-1, 1), b[O[0]+ns].reshape(-1, 1))
    I = np.ones((X_hat.shape[1],1))
    for i in range(X.shape[0]):
        if (i != O[0]+ns):
            _, ui, vi = get_dnn_interval(model, X[i].reshape(-1, 1), a[i].reshape(-1, 1), b[i].reshape(-1, 1))
            u = uo - ui
            v = vo - vi 
            u = I.T.dot(u)[0][0]
            v = I.T.dot(v)[0][0]
            sub_itv = solve_linear_inequality(mp.mpf(-u), mp.mpf(-v))
            itv2 = intersect(itv2, sub_itv)
    itv = intersect(itv1, itv2)
    # print("Interval 1:\t", itv1)
    # print("Interval 2:\t", itv2)
    # print("Interval:\t", itv)
    # print("----------------------------------------------")    
    cdf = tn_cdf(etajTX[0][0], etajTmu[0][0], np.sqrt(etajTsigmaetaj[0][0]), [itv])
    p_value = float(2 * min(cdf, 1 - cdf))
    return p_value
    
def divide_conquer(model, ns, nt, a, b, zmin, zmax):
    list_intervals = []
    list_O = []
    z = float(zmin)
    while z <= zmax:
        print(z)
        Xz = a+b*z
        Xz_hat = model.generator(torch.tensor(Xz, dtype=torch.float32)).detach().numpy()
        Oz, _ = basic_anomaly_detection(Xz_hat)
        if Oz < ns:
            z += 1e-3
            print('---------------------------')
            continue
        Oz = [Oz-ns]
        itv1 = [-mp.inf, mp.inf]
        for i in range(Xz.shape[0]):
            itv1 = intersect(itv1, get_dnn_interval(model, Xz[i].reshape(-1, 1), a[i].reshape(-1, 1), b[i].reshape(-1, 1))[0])
        itv2 = [-mp.inf, mp.inf]
        _, uo, vo = get_dnn_interval(model, Xz[Oz[0]+ns].reshape(-1, 1), a[Oz[0]+ns].reshape(-1, 1), b[Oz[0]+ns].reshape(-1, 1))
        I = np.ones((Xz_hat.shape[1],1))
        for i in range(Xz.shape[0]):
            if (i != Oz[0]+ns):
                _, ui, vi = get_dnn_interval(model, Xz[i].reshape(-1, 1), a[i].reshape(-1, 1), b[i].reshape(-1, 1))
                u = uo - ui
                v = vo - vi 
                u = I.T.dot(u)[0][0]
                v = I.T.dot(v)[0][0]
                sub_itv = solve_linear_inequality(mp.mpf(-u), mp.mpf(-v))
                itv2 = intersect(itv2, sub_itv)
        itv = intersect(itv1, itv2)
        list_intervals.append(itv)
        list_O.append(Oz)
        start = float(itv[0])
        end = float(itv[1])
        print(f'[{start}, {end}]\t\t{Oz}\n')
        print('---------------------------')
        z = float(itv[1] + 1e-3)
    return list_intervals, list_O

def run_pp(model):
    ns, nt = 100, 100
    d = 1
    mu_s, mu_t = 0, 2
    delta_s = [0, 1, 2, 3, 4]
    delta_t = [0]
    Xs, _ = gen_data(ns, d, mu_s, delta_s)
    Xt, __ = gen_data(nt, d, mu_t, delta_t)

    Xs_hat = model.generator(torch.tensor(Xs, dtype=torch.float32)).detach().numpy()
    Xt_hat = model.generator(torch.tensor(Xt, dtype=torch.float32)).detach().numpy()
    X_hat = np.vstack((Xs_hat, Xt_hat))
    O, Y_hat = basic_anomaly_detection(X_hat)
    if O < ns:
        print("Anomaly detected in source domain.")
        return None
    O = [O-ns]
    Oc = list(np.where(Y_hat[ns:ns+nt] == 0)[0])
    j = np.random.choice(O, 1, replace=False)[0]
    etj = np.zeros((nt, 1))
    etj[j][0] = 1
    etOc = np.zeros((nt, 1))
    etOc[Oc] = 1
    etaj = np.vstack((np.zeros((ns, 1)), etj-(1/len(Oc))*etOc))
    X = np.vstack((Xs, Xt))

    etajTX = etaj.T.dot(X)
    mu = np.vstack((np.full((ns,1), mu_s), np.full((nt,1), mu_t)))
    sigma = np.identity(ns+nt)
    etajTmu = etaj.T.dot(mu)
    etajTsigmaetaj = etaj.T.dot(sigma).dot(etaj)
    
    b = sigma.dot(etaj).dot(np.linalg.inv(etajTsigmaetaj))
    a = (np.identity(ns+nt) - b.dot(etaj.T)).dot(X)
    
    zmin, zmax = -20, 20
    list_intervals, list_O = divide_conquer(model, ns, nt, a, b, zmin, zmax)
    
    with open("itv_O.txt", "a") as file:
        file.write(f"{etajTX[0][0]}\t\t{O}\n")
        for i in range(len(list_intervals)):
            start = float(list_intervals[i][0])
            end = float(list_intervals[i][1])
            file.write(f"[{start}, {end}]\t\t{list_O[i]}\n")

    Z = []
    for i in range(len(list_intervals)):
        if np.array_equal(list_O[i], O):
            Z.append(list_intervals[i])

    cdf = tn_cdf(etajTX[0][0], etajTmu[0][0], np.sqrt(etajTsigmaetaj[0][0]), Z)
    pivot = float(2 * min(cdf, 1 - cdf))
    return pivot

if __name__ == '__main__':
    os.environ["MKL_NUM_THREADS"] = "1"
    os.environ["NUMEXPR_NUM_THREADS"] = "1"
    os.environ["OMP_NUM_THREADS"] = "1"

    d = 1
    generator_hidden_dims = [10, 10, 10]
    critic_hidden_dims = [10]
    print("Loading model...")
    model = WDGRL(d, generator_hidden_dims=generator_hidden_dims, critic_hidden_dims=critic_hidden_dims)
    MODEL_DIR = "models"
    INDEX_FILE = os.path.join(MODEL_DIR, "index.txt")
    index = get_next_index(INDEX_FILE) 
    # Try to load a matching model
    matching_model_found = False
    for i in range(index):
        try:
            model_name = f'wdgrl_{i}'
            # Load the saved checkpoint
            checkpoint = torch.load(f"models/{model_name}.pth", map_location=model.device, weights_only=True)
            if model_parameters_match(model, checkpoint):
                # Restore the model weights
                model.generator.load_state_dict(checkpoint['generator_state_dict'])
                model.critic.load_state_dict(checkpoint['critic_state_dict'])
                print(f"Model loaded successfully from models/{model_name}.pth!")
                matching_model_found = True
                break
        except (FileNotFoundError, ValueError):
           continue
    # If no matching model was found, train a new one
    if not matching_model_found:
        print("No matching model found in models folder. Training a new model...")
        run(d, generator_hidden_dims=generator_hidden_dims, critic_hidden_dims=critic_hidden_dims)
        index = get_next_index(INDEX_FILE) - 1
        model_name = f'wdgrl_{index}'
        # Load the saved checkpoint
        checkpoint = torch.load(f"models/{model_name}.pth", map_location=model.device, weights_only=True)
        print(f"Model loaded successfully from models/{model_name}.pth!")

    # max_iteration = 2000
    # alpha = 0.05
    # list_p_value = []
    # count = 0
    # detect = 0

    # list_model = [model] * max_iteration
    # pool = Pool(initializer=np.random.seed)
    # list_result = pool.map(run_pp, list_model)

    # for p_value in list_result:
    #     if p_value is None:
    #         continue
    #     detect += 1
    #     list_p_value.append(p_value)
    #     if p_value <= alpha:
    #         count += 1

    # plt.hist(list_p_value)
    # plt.show()
    # plt.close()

    # print(f'FPR: {count / detect}')
    # print(f"ks-test result: {stats.kstest(list_p_value, 'uniform')[1]}")

    # print(f'TPR: {count / detect}') 

    run_pp(model)