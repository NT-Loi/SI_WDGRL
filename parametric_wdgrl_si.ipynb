{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def gen_data(mu, delta, n, d: int = 2):\n",
    "    noise = np.random.normal(loc = 0, scale = 1, size=(n, d))\n",
    "    mu = np.full((n, d), mu, dtype=np.float64)\n",
    "\n",
    "    if len(delta) == 1 and delta[0] == 0:\n",
    "        return mu + noise, np.zeros(n)\n",
    "    \n",
    "    # 10% of the data are abnormal\n",
    "    m = len(delta)\n",
    "    abnormal_idx = np.random.choice(n, int(n/10), replace=False)\n",
    "\n",
    "    ptr = 0\n",
    "    for i in range(m):\n",
    "        for j in range(len(abnormal_idx)//m):\n",
    "            mu[abnormal_idx[ptr], :] += delta[i]\n",
    "            ptr += 1\n",
    "    \n",
    "    X = mu + noise \n",
    "    Y = np.zeros(n)\n",
    "    Y[abnormal_idx] = 1\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"Feature extractor network.\"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"Domain critic network.\"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class WDGRL():\n",
    "    def __init__(self, input_dim: int=2, generator_hidden_dims: List[int]=[32, 16, 8, 4, 2], critic_hidden_dims: List[int]=[32, 16, 8, 4, 2],\n",
    "                 gamma: float = 0.1, _lr_generator: float = 1e-2, _lr_critic: float = 1e-2, \n",
    "                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.generator = Generator(input_dim, generator_hidden_dims).to(self.device)\n",
    "        self.critic = Critic(generator_hidden_dims[-1], critic_hidden_dims).to(self.device)\n",
    "        self.generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=_lr_generator)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=_lr_critic)\n",
    "    \n",
    "    def compute_gradient_penalty(self, source_data: torch.Tensor, target_data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute gradient penalty.\"\"\"\n",
    "        if source_data.size(0) > target_data.size(0):\n",
    "            ms = source_data.size(0)\n",
    "            mt = target_data.size(0)\n",
    "            gradient_penalty = 0\n",
    "            for _ in range(0, ms, mt):\n",
    "                source_chunk = source_data[_:_+mt]\n",
    "                target_chunk = target_data\n",
    "                alpha = torch.rand(target_chunk.size(0), 1).to(self.device)\n",
    "                interpolates = (alpha * source_chunk + ((1 - alpha) * target_chunk)).requires_grad_(True)\n",
    "                \n",
    "                # Domain critic outputs\n",
    "                dc_output = self.critic(interpolates)\n",
    "                \n",
    "                # Compute gradients\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=dc_output,\n",
    "                    inputs=interpolates,\n",
    "                    grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True,\n",
    "                    only_inputs=True,\n",
    "                )\n",
    "                gradients = gradients[0]\n",
    "                gradient_penalty += ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "            if ms % mt != 0:\n",
    "                source_chunk = source_data[ms-mt:]\n",
    "                perm = torch.randperm(mt)\n",
    "                idx = perm[:ms % mt]\n",
    "                target_chunk = target_data[idx]\n",
    "                alpha = torch.rand(target_chunk.size(0), 1).to(self.device)\n",
    "                interpolates = (alpha * source_chunk + ((1 - alpha) * target_chunk)).requires_grad_(True)\n",
    "                \n",
    "                # Domain critic outputs\n",
    "                dc_output = self.critic(interpolates)\n",
    "                \n",
    "                # Compute gradients\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=dc_output,\n",
    "                    inputs=interpolates,\n",
    "                    grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True,\n",
    "                    only_inputs=True,\n",
    "                )\n",
    "                gradients = gradients[0]\n",
    "                gradient_penalty += ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "            return gradient_penalty / ((ms // mt) + (ms % mt != 0)) \n",
    "        \n",
    "        # For balanced batch\n",
    "        alpha = torch.rand(source_data.size(0), 1).to(self.device)\n",
    "        interpolates = (alpha * source_data + ((1 - alpha) * target_data)).requires_grad_(True)\n",
    "        \n",
    "        # Domain critic outputs\n",
    "        dc_output = self.critic(interpolates)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=dc_output,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "\n",
    "        # Compute gradient penalty\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "    def train(self, source_loader: DataLoader, target_loader: DataLoader, num_epochs: int = 100, dc_iter: int = 100) -> List[float]:\n",
    "        self.generator.train()\n",
    "        self.critic.train()\n",
    "        losses = []\n",
    "        source_critic_scores = []\n",
    "        target_critic_scores = []\n",
    "        for epoch in trange(num_epochs, desc='Epoch'):\n",
    "            loss = 0\n",
    "            for (source_data, _), (target_data, _) in zip(source_loader, target_loader):\n",
    "                source_data, target_data = source_data.to(self.device), target_data.to(self.device)\n",
    "\n",
    "                # Train domain critic\n",
    "                for _ in range(dc_iter):\n",
    "                    self.critic_optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        source_features = self.generator(source_data)\n",
    "                        target_features = self.generator(target_data)\n",
    "                    \n",
    "                    # Compute empirical Wasserstein distance\n",
    "                    dc_source = self.critic(source_features)\n",
    "                    dc_target = self.critic(target_features)\n",
    "                    wasserstein_distance = dc_source.mean() - dc_target.mean()\n",
    "\n",
    "                    # Gradient penalty\n",
    "                    gradient_penalty = self.compute_gradient_penalty(source_features, target_features)\n",
    "\n",
    "                    # Domain critic loss\n",
    "                    dc_loss = - wasserstein_distance + self.gamma * gradient_penalty\n",
    "                    dc_loss.backward()\n",
    "                    self.critic_optimizer.step()\n",
    "\n",
    "                # Train feature extractor\n",
    "                self.generator_optimizer.zero_grad()\n",
    "                source_features = self.generator(source_data)\n",
    "                target_features = self.generator(target_data)\n",
    "                dc_source = self.critic(source_features)\n",
    "                dc_target = self.critic(target_features)\n",
    "                wasserstein_distance = dc_source.mean() - dc_target.mean()\n",
    "                wasserstein_distance.backward()\n",
    "                self.generator_optimizer.step()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    loss += wasserstein_distance.item()\n",
    "                    \n",
    "            source_critic_scores.append(self.criticize(source_loader.dataset.tensors[0].to(self.device)))\n",
    "            target_critic_scores.append(self.criticize(target_loader.dataset.tensors[0].to(self.device)))\n",
    "            losses.append(loss/len(source_loader))\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} | Loss: {wasserstein_distance.item()}')\n",
    "        return losses, source_critic_scores, target_critic_scores\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract_feature(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.generator.eval()\n",
    "        return self.generator(x)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def criticize(self, x: torch.Tensor) -> float:\n",
    "        self.generator.eval()\n",
    "        self.critic.eval()\n",
    "        return self.critic(self.generator(x)).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of the WDGRL model (same architecture as before)\n",
    "model = WDGRL(input_dim=1,generator_hidden_dims=[10, 10, 10, 10], critic_hidden_dims=[10])\n",
    "\n",
    "# Load the saved checkpoint\n",
    "checkpoint = torch.load(\"wdgrl.pth\", map_location=model.device, weights_only=True)\n",
    "\n",
    "# Restore the model weights\n",
    "model.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "model.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Create synthetic dataset and dataloaders for domain adaptation.\"\"\"\n",
    "# Create datasets\n",
    "ns, nt, d = 100, 10, 1\n",
    "mu_s, mu_t = 0, 20\n",
    "delta_s, delta_t = [1, 2, 3, 4], [4]\n",
    "xs, ys = gen_data(mu_s, delta_s, ns, d)\n",
    "xt, yt = gen_data(mu_t, delta_t, nt, d)\n",
    "\n",
    "plt.scatter(xs[:, 0], np.zeros_like(xs[:, 0]), c=ys, cmap='viridis', s=2)\n",
    "plt.scatter(xt[:, 0], np.zeros_like(xt[:, 0]), c=yt, cmap='cool', s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2146,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.FloatTensor(xs)\n",
    "ys = torch.LongTensor(ys)\n",
    "xt = torch.FloatTensor(xt)\n",
    "yt = torch.LongTensor(yt)\n",
    "xs_hat = model.extract_feature(xs.cuda())\n",
    "xt_hat = model.extract_feature(xt.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 2147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_sum(X):\n",
    "    return np.argmax(np.sum(X, axis=1))\n",
    "x_hat = torch.cat([xs_hat, xt_hat], dim=0).cpu().numpy()\n",
    "print(x_hat)\n",
    "O = max_sum(x_hat)\n",
    "O = [O-ns]\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 2148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_hat = torch.zeros_like(yt)\n",
    "yt_hat[O[0]] = 1\n",
    "print(yt)\n",
    "yt_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score\n",
    "accuracy = accuracy_score(yt, yt_hat)\n",
    "precision = precision_score(yt, yt_hat)\n",
    "recall = recall_score(yt, yt_hat)\n",
    "f1 = f1_score(yt, yt_hat)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(yt, yt_hat))\n",
    "\n",
    "# Print the scores\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_matrix = confusion_matrix(yt, yt_hat)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "disp.plot(cmap=plt.cm.Blues)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpmath import mp\n",
    "\n",
    "mp.dps = 500\n",
    "def intersect(itv1, itv2):\n",
    "    # print(itv1, itv2)\n",
    "    itv = [max(itv1[0], itv2[0]), min(itv1[1], itv2[1])]\n",
    "    if itv[0] > itv[1]:\n",
    "        return None    \n",
    "    return itv\n",
    "\n",
    "def solve_linear_inequality(u, v): #u + vz < 0\n",
    "    u = float(u)\n",
    "    v = float(v)\n",
    "    if (v > -1e-16 and v < 1e-16):\n",
    "        if (u < 0):\n",
    "            return [-np.Inf, np.Inf]\n",
    "        else:\n",
    "            print('error')\n",
    "            return None\n",
    "    if (v < 0):\n",
    "        return [-u/v, np.Inf]\n",
    "    return [np.NINF, -u/v]\n",
    "\n",
    "def get_dnn_interval(Xtj, a, b):\n",
    "    layers = []\n",
    "\n",
    "    for name, param in model.generator.named_children():\n",
    "        temp = dict(param._modules)\n",
    "        \n",
    "        for layer_name in temp.values():\n",
    "            if ('Linear' in str(layer_name)):\n",
    "                layers.append('Linear')\n",
    "            elif ('ReLU' in str(layer_name)):\n",
    "                layers.append('ReLU')\n",
    "\n",
    "    ptr = 0\n",
    "    itv = [np.NINF, np.Inf]\n",
    "    u = a\n",
    "    v = b\n",
    "    temp = Xtj\n",
    "    weight = None\n",
    "    bias = None\n",
    "    for name, param in model.generator.named_parameters():\n",
    "        if (layers[ptr] == 'Linear'):\n",
    "            if ('weight' in name):\n",
    "                weight = param.data.cpu().detach().numpy()\n",
    "            elif ('bias' in name):\n",
    "                bias = param.data.cpu().detach().numpy().reshape(-1, 1)\n",
    "                ptr += 1\n",
    "                temp = weight.dot(temp) + bias\n",
    "                u = weight.dot(u) + bias\n",
    "                v = weight.dot(v)\n",
    "\n",
    "        if (ptr < len(layers) and layers[ptr] == 'ReLU'):\n",
    "            ptr += 1\n",
    "            Relu_matrix = np.zeros((temp.shape[0], temp.shape[0]))\n",
    "            sub_itv = [np.NINF, np.inf]\n",
    "            for i in range(temp.shape[0]):\n",
    "                if temp[i] > 0:\n",
    "                    Relu_matrix[i][i] = 1\n",
    "                    sub_itv = intersect(sub_itv, solve_linear_inequality(-u[i], -v[i]))\n",
    "                else:\n",
    "                    sub_itv = intersect(sub_itv, solve_linear_inequality(u[i], v[i]))\n",
    "            itv = intersect(itv, sub_itv)\n",
    "            temp = Relu_matrix.dot(temp)\n",
    "            u = Relu_matrix.dot(u)\n",
    "            v = Relu_matrix.dot(v)\n",
    "\n",
    "    return itv, u, v\n",
    "\n",
    "def get_ad_interval(X, X_hat, O, a, b):\n",
    "    itv = [np.NINF, np.Inf]\n",
    "    for i in range(X.shape[0]):\n",
    "        itv = intersect(itv, get_dnn_interval(X[i].reshape(-1, 1), a[i].reshape(-1, 1), b[i].reshape(-1, 1))[0])\n",
    "    _, uo, vo = get_dnn_interval(X[O].reshape(-1, 1), a[O].reshape(-1, 1), b[O].reshape(-1, 1))\n",
    "    I = np.ones((X_hat.shape[1],1))\n",
    "    sub_itv = [np.NINF, np.inf]\n",
    "    for i in range(X.shape[0]):\n",
    "        if (i != O):\n",
    "            _, ui, vi = get_dnn_interval(X[i].reshape(-1, 1), a[i].reshape(-1, 1), b[i].reshape(-1, 1))\n",
    "            u = uo - ui\n",
    "            v = vo - vi \n",
    "            u = I.T.dot(u)\n",
    "            v = I.T.dot(v)\n",
    "            sub_itv = intersect(sub_itv, solve_linear_inequality(-u, -v))\n",
    "    itv = intersect(itv, sub_itv)\n",
    "    return itv\n",
    "\n",
    "def compute_yz(X, etaj, zk, n):\n",
    "    sq_norm = (np.linalg.norm(etaj))**2\n",
    "\n",
    "    e1 = np.identity(n) - (np.dot(etaj, etaj.T))/sq_norm\n",
    "    a = np.dot(e1, X)\n",
    "\n",
    "    b = etaj/sq_norm\n",
    "\n",
    "    Xz = a + b*zk\n",
    "\n",
    "    return Xz, a, b\n",
    "\n",
    "def parametric_wdgrl(Xz, a, b, n, zk):\n",
    "    Xz = torch.FloatTensor(Xz)\n",
    "    Xz_hat = model.extract_feature(Xz.cuda())\n",
    "    Oz = [max_sum(Xz_hat.cpu().numpy())]\n",
    "    itv = get_ad_interval(Xz, Xz_hat, Oz[0], a, b)\n",
    "    return itv[1] - min(zk, itv[1]), Oz\n",
    "\n",
    "\n",
    "def run_parametric_wdgrl(X, etaj, n, threshold):\n",
    "    zk = -threshold\n",
    "\n",
    "    list_zk = [zk]\n",
    "    list_Oz = []\n",
    "\n",
    "    while zk < threshold:\n",
    "        Xz, a, b = compute_yz(X, etaj, zk, n)\n",
    "        skz, Oz = parametric_wdgrl(Xz, a, b, n, zk)\n",
    "\n",
    "        zk = zk + skz + 1e-6 \n",
    "        if zk < threshold:\n",
    "            list_zk.append(zk)\n",
    "        else:\n",
    "            list_zk.append(threshold)\n",
    "        list_Oz.append(Oz)\n",
    "        # print(f'intervals: {zk-skz-1e-6} - {zk -1e-6}')\n",
    "        # print(f'Anomaly index: {Oz}')\n",
    "    return list_zk, list_Oz\n",
    "        \n",
    "def cdf(mu, sigma, list_zk, list_Oz, etajTX, O):\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for each_interval in range(len(list_zk) - 1):\n",
    "        al = list_zk[each_interval]\n",
    "        ar = list_zk[each_interval + 1] - 1e-6\n",
    "\n",
    "        if (np.array_equal(O, list_Oz[each_interval]) == False):\n",
    "            continue\n",
    "\n",
    "        denominator = denominator + mp.ncdf((ar - mu)/sigma) - mp.ncdf((al - mu)/sigma)\n",
    "        if etajTX >= ar:\n",
    "            numerator = numerator + mp.ncdf((ar - mu)/sigma) - mp.ncdf((al - mu)/sigma)\n",
    "        elif (etajTX >= al) and (etajTX< ar):\n",
    "            numerator = numerator + mp.ncdf((etajTX - mu)/sigma) - mp.ncdf((al - mu)/sigma)\n",
    "\n",
    "    if denominator != 0:\n",
    "        return float(numerator/denominator)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def run_fpr():\n",
    "    # np.random.seed(42)\n",
    "    ns, nt, d = 100, 10, 1\n",
    "    mu_s, mu_t = 0, 20\n",
    "    delta_s, delta_t = [4], [0]\n",
    "    xs, ys = gen_data(mu_s, delta_s, ns, d)\n",
    "    xt, yt = gen_data(mu_s, delta_t, nt, d)\n",
    "\n",
    "    xs = torch.FloatTensor(xs)\n",
    "    ys = torch.LongTensor(ys)\n",
    "    xt = torch.FloatTensor(xt)\n",
    "    yt = torch.LongTensor(yt)\n",
    "\n",
    "    xs = xs.cuda()\n",
    "    xt = xt.cuda()\n",
    "    ys = ys.cuda()\n",
    "    yt = yt.cuda()\n",
    "\n",
    "    xs_hat = model.extract_feature(xs)\n",
    "    xt_hat = model.extract_feature(xt)\n",
    "    x_hat = torch.cat([xs_hat, xt_hat], dim=0)\n",
    "\n",
    "    xs_hat = xs_hat.cpu()\n",
    "    xt_hat = xt_hat.cpu()\n",
    "    x_hat = x_hat.cpu()\n",
    "    xs = xs.cpu()\n",
    "    xt = xt.cpu()\n",
    "    ys = ys.cpu()\n",
    "    yt = yt.cpu()\n",
    "\n",
    "    O = max_sum(x_hat.numpy())\n",
    "    if (O < 100):\n",
    "        return None\n",
    "    else:\n",
    "        O = [O - 100]   \n",
    "    Oc = list(torch.where(yt == 0)[0])\n",
    "    X = np.vstack((xs, xt))\n",
    "    X = torch.FloatTensor(X)\n",
    "    j = np.random.choice(O)\n",
    "    etj = np.zeros((nt, 1))\n",
    "    etj[j][0] = 1\n",
    "    etOc = np.zeros((nt, 1))\n",
    "    etOc[Oc] = 1\n",
    "    etaj = np.vstack((np.zeros((ns, 1)), etj-(1/len(Oc))*etOc))\n",
    "\n",
    "    etajTx = etaj.T.dot(X)\n",
    "    \n",
    "    print(f'Anomaly index: {O[0] + 100}')\n",
    "    print(f'etajTX: {etajTx}')\n",
    "    mu = np.vstack((np.full((ns,1), mu_s), np.full((nt,1), mu_t)))\n",
    "    sigma = np.identity(ns+nt)\n",
    "    etajTmu = etaj.T.dot(mu)\n",
    "    etajTsigmaetaj = etaj.T.dot(sigma).dot(etaj)\n",
    "    b = sigma.dot(etaj).dot(np.linalg.inv(etajTsigmaetaj))\n",
    "    a = (np.identity(ns+nt) - b.dot(etaj.T)).dot(X)\n",
    "    threshold = 20\n",
    "    list_zk, list_Oz = run_parametric_wdgrl(X, etaj, ns+nt, threshold)\n",
    "    CDF = cdf(etajTmu[0][0], etajTsigmaetaj[0][0], list_zk, list_Oz, etajTx[0][0], [O[0] + 100])\n",
    "    p_value = 2 * min(CDF, 1 - CDF)\n",
    "    print(f'p-value: {p_value}')\n",
    "    print('-------------------------------------------------')\n",
    "    return p_value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #0:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2151], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miteration #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_p_value) \u001b[38;5;241m<\u001b[39m max_iteration:\n\u001b[1;32m---> 10\u001b[0m     p_value \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fpr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2150], line 168\u001b[0m, in \u001b[0;36mrun_fpr\u001b[1;34m()\u001b[0m\n\u001b[0;32m    165\u001b[0m ys \u001b[38;5;241m=\u001b[39m ys\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    166\u001b[0m yt \u001b[38;5;241m=\u001b[39m yt\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m--> 168\u001b[0m xs_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m xt_hat \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mextract_feature(xt)\n\u001b[0;32m    170\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([xs_hat, xt_hat], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2143], line 183\u001b[0m, in \u001b[0;36mWDGRL.extract_feature\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_feature\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2143], line 21\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "max_iteration = 1000\n",
    "alpha = 0.05\n",
    "list_p_value = []\n",
    "count = 0\n",
    "print(f'iteration #{0}:')\n",
    "\n",
    "\n",
    "while len(list_p_value) < max_iteration:\n",
    "    p_value = run_fpr()\n",
    "    if p_value is None:\n",
    "        continue\n",
    "    print(f'iteration #{len(list_p_value)+1}:')\n",
    "    list_p_value.append(p_value)\n",
    "    if p_value <= alpha:\n",
    "        count += 1\n",
    "print(f'FPR: {count / len(list_p_value)}')\n",
    "print(f'KS-test p-value: {stats.kstest(list_p_value, 'uniform')[1]}')\n",
    "\n",
    "\n",
    "plt.hist(list_p_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tpr():\n",
    "    # np.random.seed(42)\n",
    "    # Create datasets\n",
    "    ns, nt, d = 100, 10, 1\n",
    "    mu_s, mu_t = 0, 2\n",
    "    delta_s, delta_t = [1, 2, 3,4], [4]\n",
    "    xs, ys = gen_data(mu_s, delta_s, ns, d)\n",
    "    xt, yt = gen_data(mu_t, delta_t, nt, d)\n",
    "\n",
    "    xs = torch.FloatTensor(xs)\n",
    "    ys = torch.LongTensor(ys)\n",
    "    xt = torch.FloatTensor(xt)\n",
    "    yt = torch.LongTensor(yt)\n",
    "\n",
    "    xs = xs.cuda()\n",
    "    xt = xt.cuda()\n",
    "    ys = ys.cuda()\n",
    "    yt = yt.cuda()\n",
    "\n",
    "    xs_hat = model.extract_feature(xs)\n",
    "    xt_hat = model.extract_feature(xt)\n",
    "    x_hat = torch.cat([xs_hat, xt_hat], dim=0)\n",
    "\n",
    "    xs_hat = xs_hat.cpu()\n",
    "    xt_hat = xt_hat.cpu()\n",
    "    x_hat = x_hat.cpu()\n",
    "    xs = xs.cpu()\n",
    "    xt = xt.cpu()\n",
    "    ys = ys.cpu()\n",
    "    yt = yt.cpu()\n",
    "\n",
    "    O = max_sum(x_hat.numpy())\n",
    "    if (O < ns):\n",
    "        return None\n",
    "    else:\n",
    "        O = [O - ns]   \n",
    "    if yt[O[0]] == 0:\n",
    "        return None\n",
    "    Oc = list(torch.where(yt == 0)[0])\n",
    "    X = np.vstack((xs, xt))\n",
    "    X = torch.FloatTensor(X)\n",
    "    j = np.random.choice(O)\n",
    "    etj = np.zeros((nt, 1))\n",
    "    etj[j][0] = 1\n",
    "    etOc = np.zeros((nt, 1))\n",
    "    etOc[Oc] = 1\n",
    "    etaj = np.vstack((np.zeros((ns, 1)), etj-(1/len(Oc))*etOc))\n",
    "\n",
    "    etajTx = etaj.T.dot(X)\n",
    "    \n",
    "    print(f'Anomaly index: {O[0] + ns}')\n",
    "    print(f'etajTX: {etajTx}')\n",
    "    mu = np.vstack((np.full((ns,1), mu_s), np.full((nt,1), mu_t)))\n",
    "    sigma = np.identity(ns+nt)\n",
    "    etajTmu = etaj.T.dot(mu)\n",
    "    etajTsigmaetaj = etaj.T.dot(sigma).dot(etaj)\n",
    "    b = sigma.dot(etaj).dot(np.linalg.inv(etajTsigmaetaj))\n",
    "    a = (np.identity(ns+nt) - b.dot(etaj.T)).dot(X)\n",
    "    threshold = 20\n",
    "    list_zk, list_Oz = run_parametric_wdgrl(X, etaj, ns+nt, threshold)\n",
    "    CDF = cdf(etajTmu[0][0], etajTsigmaetaj[0][0], list_zk, list_Oz, etajTx[0][0], [O[0] + ns])\n",
    "    p_value = 2 * min(CDF, 1 - CDF)\n",
    "    print(f'p-value: {p_value}')\n",
    "    print('-------------------------------------------------')\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration #0:\n",
      "Anomaly index: 107\n",
      "etajTX: [[3.31410458]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_22296\\2610257826.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  u = float(u)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_22296\\2610257826.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  v = float(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.7567371453392275\n",
      "-------------------------------------------------\n",
      "iteration #1:\n",
      "Anomaly index: 107\n",
      "etajTX: [[3.20405869]]\n",
      "p-value: 0.8859762085548786\n",
      "-------------------------------------------------\n",
      "iteration #2:\n",
      "Anomaly index: 107\n",
      "etajTX: [[3.34925913]]\n",
      "p-value: 0.04049195150750706\n",
      "-------------------------------------------------\n",
      "iteration #3:\n",
      "Anomaly index: 102\n",
      "etajTX: [[5.34805247]]\n",
      "p-value: 8.105448723183528e-05\n",
      "-------------------------------------------------\n",
      "iteration #4:\n",
      "Anomaly index: 108\n",
      "etajTX: [[5.12805402]]\n",
      "p-value: 0.0001646541569233495\n",
      "-------------------------------------------------\n",
      "iteration #5:\n",
      "Anomaly index: 101\n",
      "etajTX: [[3.31604115]]\n",
      "p-value: 0.044897989969270746\n",
      "-------------------------------------------------\n",
      "iteration #6:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.86723504]]\n",
      "p-value: 0.03897216754626531\n",
      "-------------------------------------------------\n",
      "iteration #7:\n",
      "Anomaly index: 106\n",
      "etajTX: [[4.35843623]]\n",
      "p-value: 0.0009091146503623637\n",
      "-------------------------------------------------\n",
      "iteration #8:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.92523285]]\n",
      "p-value: 0.49212270328135244\n",
      "-------------------------------------------------\n",
      "iteration #9:\n",
      "Anomaly index: 103\n",
      "etajTX: [[4.7093945]]\n",
      "p-value: 0.005492874054011443\n",
      "-------------------------------------------------\n",
      "iteration #10:\n",
      "Anomaly index: 108\n",
      "etajTX: [[3.11700172]]\n",
      "p-value: 0.5237853062071174\n",
      "-------------------------------------------------\n",
      "iteration #11:\n",
      "Anomaly index: 101\n",
      "etajTX: [[5.04026151]]\n",
      "p-value: 0.0043105775271277125\n",
      "-------------------------------------------------\n",
      "iteration #12:\n",
      "Anomaly index: 103\n",
      "etajTX: [[3.09421506]]\n",
      "p-value: 0.13787613467467286\n",
      "-------------------------------------------------\n",
      "iteration #13:\n",
      "Anomaly index: 108\n",
      "etajTX: [[4.85074055]]\n",
      "p-value: 0.01855896984373895\n",
      "-------------------------------------------------\n",
      "iteration #14:\n",
      "Anomaly index: 103\n",
      "etajTX: [[5.22980971]]\n",
      "p-value: 0.0007007718779388217\n",
      "-------------------------------------------------\n",
      "iteration #15:\n",
      "Anomaly index: 105\n",
      "etajTX: [[5.4466828]]\n",
      "p-value: 1.3128220066160878e-05\n",
      "-------------------------------------------------\n",
      "iteration #16:\n",
      "Anomaly index: 108\n",
      "etajTX: [[3.76650483]]\n",
      "p-value: 0.08967519479211505\n",
      "-------------------------------------------------\n",
      "iteration #17:\n",
      "Anomaly index: 107\n",
      "etajTX: [[5.37124496]]\n",
      "p-value: 2.0522969268021996e-05\n",
      "-------------------------------------------------\n",
      "iteration #18:\n",
      "Anomaly index: 102\n",
      "etajTX: [[5.30846182]]\n",
      "p-value: 5.854297768648209e-05\n",
      "-------------------------------------------------\n",
      "iteration #19:\n",
      "Anomaly index: 103\n",
      "etajTX: [[5.15131615]]\n",
      "p-value: 0.00017526756687291645\n",
      "-------------------------------------------------\n",
      "iteration #20:\n",
      "Anomaly index: 108\n",
      "etajTX: [[4.44172076]]\n",
      "p-value: 0.0005330774169580721\n",
      "-------------------------------------------------\n",
      "iteration #21:\n",
      "Anomaly index: 103\n",
      "etajTX: [[5.99794681]]\n",
      "p-value: 1.5318914722284305e-05\n",
      "-------------------------------------------------\n",
      "iteration #22:\n",
      "Anomaly index: 105\n",
      "etajTX: [[3.81100534]]\n",
      "p-value: 0.003920396086407818\n",
      "-------------------------------------------------\n",
      "iteration #23:\n",
      "Anomaly index: 108\n",
      "etajTX: [[5.34861905]]\n",
      "p-value: 0.00977341543131205\n",
      "-------------------------------------------------\n",
      "iteration #24:\n",
      "Anomaly index: 109\n",
      "etajTX: [[3.63230631]]\n",
      "p-value: 0.09064560365390384\n",
      "-------------------------------------------------\n",
      "iteration #25:\n",
      "Anomaly index: 108\n",
      "etajTX: [[3.82825008]]\n",
      "p-value: 0.007489442072676988\n",
      "-------------------------------------------------\n",
      "iteration #26:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.98516255]]\n",
      "p-value: 0.040544199878046916\n",
      "-------------------------------------------------\n",
      "iteration #27:\n",
      "Anomaly index: 102\n",
      "etajTX: [[6.13824815]]\n",
      "p-value: 1.0342984158029367e-06\n",
      "-------------------------------------------------\n",
      "iteration #28:\n",
      "Anomaly index: 100\n",
      "etajTX: [[3.4008453]]\n",
      "p-value: 0.050324595082697154\n",
      "-------------------------------------------------\n",
      "iteration #29:\n",
      "Anomaly index: 106\n",
      "etajTX: [[3.38725619]]\n",
      "p-value: 0.07547521489471931\n",
      "-------------------------------------------------\n",
      "iteration #30:\n",
      "Anomaly index: 105\n",
      "etajTX: [[5.03862394]]\n",
      "p-value: 0.00026545288494217445\n",
      "-------------------------------------------------\n",
      "iteration #31:\n",
      "Anomaly index: 100\n",
      "etajTX: [[3.61559881]]\n",
      "p-value: 0.019430734722318155\n",
      "-------------------------------------------------\n",
      "iteration #32:\n",
      "Anomaly index: 101\n",
      "etajTX: [[4.7784825]]\n",
      "p-value: 0.0014905001358416214\n",
      "-------------------------------------------------\n",
      "iteration #33:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.48031875]]\n",
      "p-value: 0.03226489619745743\n",
      "-------------------------------------------------\n",
      "iteration #34:\n",
      "Anomaly index: 103\n",
      "etajTX: [[5.39097846]]\n",
      "p-value: 1.0268071098495923e-05\n",
      "-------------------------------------------------\n",
      "iteration #35:\n",
      "Anomaly index: 105\n",
      "etajTX: [[7.32704778]]\n",
      "p-value: 1.328013699009034e-09\n",
      "-------------------------------------------------\n",
      "iteration #36:\n",
      "Anomaly index: 104\n",
      "etajTX: [[2.84975255]]\n",
      "p-value: 0.39110706690803093\n",
      "-------------------------------------------------\n",
      "iteration #37:\n",
      "Anomaly index: 100\n",
      "etajTX: [[4.75995792]]\n",
      "p-value: 0.005002928682212637\n",
      "-------------------------------------------------\n",
      "iteration #38:\n",
      "Anomaly index: 103\n",
      "etajTX: [[3.67672247]]\n",
      "p-value: 0.03869971722182286\n",
      "-------------------------------------------------\n",
      "iteration #39:\n",
      "Anomaly index: 105\n",
      "etajTX: [[6.19313438]]\n",
      "p-value: 2.6966044990306415e-06\n",
      "-------------------------------------------------\n",
      "iteration #40:\n",
      "Anomaly index: 101\n",
      "etajTX: [[2.66325007]]\n",
      "p-value: 0.28489820052527826\n",
      "-------------------------------------------------\n",
      "iteration #41:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.86224727]]\n",
      "p-value: 0.8091037313047456\n",
      "-------------------------------------------------\n",
      "iteration #42:\n",
      "Anomaly index: 100\n",
      "etajTX: [[4.46576903]]\n",
      "p-value: 0.015057225661497364\n",
      "-------------------------------------------------\n",
      "iteration #43:\n",
      "Anomaly index: 103\n",
      "etajTX: [[4.00207054]]\n",
      "p-value: 0.042766090398209444\n",
      "-------------------------------------------------\n",
      "iteration #44:\n",
      "Anomaly index: 103\n",
      "etajTX: [[2.76712432]]\n",
      "p-value: 0.15076655725374444\n",
      "-------------------------------------------------\n",
      "iteration #45:\n",
      "Anomaly index: 103\n",
      "etajTX: [[4.81254476]]\n",
      "p-value: 0.12423522001638077\n",
      "-------------------------------------------------\n",
      "iteration #46:\n",
      "Anomaly index: 101\n",
      "etajTX: [[4.85043516]]\n",
      "p-value: 0.00017113373172361612\n",
      "-------------------------------------------------\n",
      "iteration #47:\n",
      "Anomaly index: 108\n",
      "etajTX: [[4.44415239]]\n",
      "p-value: 0.0006720912131972234\n",
      "-------------------------------------------------\n",
      "iteration #48:\n",
      "Anomaly index: 100\n",
      "etajTX: [[4.61627991]]\n",
      "p-value: 0.0034923364211310037\n",
      "-------------------------------------------------\n",
      "iteration #49:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.04762868]]\n",
      "p-value: 0.610602389103978\n",
      "-------------------------------------------------\n",
      "iteration #50:\n",
      "Anomaly index: 100\n",
      "etajTX: [[4.42985323]]\n",
      "p-value: 0.02051298100716825\n",
      "-------------------------------------------------\n",
      "iteration #51:\n",
      "Anomaly index: 104\n",
      "etajTX: [[4.49638681]]\n",
      "p-value: 0.010410492175604302\n",
      "-------------------------------------------------\n",
      "iteration #52:\n",
      "Anomaly index: 109\n",
      "etajTX: [[5.74696659]]\n",
      "p-value: 1.330035138868979e-05\n",
      "-------------------------------------------------\n",
      "iteration #53:\n",
      "Anomaly index: 107\n",
      "etajTX: [[4.05187325]]\n",
      "p-value: 0.025445313476766485\n",
      "-------------------------------------------------\n",
      "iteration #54:\n",
      "Anomaly index: 104\n",
      "etajTX: [[5.80787296]]\n",
      "p-value: 0.0006943180174310282\n",
      "-------------------------------------------------\n",
      "iteration #55:\n",
      "Anomaly index: 100\n",
      "etajTX: [[2.87760981]]\n",
      "p-value: 0.3779622938983378\n",
      "-------------------------------------------------\n",
      "iteration #56:\n",
      "Anomaly index: 107\n",
      "etajTX: [[6.4016295]]\n",
      "p-value: 0.0005568827267541376\n",
      "-------------------------------------------------\n",
      "iteration #57:\n",
      "Anomaly index: 106\n",
      "etajTX: [[3.77905257]]\n",
      "p-value: 0.0294060097982054\n",
      "-------------------------------------------------\n",
      "iteration #58:\n",
      "Anomaly index: 100\n",
      "etajTX: [[3.88637145]]\n",
      "p-value: 0.0031039894678281943\n",
      "-------------------------------------------------\n",
      "iteration #59:\n",
      "Anomaly index: 107\n",
      "etajTX: [[3.43257889]]\n",
      "p-value: 0.09416648737420275\n",
      "-------------------------------------------------\n",
      "iteration #60:\n",
      "Anomaly index: 107\n",
      "etajTX: [[4.28350157]]\n",
      "p-value: 0.0031109689285853115\n",
      "-------------------------------------------------\n",
      "iteration #61:\n",
      "Anomaly index: 109\n",
      "etajTX: [[3.45620557]]\n",
      "p-value: 0.026338512821972193\n",
      "-------------------------------------------------\n",
      "iteration #62:\n",
      "Anomaly index: 109\n",
      "etajTX: [[3.61792453]]\n",
      "p-value: 0.012395102615309517\n",
      "-------------------------------------------------\n",
      "iteration #63:\n",
      "Anomaly index: 102\n",
      "etajTX: [[5.12135644]]\n",
      "p-value: 0.005723910235001517\n",
      "-------------------------------------------------\n",
      "iteration #64:\n",
      "Anomaly index: 109\n",
      "etajTX: [[4.21208486]]\n",
      "p-value: 0.0011990683720175532\n",
      "-------------------------------------------------\n",
      "iteration #65:\n",
      "Anomaly index: 109\n",
      "etajTX: [[5.01351547]]\n",
      "p-value: 3.7300727356193164e-05\n",
      "-------------------------------------------------\n",
      "iteration #66:\n",
      "Anomaly index: 102\n",
      "etajTX: [[3.94134647]]\n",
      "p-value: 0.011496890118652292\n",
      "-------------------------------------------------\n",
      "iteration #67:\n",
      "Anomaly index: 100\n",
      "etajTX: [[3.92783617]]\n",
      "p-value: 0.00512957325516461\n",
      "-------------------------------------------------\n",
      "iteration #68:\n",
      "Anomaly index: 102\n",
      "etajTX: [[3.65629841]]\n",
      "p-value: 0.21286478957472577\n",
      "-------------------------------------------------\n",
      "iteration #69:\n",
      "Anomaly index: 104\n",
      "etajTX: [[4.27800596]]\n",
      "p-value: 0.01602927262200171\n",
      "-------------------------------------------------\n",
      "iteration #70:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.0113976]]\n",
      "p-value: 0.9971689316260082\n",
      "-------------------------------------------------\n",
      "iteration #71:\n",
      "Anomaly index: 102\n",
      "etajTX: [[3.49665476]]\n",
      "p-value: 0.533765730490928\n",
      "-------------------------------------------------\n",
      "iteration #72:\n",
      "Anomaly index: 102\n",
      "etajTX: [[4.2220235]]\n",
      "p-value: 0.001237882963198711\n",
      "-------------------------------------------------\n",
      "iteration #73:\n",
      "Anomaly index: 103\n",
      "etajTX: [[3.30474125]]\n",
      "p-value: 0.9422090014259983\n",
      "-------------------------------------------------\n",
      "iteration #74:\n",
      "Anomaly index: 105\n",
      "etajTX: [[4.4660129]]\n",
      "p-value: 0.007624358362158468\n",
      "-------------------------------------------------\n",
      "iteration #75:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.93519155]]\n",
      "p-value: 0.004851082373346749\n",
      "-------------------------------------------------\n",
      "iteration #76:\n",
      "Anomaly index: 102\n",
      "etajTX: [[4.1899292]]\n",
      "p-value: 0.3800690590918667\n",
      "-------------------------------------------------\n",
      "iteration #77:\n",
      "Anomaly index: 101\n",
      "etajTX: [[3.54391025]]\n",
      "p-value: 0.0163904503038812\n",
      "-------------------------------------------------\n",
      "iteration #78:\n",
      "Anomaly index: 100\n",
      "etajTX: [[4.60944607]]\n",
      "p-value: 0.4954578743964482\n",
      "-------------------------------------------------\n",
      "iteration #79:\n",
      "Anomaly index: 105\n",
      "etajTX: [[5.57089897]]\n",
      "p-value: 0.025968031579753248\n",
      "-------------------------------------------------\n",
      "iteration #80:\n",
      "Anomaly index: 103\n",
      "etajTX: [[3.55401303]]\n",
      "p-value: 0.055893209923676634\n",
      "-------------------------------------------------\n",
      "iteration #81:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.4633961]]\n",
      "p-value: 0.14272397733481657\n",
      "-------------------------------------------------\n",
      "iteration #82:\n",
      "Anomaly index: 105\n",
      "etajTX: [[3.99144069]]\n",
      "p-value: 0.027298124532920243\n",
      "-------------------------------------------------\n",
      "iteration #83:\n",
      "Anomaly index: 102\n",
      "etajTX: [[3.32998272]]\n",
      "p-value: 0.038703268868007124\n",
      "-------------------------------------------------\n",
      "iteration #84:\n",
      "Anomaly index: 104\n",
      "etajTX: [[4.8613787]]\n",
      "p-value: 0.0012591412808029556\n",
      "-------------------------------------------------\n",
      "iteration #85:\n",
      "Anomaly index: 109\n",
      "etajTX: [[2.74783521]]\n",
      "p-value: 0.07605655936161093\n",
      "-------------------------------------------------\n",
      "iteration #86:\n",
      "Anomaly index: 103\n",
      "etajTX: [[2.67813545]]\n",
      "p-value: 0.16422770346953897\n",
      "-------------------------------------------------\n",
      "iteration #87:\n",
      "Anomaly index: 107\n",
      "etajTX: [[4.07469326]]\n",
      "p-value: 0.14355900222809592\n",
      "-------------------------------------------------\n",
      "iteration #88:\n",
      "Anomaly index: 100\n",
      "etajTX: [[4.52276297]]\n",
      "p-value: 0.0022766550095758564\n",
      "-------------------------------------------------\n",
      "iteration #89:\n",
      "Anomaly index: 106\n",
      "etajTX: [[5.64968007]]\n",
      "p-value: 6.210962674613185e-06\n",
      "-------------------------------------------------\n",
      "iteration #90:\n",
      "Anomaly index: 109\n",
      "etajTX: [[5.49910478]]\n",
      "p-value: 0.0002852337743211031\n",
      "-------------------------------------------------\n",
      "iteration #91:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.99748115]]\n",
      "p-value: 0.11780764234956709\n",
      "-------------------------------------------------\n",
      "iteration #92:\n",
      "Anomaly index: 109\n",
      "etajTX: [[4.1529612]]\n",
      "p-value: 0.06683597884539005\n",
      "-------------------------------------------------\n",
      "iteration #93:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.06405624]]\n",
      "p-value: 0.642270241401074\n",
      "-------------------------------------------------\n",
      "iteration #94:\n",
      "Anomaly index: 101\n",
      "etajTX: [[3.73697098]]\n",
      "p-value: 0.01710832209666724\n",
      "-------------------------------------------------\n",
      "iteration #95:\n",
      "Anomaly index: 109\n",
      "etajTX: [[4.22844509]]\n",
      "p-value: 0.030035183661711207\n",
      "-------------------------------------------------\n",
      "iteration #96:\n",
      "Anomaly index: 100\n",
      "etajTX: [[3.43376423]]\n",
      "p-value: 0.2352046551548692\n",
      "-------------------------------------------------\n",
      "iteration #97:\n",
      "Anomaly index: 108\n",
      "etajTX: [[4.49638751]]\n",
      "p-value: 0.0086896986382492\n",
      "-------------------------------------------------\n",
      "iteration #98:\n",
      "Anomaly index: 107\n",
      "etajTX: [[4.87786347]]\n",
      "p-value: 0.0029353109009297995\n",
      "-------------------------------------------------\n",
      "iteration #99:\n",
      "Anomaly index: 108\n",
      "etajTX: [[3.87442712]]\n",
      "p-value: 0.03406707131543474\n",
      "-------------------------------------------------\n",
      "iteration #100:\n",
      "Anomaly index: 102\n",
      "etajTX: [[5.1099679]]\n",
      "p-value: 0.002841667201768816\n",
      "-------------------------------------------------\n",
      "iteration #101:\n",
      "Anomaly index: 104\n",
      "etajTX: [[2.76890996]]\n",
      "p-value: 0.8649894233564909\n",
      "-------------------------------------------------\n",
      "iteration #102:\n",
      "Anomaly index: 105\n",
      "etajTX: [[3.66081673]]\n",
      "p-value: 0.019930155489248147\n",
      "-------------------------------------------------\n",
      "iteration #103:\n",
      "Anomaly index: 107\n",
      "etajTX: [[3.96340129]]\n",
      "p-value: 0.045350034446868914\n",
      "-------------------------------------------------\n",
      "iteration #104:\n",
      "Anomaly index: 106\n",
      "etajTX: [[4.23711456]]\n",
      "p-value: 0.013629014242108939\n",
      "-------------------------------------------------\n",
      "iteration #105:\n",
      "Anomaly index: 107\n",
      "etajTX: [[5.33129041]]\n",
      "p-value: 3.532382495707864e-05\n",
      "-------------------------------------------------\n",
      "iteration #106:\n",
      "Anomaly index: 101\n",
      "etajTX: [[4.68774005]]\n",
      "p-value: 0.0005358142305185165\n",
      "-------------------------------------------------\n",
      "iteration #107:\n",
      "Anomaly index: 100\n",
      "etajTX: [[4.55578143]]\n",
      "p-value: 0.020014917699639145\n",
      "-------------------------------------------------\n",
      "iteration #108:\n",
      "Anomaly index: 103\n",
      "etajTX: [[3.86856763]]\n",
      "p-value: 0.23302377517746597\n",
      "-------------------------------------------------\n",
      "iteration #109:\n",
      "Anomaly index: 103\n",
      "etajTX: [[4.14436193]]\n",
      "p-value: 0.013369658987264721\n",
      "-------------------------------------------------\n",
      "iteration #110:\n",
      "Anomaly index: 106\n",
      "etajTX: [[4.09301839]]\n",
      "p-value: 0.4438504411009944\n",
      "-------------------------------------------------\n",
      "iteration #111:\n",
      "Anomaly index: 102\n",
      "etajTX: [[5.19104495]]\n",
      "p-value: 0.000329514664767272\n",
      "-------------------------------------------------\n",
      "iteration #112:\n",
      "Anomaly index: 109\n",
      "etajTX: [[3.85368436]]\n",
      "p-value: 0.007788325471865587\n",
      "-------------------------------------------------\n",
      "iteration #113:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.95958399]]\n",
      "p-value: 0.20252719139035968\n",
      "-------------------------------------------------\n",
      "iteration #114:\n",
      "Anomaly index: 104\n",
      "etajTX: [[3.3240793]]\n",
      "p-value: 0.84600210899725\n",
      "-------------------------------------------------\n",
      "iteration #115:\n",
      "Anomaly index: 105\n",
      "etajTX: [[2.73477577]]\n",
      "p-value: 0.994111005969565\n",
      "-------------------------------------------------\n",
      "iteration #116:\n",
      "Anomaly index: 102\n",
      "etajTX: [[3.33444306]]\n",
      "p-value: 0.23688254502983574\n",
      "-------------------------------------------------\n",
      "iteration #117:\n",
      "Anomaly index: 107\n",
      "etajTX: [[6.02587958]]\n",
      "p-value: 0.04660284871130438\n",
      "-------------------------------------------------\n",
      "iteration #118:\n",
      "Anomaly index: 100\n",
      "etajTX: [[5.18385946]]\n",
      "p-value: 0.0026958426391334722\n",
      "-------------------------------------------------\n",
      "iteration #119:\n",
      "Anomaly index: 103\n",
      "etajTX: [[4.88331952]]\n",
      "p-value: 0.007840271194784476\n",
      "-------------------------------------------------\n",
      "iteration #120:\n",
      "Anomaly index: 106\n",
      "etajTX: [[4.79274779]]\n",
      "p-value: 0.0003800299346148339\n",
      "-------------------------------------------------\n",
      "iteration #121:\n",
      "TPR: 0.6776859504132231\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "max_iteration = 120\n",
    "alpha = 0.05\n",
    "list_p_value = []\n",
    "count = 0\n",
    "print(f'iteration #{0}:')\n",
    "\n",
    "\n",
    "while len(list_p_value) <= max_iteration:\n",
    "    p_value = run_tpr()\n",
    "    if p_value is None:\n",
    "        continue\n",
    "    print(f'iteration #{len(list_p_value)+1}:')\n",
    "    list_p_value.append(p_value)\n",
    "    if p_value <= alpha:\n",
    "        count += 1\n",
    "print(f'TPR: {count / len(list_p_value)}')\n",
    "plt.hist(list_p_value)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
